% Chapter 2

\chapter{Background and Related Work} % (fold)
\label{chap:Background and Related Work}

This chapter provides the foundational knowledge required to understand the core concepts of this thesis. It begins by exploring the fundamental principles of cloud security and generative AI. Subsequently, it reviews the current landscape of major cloud providers and their security ecosystems. The chapter culminates in a comprehensive review of the state-of-the-art literature, identifying key research trends and existing gaps in the field of GenAI-driven security automation.

\section{Foundational Concepts in Cloud Security} % (fold)
\label{sec:Foundational Concepts in Cloud Computing}

This section provides an overview of the fundamental concepts in cloud security, which form the basis for the subsequent discussion on security automation.

% section Foundational Concepts in Cloud Computing (end)

\section{Foundational Concepts in Generative AI} % (fold)
\label{sec:Foundational Concepts in Generative AI}

This section introduces the core concepts of Generative AI, setting the stage for its application in the context of cloud security.

% section Foundational Concepts in Generative AI (end)

\section{State of Cloud Provider Ecosystems} % (fold)
\label{sec:State of Cloud Provider Ecosystems}

This section reviews the current landscape of major cloud providers, highlighting their security services and the evolving ecosystem.

% section State of Cloud Provider Ecosystems (end)

\section{Literature State of the Art} % (fold)
\label{sec:State-of-the-Art}

The convergence of \gls{genai} and hyperscale cloud platforms presents a rapidly evolving frontier for cybersecurity. As organizations increasingly rely on complex cloud environments, the scale and sophistication of threats necessitate advanced automation capabilities. \gls{genai} offers promising avenues for enhancing security posture through intelligent automation, but its integration also introduces challenges and risks. A comprehensive understanding of the existing research landscape is therefore essential to identify established practices, emerging trends, and critical gaps in knowledge.

This literature review synthesizes current academic and industry contributions pertinent to the application of \gls{genai} for security automation within hyperscale and multi-cloud contexts. It begins by outlining the methodology employed to select and analyze relevant works. Subsequently, the review dives into several key thematic areas: the evolution from traditional security methods to \gls{ai}-driven approaches, specific frameworks for scoping and managing \gls{genai} security implementations, architectural patterns and techniques for security automation, a critical examination of the unique security risks associated with \gls{genai} itself, risk management frameworks and the ongoing crucial discussion regarding the necessary balance between automation and human oversight.

By examining these facets, this review aims to provide a foundational understanding of the state-of-the-art, highlighting both the potential of \gls{genai} in cloud security automation and the significant considerations that must be addressed for its responsible and effective deployment. This synthesis will builds the basis for the subsequent research presented in this thesis.

\subsection{Methodology} % (fold)
\label{sec:Methodology}

This literature review followed a structured approach to identify relevant publications, focusing on peer-reviewed articles addressing \gls{genai} applications in hyperscale cloud security published primarily within the last five years. The search utilized academic databases with key search terms related to generative AI, cloud security automation, hyperscale platforms, and multi-cloud orchestration.

Papers were selected based on their relevance to:

\begin{itemize}
\item \gls{genai} applications specifically in cloud security contexts
\item Hyperscale or multi-cloud environments
\item Technical solutions for security automation
\item Empirical evidence or theoretical frameworks with substantial methodological rigor
\end{itemize}

The selection process involved initial screening of titles and abstracts followed by full-text review of promising papers. The analysis employed a thematic approach, identifying recurring concepts, methodological approaches, and gaps in existing research. Particular attention was paid to identifying the theoretical foundations underpinning \gls{genai} applications in security contexts, empirical evidence of effectiveness, and limitations of current approaches.

% subsection Methodology (end)

\subsection{AI-Driven Security Approaches} % (fold)
\label{sec:AI-Driven Security Approaches}

The landscape of cloud security is undergoing a significant transformation, shifting from traditional, often reactive methods towards more proactive and adaptive strategies powered by \gls{ai}, particularly \gls{genai}. This evolution marks a move beyond basic anomaly detection towards sophisticated security postures capable of learning from and responding dynamically to novel threat vectors in complex cloud environments.

Foundational work by Khanna \cite{khanna_enhancing_2024} explores the integration of \gls{genai} into cloud security, outlining its core applications. According to Khanna, modern \gls{genai} implementations focus on key capabilities such as processing vast amounts of data (e.g., log entries, network packets) for advanced anomaly detection and threat intelligence, enabling automated response mechanisms that dynamically adjust security protocols, and facilitating predictive security measures to forecast potential vulnerabilities. While highlighting these advancements, Khanna also acknowledges inherent challenges, including the need for large datasets and mitigating potential adversarial manipulation.

Building upon these foundational capabilities, \cite{seth_ai_2025} also concludes that the integration of \gls{genai} represents a significant leap beyond conventional rule-based security systems. Its research indicates that \gls{genai} enhances security automation, particularly within multi-cloud and hybrid architectures. It allows systems to adapt infrastructure dynamically in response to varying traffic patterns and implements \gls{ai}-powered defenses against continuously evolving cyber threats. This adaptive capability directly addresses persistent challenges in cloud security related to optimizing workload distribution, ensuring performance, and managing costs effectively.

Furthermore, recent studies underscore the practical impact of integrating \gls{genai} with established cloud-native security tools. Patel et al. \cite{patel_generative_2025} demonstrate how layering \gls{genai} onto platforms like \gls{aws} GuardDuty and Google Cloud Security Command Center significantly boosts automated threat detection, enables real-time incident response, and improves comprehensive vulnerability management across distributed cloud infrastructures. Their work provides empirical evidence, citing examples like Netflix and JPMorgan Chase, which reported measurable improvements in detection accuracy and notable reductions in security incidents following the adoption of \gls{genai}-driven security automation strategies \cite{patel_generative_2025}. This convergence of \gls{genai} with existing security frameworks highlights its potential to transform \glspl{soc} by enhancing both efficiency and effectiveness.

% subsection AI-Driven Security Approaches (end)

\subsection{GenAI Security Frameworks}
\label{sec:GenAI Security Frameworks}

The rapid integration of \gls{genai} into various domains, including security automation within hyperscale cloud platforms, necessitates robust frameworks to govern its development, deployment, and operation securely. Unlike traditional software, \gls{genai} systems introduce unique risks stemming from their data dependency, complexity, potential for emergent behaviors, and socio-technical nature \cite{tabassi_artificial_2023}. These risks include prompt injection, data leakage through model inversion or training data extraction, adversarial attacks, generation of harmful or biased content, and insecure code generation \cite{haryanto_secgenai_2024, hansen_introducing_2023}. Consequently, a multi-faceted approach to security is required, encompassing foundational risk management principles, organizational governance structures, technical control implementation, and context-specific guidance. This subchapter reviews several key frameworks and guides that collectively address the challenge of securing \gls{genai} systems.

A foundational element in managing \gls{ai} risks is provided by the \gls{airmf} 1.0 developed by the \gls{nist} \cite{tabassi_artificial_2023}. This voluntary, non-sector-specific framework aims to help organizations manage \gls{ai} risks and promote trustworthy and responsible \gls{ai} development and use. It is structured around four core functions: GOVERN, MAP, MEASURE, and MANAGE. The GOVERN function is cross-cutting, establishing a risk management culture and processes throughout the organization. MAP involves contextualizing risks and understanding potential impacts. MEASURE employs qualitative and quantitative tools to analyze, assess, and track \gls{ai} risks. MANAGE focuses on prioritizing and acting on risks based on the previous functions \cite{tabassi_artificial_2023}. The \gls{nist} \gls{airmf} emphasizes characteristics of trustworthy \gls{ai} systems, including validity and reliability, safety, security and resilience, accountability and transparency, explainability and interpretability, privacy-enhancement, and fairness with harmful bias managed \cite{tabassi_artificial_2023}. It acknowledges the challenges specific to \gls{ai} risk management, such as difficulties in risk measurement (especially with third-party components and emergent risks), defining risk tolerance, prioritizing risks, and integrating \gls{ai} risk management into broader enterprise strategies \cite{tabassi_artificial_2023}.

Building upon similar principles but offering a perspective from a major cloud provider, Google's \gls{saif} presents a conceptual framework inspired by security best practices applied to software development, adapted for \gls{ai}-specific risks \cite{hansen_introducing_2023}. \gls{saif} proposes six core elements for secure \gls{ai} systems\cite{hansen_introducing_2023}:

\begin{enumerate}
\item Expand strong security foundations to the \gls{ai} ecosystem by applying and adapting existing controls.
\item Extend detection and response to include \gls{ai}-specific threats and outputs.
\item Automate defenses using \gls{ai} itself where appropriate, while keeping humans in the loop.
\item Harmonize platform-level controls to ensure consistency and prevent fragmentation.
\item Adapt controls with faster feedback loops, incorporating insights from red teaming and novel attack awareness.
\item Contextualize \gls{ai} system risks within surrounding business processes, including model risk management and shared responsibility.
\end{enumerate}

\gls{saif} advocates a practical implementation approach involving understanding the specific use case, assembling a multidisciplinary team, providing an \gls{ai} primer for all stakeholders, and applying the six core elements iteratively \cite{hansen_introducing_2023}. Like the \gls{nist} \gls{airmf}, \gls{saif} highlights the socio-technical nature of \gls{ai} risks and the importance of context.

While \gls{nist} and Google provide overarching frameworks, securing \gls{genai}, particularly \glspl{llm}, requires specific organizational structures and practices. The \gls{llm} and Generative \gls{ai} Security \gls{coe} Guide from OWASP addresses this by outlining how to establish a dedicated \gls{coe} \cite{editor_llm_nodate-1}. The primary objective of such a \gls{coe} is "to develop and enforce security policies and protocols for generative \gls{ai} applications, facilitate cross-departmental collaboration to harness expertise from various fields, educate and train teams on the ethical and secure use of generative \gls{ai} technologies, and serve as an advisory body for \gls{ai}-related projects and initiatives within the organization" \cite[p.4]{editor_llm_nodate-1}. This guide emphasizes the necessity of a multidisciplinary team, bringing together expertise from Cybersecurity, AI/ML Development, IT Operations, Legal, Compliance, Ethics, Governance, Risk Management, Data Science, and various other user groups\cite{editor_llm_nodate-1}. Establishing clear objectives, Key Performance Indicators, roles, and responsibilities is crucial for the \gls{coe}'s success. The guide suggests a phased implementation (Planning, Integration, Operationalization, Evaluation) and highlights the importance of leveraging both internal and external expertise to address challenges like communication barriers, resistance to change, and skill gaps \cite{editor_llm_nodate-1}. The \gls{coe} structure directly supports the GOVERN function described in the \gls{nist} \gls{airmf} and aligns with \gls{saif}'s recommendation to assemble a cross-functional team.

To translate high-level governance and risk management principles into concrete verification steps, the \gls{owasp} \gls{llm} Applications Cybersecurity and Governance Checklist provides a practical, actionable tool \cite{editor_llm_nodate}. Derived from the well-known \gls{owasp} Top 10 for \gls{llm} Applications project, this checklist offers a structured approach to assessing the security posture of \gls{llm}-based systems. It covers a wide array of control areas critical for \gls{llm} security, extending beyond purely technical vulnerabilities to encompass essential governance aspects \cite{editor_llm_nodate}. Key domains addressed include Input Validation and handling, Output Encoding and Handling, Access Control and Authorization, Data Privacy and Confidentiality, Model Training and Fine-tuning Security, \gls{api} and Integration Security, robust Logging and Monitoring, Incident Response Planning, and overall Governance, Risk, and Compliance\cite{editor_llm_nodate}. The checklist serves as a valuable resource for development teams, security assessors, and governance bodies to systematically identify potential weaknesses, guide the implementation of specific security controls, and perform gap analyses against established best practices \cite{editor_llm_nodate}. In the context of broader frameworks, this checklist acts as a practical instrument for the MEASURE and MANAGE functions outlined in the \gls{nist} \gls{airmf} \cite{tabassi_artificial_2023}, providing specific checks aligned with the risks highlighted by \gls{saif} \cite{hansen_introducing_2023} and the technical controls advocated by SecGenAI \cite{haryanto_secgenai_2024}. It furnishes the \gls{coe}\cite{editor_llm_nodate} with a concrete tool for enforcing security policies and protocols.
Regarding a specific \gls{genai} architecture, the SecGenAI framework focuses on enhancing the security of cloud-based \gls{genai} applications, particularly \gls{rag} systems, within the context of Australian critical technologies \cite{haryanto_secgenai_2024}. SecGenAI adopts an end-to-end perspective covering Functional, Infrastructure, and Governance requirements. Functionally, it analyzes \gls{rag} components and identifies root causes for security concerns like injection attacks, data leakage, and model inversion \cite{haryanto_secgenai_2024}. It proposes specific countermeasures such as input validation, robust access controls, data protection techniques, and model security measures\cite{haryanto_secgenai_2024}. On the infrastructure side, SecGenAI details requirements for sandboxing, secure database connections, network segmentation, external attack prevention, and disaster recovery within a cloud environment\cite{haryanto_secgenai_2024}. Governance requirements emphasize alignment with Australian AI Ethics Principles and \gls{app}, advocating for fairness, accountability, traceability, data protection, regular audits, reliability, transparency, and compliance, structured using the \gls{iso} 38500 Evaluate-Direct-Monitor cycle \cite{noauthor_isoiec_nodate}. SecGenAI explicitly incorporates the Shared Responsibility Model, detailing Cloud Service Provider and customer obligations for \gls{genai} security\cite{haryanto_secgenai_2024}. This framework provides a detailed blueprint for securing a specific, common \gls{genai} patters by integrating technical and governance controls, reflecting a practical application of the principles found in \gls{nist} \gls{airmf} and \gls{saif}.

The successful implementation of these security frameworks inherently depends on the underlying platform architecture. The paper "Integration patterns in unified AI and cloud platforms" provides context by reviewing how \gls{ai}, \gls{mlops}, workflow orchestration, and data processing converge in cloud-native environments\cite{sushil_prabhu_prabhakaran_integration_2024}. It highlights the importance of \gls{mlops} frameworks for lifecycle management, workflow orchestration engines for process automation, and robust data processing systems as core components \cite{sushil_prabhu_prabhakaran_integration_2024}. Security considerations must be embedded within these components and their integration patterns. For instance, securing data pipelines within \gls{mlops}, ensuring secure communication in federated learning setups, or implementing access controls within workflow orchestration are crucial \cite{sushil_prabhu_prabhakaran_integration_2024, hansen_introducing_2023, haryanto_secgenai_2024}. The paper implicitly underscores that security cannot be an afterthought but must be integrated into the design and automation processes of these unified platforms, aligning with the principles of secure-by-design advocated by frameworks like \gls{saif} and SecGenAI.
Facilitating this integration within a specific hyperscale cloud platform, the \gls{aws} \gls{genai} Security Scoping Matrix serves as a practical aid for organizations utilizing \gls{aws}\cite{noauthor_securing_nodate}. This matrix is designed explicitly to help customers navigate the complexities of the \gls{srm} as it applies to \gls{genai} workloads deployed on \gls{aws}. It systematically maps common \gls{genai} architectural components and layers against key security domains\cite{noauthor_securing_nodate}. For each intersection of a \gls{genai} component and a security domain, the matrix clarifies whether the responsibility for implementing controls lies primarily with \gls{aws}, the customer, or is shared between them \cite{noauthor_securing_nodate}. This structured delineation is crucial for organizations to understand their specific security obligations when building and operating \gls{genai} systems on \gls{aws}. Furthermore, the matrix guides customers in identifying and scoping the relevant \gls{aws} security services needed to fulfill their responsibilities \cite{noauthor_securing_nodate}. It acts as a translator, converting the high-level principles of frameworks like \gls{nist} \gls{airmf} \cite{tabassi_artificial_2023} and \gls{saif} \cite{hansen_introducing_2023}, and the specific control requirements suggested by SecGenAI \cite{haryanto_secgenai_2024} or the \gls{owasp} Checklist \cite{editor_llm_nodate}, into actionable configurations and service selections within the \gls{aws} ecosystem. It directly supports the practical implementation of the \gls{srm}, a concept emphasized across multiple frameworks \cite{tabassi_artificial_2023, hansen_introducing_2023, haryanto_secgenai_2024, editor_llm_nodate}, thereby enabling organizations to effectively manage risks within their \gls{aws} environment.

In summary, these frameworks and guides offer a layered approach to \gls{genai} security. The \gls{nist} \gls{airmf} provides a foundational, risk-based structure and defines trustworthiness. Google's \gls{saif} offers a conceptual implementation path with core security elements, emphasizing adaptation and integration. The \gls{owasp} \gls{llm} \gls{coe} Guide focuses on the essential organizational and collaborative structures needed for effective governance. SecGenAI provides a detailed, integrated blueprint for securing a specific architecture, demonstrating how broader principles can be applied in context, including alignment with regional regulations. Practical tools like the \gls{owasp} \gls{llm} Checklist and provider-specific resources like the \gls{aws} Scoping Matrix aid in the MEASURE and MANAGE functions by providing concrete checks and configurations. The insights from \cite{sushil_prabhu_prabhakaran_integration_2024} remind us that these security measures must be seamlessly integrated within the complex fabric of unified \gls{ai} and cloud platforms, particularly within \gls{mlops} and automation workflows, to be effective. The recurring theme of the \gls{srm} across multiple frameworks \cite{tabassi_artificial_2023, hansen_introducing_2023, haryanto_secgenai_2024, editor_llm_nodate} highlights the collaborative nature of securing \gls{genai} in cloud environments. Collectively, these resources provide a comprehensive toolkit for organizations aiming to leverage \gls{genai} for security automation and other critical tasks on hyperscale cloud platforms, enabling them to manage risks effectively and build trustworthy \gls{ai} systems.

\subsection{Approaches for Automated Cloud Security} % (fold)
\label{sec:Approaches for Automated Cloud Security}

This subsection details specific technical approaches and architectural patterns crucial for enabling automated cloud security. It reviews research on unified platforms integrating \gls{ai} and \gls{mlops} across multi-cloud environments, techniques for automated policy orchestration in complex Kubernetes setups, and the application of digital twins for robust security policy validation. These approaches represent concrete mechanisms for realizing the potential of automation in dynamic cloud infrastructures.
For organizations operating containerized workloads across multiple clusters, particularly in multi-domain architectures involving different administrative entities, research from 2023 proposes an automated approach for generating network security policies in Kubernetes deployments\cite{bringhenti_security_2023}. Manually configuring security in such environments is complex, often leading to inconsistencies between policies defined in different clusters and requiring domain administrators to possess knowledge about other domains' configurations (like service locations or \gls{ip} addresses), which is not always feasible\cite{bringhenti_security_2023}. This approach addresses two critical challenges in multi-cluster security: reducing the configuration errors commonly made by human administrators and creating transparent cross-cluster communications without requiring extensive information sharing between domains\cite{bringhenti_security_2023}.

The proposed solution involves a top-level entity named the "Multi-Cluster Orchestrator"\cite{bringhenti_security_2023}. This orchestrator acts as a central management point, receiving inputs from managers of different domains\cite{bringhenti_security_2023}. These inputs include:
\begin{itemize}
\item A description of each domain's structure\cite{bringhenti_security_2023}.
\item High-level security requirements specifying allowed communications\cite{bringhenti_security_2023}. These requirements can be defined using an extended \gls{yaml} syntax with special labels that abstract away low-level details\cite{bringhenti_security_2023}.
\end{itemize}
Based on these inputs, the Multi-Cluster Orchestrator refines the high-level requirements into concrete configurations through a two-step process\cite{bringhenti_security_2023}:
\begin{enumerate}
\item It generates a "Global Configuration" that tracks communication pairs between services and required links between clusters, optimizing the overall cluster mesh setup\cite{bringhenti_security_2023}.
\item It derives "Single Configurations" for each individual cluster, containing the specific parameters needed to connect the cluster to the mesh, the Kubernetes Network Policies to enforce the desired security rules, and commands to create local service entries that enable transparent name resolution for services located in external clusters\cite{bringhenti_security_2023}.
\end{enumerate}
The implementation, known as Multi-Cluster Orchestrator, demonstrates how automated policy generation can improve security consistency across distributed environments while reducing the cognitive load on security administrators by handling the complexity of multi-domain interactions transparently\cite{bringhenti_security_2023}. This research is particularly relevant for hyperscale cloud platforms and organizations that utilize container orchestration technologies like Kubernetes to manage numerous workloads across multiple clusters, potentially spanning different regions, availability zones, or administrative boundaries\cite{bringhenti_security_2023}.

Another approach to security automation in the context of policies involves the use of digital twins for validating security policies before deployment in production environments\cite{hammar_digital_2023}. This approach utilizes an emulation system specifically designed to create high-fidelity digital replicas of target \gls{it} infrastructures\cite{hammar_digital_2023}. These digital twins replicate key functionalities of the corresponding physical or virtual systems, allowing security teams to play out complex security scenarios, such as intrusion attempts and defense responses, within a safe and controlled environment\cite{hammar_digital_2023}. This capability avoids impacting operational workflows on the real-world infrastructure\cite{hammar_digital_2023}.
The digital twin approach, following the research by Hammar and Stadler, enables a closed-loop learning process for crafting and refining security policies\cite{hammar_digital_2023}. It starts with generating a digital twin of the target infrastructure. This is achieved using an emulation system constructed with virtualization tools like Docker containers, alongside virtual links and switches\cite{hammar_digital_2023}. Within this digital twin, various security scenarios involving emulated attackers, defenders, and client populations are executed\cite{hammar_digital_2023}. During these runs, monitoring agents collect detailed system measurements and logs, channeling this data through pipelines for analysis\cite{hammar_digital_2023}. The gathered data and statistics are then used to build simulations\cite{hammar_digital_2023}. Reinforcement learning techniques are applied to these simulations to learn potentially optimal security policies\cite{hammar_digital_2023}. Finally, the performance of these learned policies is rigorously evaluated back in the digital twin, allowing for validation and further iteration\cite{hammar_digital_2023}.
This methodology provides continuous, iterative feedback and improvement cycles, as the results from validation can inform further scenario runs and learning phases, enhancing policy effectiveness over time\cite{hammar_digital_2023}. The authors demonstrate this by applying the approach to an intrusion response scenario, showing that the digital twin provided the necessary evaluative feedback to learn near-optimal policies that outperformed baseline systems like the SNORT \gls{idps}\cite{zhou_study_2010}. This represents a significant advancement in validation mechanisms, particularly relevant for potentially complex GenAI-driven security automation strategies, by bridging the gap between simulation-based learning and real-world applicability\cite{hammar_digital_2023}.

Regarding policies, ensuring the trustworthiness and accuracy of \gls{genai}-generated security policies and responses remains a significant challenge. The already mentioned SecGenAI framework demonstrates how advanced machine learning techniques can be combined with robust security measures to enhance the reliability of \gls{genai} systems while maintaining compliance with regulatory requirements.\cite{haryanto_secgenai_2024}
As described, this approach integrates continuous validation processes throughout the \gls{ai} lifecycle, from model development to deployment and monitoring, creating multiple checkpoints that verify the integrity and effectiveness of security responses. By emphasizing explainability alongside accuracy, the framework addresses one of the primary concerns associated with \gls{genai} applications in security contexts: the "black box" nature of complex models.\cite{haryanto_secgenai_2024}

While not specifically focused on cloud security, research on \gls{genai} applications in the energy sector offers transferable insights into implementation approaches for complex operating environments. This comprehensive literature review identifies how \gls{genai} enhances productivity through data creation, forecasting, optimization, and natural language understanding, while also addressing challenges such as hallucinations, data biases, privacy concerns, and system errors \cite{surathunmanun_exploring_2024}.
The proposed solutions including improving training data quality, implementing system fine-tuning processes, establishing human oversight mechanisms, and deploying robust security measures provide a valuable framework for \gls{genai} implementations in cloud security contexts. These approaches are particularly relevant for hyperscale environments where scale and complexity amplify both the benefits and risks of \gls{genai} adoption \cite{surathunmanun_exploring_2024}.

% subsection Approaches for Automated Cloud Security (end)

\subsection{Agent-Based Approaches} % (fold)
\label{sec:Agent-Based Approaches}

A recent paper from 2024 introduces and validates the concept of employing \gls{genai}-driven agentic workflows to achieve comprehensive security automation, particularly in complex modern environments. A notable example is the \gls{devsecops} Sentinel system\cite{pillala_devsecops_2024}, specifically designed to address the mounting security challenges inherent in modern software supply chains. Challenges coming from microservices, containerization, and cloud-native architectures that often outpace traditional \gls{devsecops} practices\cite{pillala_devsecops_2024}.

The \gls{devsecops} Sentinel system exemplifies this approach by utilizing intelligent agents integrated into automated workflows. These agents are powered by advanced \gls{genai} models, such as \glspl{llm} enhanced with \gls{rag}, enabling sophisticated analysis capabilities\cite{pillala_devsecops_2024}. Key characteristics of these agents include:

\begin{itemize}
\item \textbf{Autonomy:} Operating independently based on predefined goals and policies.
\item \textbf{Reactivity:} Responding in real-time to environmental changes like new vulnerability disclosures.
\item \textbf{Proactivity:} Taking initiative, such as preemptively scanning for risks or suggesting improvements\cite{pillala_devsecops_2024}.
\end{itemize}
These agents execute critical security tasks throughout the software development lifecycle, including:
\begin{itemize}
\item \textbf{Automated Vulnerability and Impact Analysis:} Leveraging \gls{genai} to analyze code, dependencies and infrastructure configurations for potential threats, assessing their potential impact in context\cite{pillala_devsecops_2024}.
\item \textbf{Adaptive Compliance and Release Gating:} Enforcing security policies and compliance requirements dynamically, acting as automated checks before deployment\cite{pillala_devsecops_2024}.
\item \textbf{Predictive Security:} Utilizing \gls{ai} to identify potential future risks based on historical data and emerging threat patterns\cite{pillala_devsecops_2024}.
\end{itemize}
The implementation and testing of \gls{devsecops} Sentinel demonstrate several key points relevant to broader security automation:
\begin{enumerate}
\item \textbf{Viability for Complexity:} Agentic workflows powered by \gls{genai} are shown to be a viable and effective method for tackling the intricate and rapidly evolving security issues found in modern, distributed systems\cite{pillala_devsecops_2024}.
\item \textbf{Synergy of AI and Agents:} The integration of \gls{genai}'s deep analysis capabilities with the autonomous, proactive nature of agentic systems offers a powerful paradigm for strengthening organizational security posture\cite{pillala_devsecops_2024}. While Sentinel focuses on the supply chain, the principle applies broadly to automating security operations in complex cloud environments.
\item \textbf{Measurable Improvements:} Such systems can contribute to building and deploying software that is simultaneously faster, safer, and more reliable. The \gls{devsecops} Sentinel study reported significant quantitative improvements in key security and operational metrics, including reduced \gls{mttd} and \gls{mttr} for vulnerabilities, lower false positive rates, increased compliance pass rates, higher deployment frequency, and reduced change failure rates\cite{pillala_devsecops_2024}.
\end{enumerate}

This approach, exemplified by \gls{devsecops} Sentinel, highlights a promising direction for leveraging \gls{genai} to automate and enhance security functions, moving beyond traditional limitations to offer more adaptive, context-aware, and efficient security management in demanding environments like hyperscale clouds.

% subsection Agent-Based Approaches (end)

\subsection{Security Risks} % (fold)
\label{sec:Security Risks}

The increasing integration of \gls{genai} into various domains, including cybersecurity, presents significant opportunities but also introduces complex and multifaceted risks. Insights from recent literature reviews highlight these emerging challenges. A systematic literature review by Nyoto et al. \cite{nyoto_cyber_2024}, analyzing 17 relevant studies according to \gls{prisma} 2020 guidelines \cite{page_prisma_2021}, identifies several significant cybersecurity threats stemming primarily from the irresponsible application of \gls{genai} technology. Complementing this, Surathunmanun et al. \cite{surathunmanun_exploring_2024}, while reviewing \gls{genai} in the energy sector, outline key challenges that possess direct and critical relevance to security implementations, particularly within cloud environments reliant on third-party models and data. Synthesizing these findings provides a comprehensive overview of the risks:

\begin{itemize}
\item \textbf{Enhanced Malicious Content Generation and Misuse:} \gls{genai} significantly lowers the barrier for creating sophisticated malicious content and tools. It can be abused to generate highly personalized and convincing phishing messages and social engineering tactics, increasing their effectiveness even with minimal target information \cite{nyoto_cyber_2024}. Furthermore, \gls{genai} facilitates the creation of effective ransomware and diverse forms of malware, potentially empowering individuals with limited coding expertise to launch attacks \cite{nyoto_cyber_2024}. Beyond typical malware, it can also generate other executable attack code, such as \gls{sql} injection scripts \cite{nyoto_cyber_2024}. This potential for misuse is a major concern, where uncontrolled access or improper application can lead to significant harm \cite{surathunmanun_exploring_2024}. This includes leveraging \gls{genai} to bypass security controls through techniques like prompt injection or jailbreaking, as highlighted in literature concerning \glspl{llm}. \cite{surathunmanun_exploring_2024}.

\item \textbf{Information Integrity:} \gls{genai} poses substantial risks to information integrity. It enables the creation of highly realistic deepfake audio and video content, often without clear legal frameworks or consent, leading to potential fraud, manipulation, reputational damage, and the spread of disinformation \cite{nyoto_cyber_2024}. Concurrently, \gls{genai} models are prone to generating plausible but factually incorrect or nonsensical information, known as hallucinations \cite{nyoto_cyber_2024, surathunmanun_exploring_2024}. This issue often arises from poor quality training data or suboptimal parameter settings \cite{surathunmanun_exploring_2024} and can be exacerbated by data poisoning during the model training phase \cite{nyoto_cyber_2024}. In a security context, hallucinations can manifest as faulty threat analyses, incorrect vulnerability assessments, or misleading security recommendations \cite{surathunmanun_exploring_2024}. Compounding this is the issue of data bias, where biases inherent in training data or introduced during feature selection lead to skewed or unfair outputs \cite{surathunmanun_exploring_2024}. For security applications, this could result in certain threat types being consistently overlooked or specific user groups being unfairly flagged, thereby undermining the reliability of automated systems \cite{surathunmanun_exploring_2024}. These challenges are often exacerbated by the inherent 'black box' nature of many \glspl{llm}, characterized by their complexity, lack of transparency in internal decision-making, and limited explainability, making it difficult to fully diagnose or prevent issues like hallucinations or bias \cite{dash_zero-trust_2024}.

\item \textbf{Data Privacy, Security Vulnerabilities, and Intellectual Property:} The foundation of \gls{genai} models, vast datasets, introduces significant privacy and security risks. Models are often trained on data scraped without explicit consent, potentially including sensitive personal information or copyrighted material \cite{nyoto_cyber_2024}. User interactions and prompts can also be incorporated into training data, leading to potential data leakage and privacy violations \cite{nyoto_cyber_2024}. This raises substantial intellectual property concerns and challenges compliance with regulations like \gls{gdpr} \cite{nyoto_cyber_2024}. The lack of transparency and control over how data is utilized presents considerable privacy risks \cite{nyoto_cyber_2024}. Furthermore, insecure data handling practices can create security vulnerabilities \cite{surathunmanun_exploring_2024}. Specific risks associated with \glspl{llm}, often used in cloud-hosted \gls{genai} services, include inference attacks, data extraction attacks, data poisoning, supply chain vulnerabilities \cite{surathunmanun_exploring_2024} and vulnerabilities to adversarial attacks stemming from the models complex and often opaque nature \cite{dash_zero-trust_2024}.

\item \textbf{Systemic and Operational Risks:} Beyond content generation and data issues, \gls{genai} systems can introduce operational risks. Logical inconsistencies within the model or unforeseen external events can cause \gls{genai} systems to produce errors or fail entirely \cite{surathunmanun_exploring_2024}. In automated security workflows operating in cloud environments, such errors could propagate rapidly, leading to service disruptions, critical misconfigurations, or a failure to respond effectively to genuine threats \cite{surathunmanun_exploring_2024}.
\end{itemize}

These diverse risks, spanning malicious misuse, information integrity compromises, privacy violations, intellectual property infringements, and operational failures, underscore the critical need for robust countermeasures and responsible governance. Addressing these challenges necessitates comprehensive approaches, including rigorous data governance frameworks, cross-verification of \gls{genai} outputs, continuous model monitoring and updating, incorporating human-in-the-loop validation processes, implementing strong security measures \cite{surathunmanun_exploring_2024} and architectures like Zero Trust \cite{surathunmanun_exploring_2024, dash_zero-trust_2024}, establishing clear ethical guidelines, and potentially developing new regulations specific to \gls{genai} development and deployment \cite{nyoto_cyber_2024}. Ensuring the responsible use of \gls{genai} is paramount to harnessing its benefits while mitigating the significant emerging cybersecurity challenges, particularly in sensitive contexts like cloud security where the consequences of unreliable or misused \gls{ai} can severely impact organizational risk posture and operational integrity \cite{surathunmanun_exploring_2024}.

\newpage

Another significant challenge in implementing \gls{genai} for security automation is the comprehensive identification and management of the unique risks these systems introduce, which differ significantly from traditional software risks. The \gls{nist} Artificial Intelligence Risk Management Framework (\gls{airmf}) 1.0 \cite{tabassi_artificial_2023} provides a structured, voluntary approach to address these challenges.

The \gls{airmf} defines an \gls{ai} system as an "engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments" \cite[p.1]{tabassi_artificial_2023}. It acknowledges that while \gls{ai} offers transformative potential, it also poses distinct risks due to factors like data dependency, complexity, opacity, and the socio-technical context of deployment \cite{tabassi_artificial_2023}.

In the paper, the \gls{nist} describes some key points relevant to \gls{genai} Security Risks in Cloud Computing.
\begin{enumerate}
\item \textbf{Unique \gls{ai} Risk Landscape:} The framework highlights that \gls{ai} risks differ from traditional software risks. Appendix B specifically notes challenges pertinent to \gls{genai} and cloud environments, including:
\begin{itemize}
\item Dependency on vast datasets which may harbor biases or quality issues, and are susceptible to poisoning attacks \cite{tabassi_artificial_2023}.
\item Risks associated with using pre-trained models, which can "increase levels of statistical uncertainty and cause issues with bias management, scientific validity, and reproducibility" \cite[p.38]{tabassi_artificial_2023}. This is crucial in cloud settings where models might be sourced from third parties.
\item Increased opacity and difficulty in predicting failure modes or emergent behaviors, complicating security validation \cite{tabassi_artificial_2023}. This aligns with the widely recognized 'black box' problems of \glspl{llm}, encompassing their complexity, lack of transparency, and limited explainability \cite{dash_zero-trust_2024}. Specific security concerns not fully addressed by traditional frameworks, such as "evasion, model extraction, membership inference, availability, or other machine learning attacks" \cite[p.39]{tabassi_artificial_2023}, including adversarial vulnerabilities common in \glspl{llm} \cite{dash_zero-trust_2024}.
\item Risks associated with "third-party \gls{ai} technologies, transfer learning, and off-label use," which are highly relevant when using \gls{genai} models hosted or integrated via cloud services \cite[p.39]{tabassi_artificial_2023}.
\end{itemize}

\item \textbf{Trustworthiness Characteristics:} The \gls{rmf} emphasizes achieving trustworthy \gls{ai} by balancing several characteristics \cite{tabassi_artificial_2023}. For security, the most critical are:
\begin{itemize}
\item \textbf{Secure and Resilient:} \gls{ai} systems should maintain "confidentiality, integrity, and availability" and be able to "withstand unexpected adverse events or unexpected changes" \cite[p.15]{tabassi_artificial_2023}. This includes protecting against data poisoning, adversarial examples, and model exfiltration key threats for \gls{genai}. The \gls{rmf} notes applicability of existing standards like the \gls{nist} Cybersecurity Framework here \cite[p.15]{tabassi_artificial_2023}.
\item \textbf{Accountable and Transparent:} While distinct from security, transparency and accountability are vital for security incident analysis, understanding vulnerabilities, and assigning responsibility, especially in complex cloud supply chains \cite{tabassi_artificial_2023}.
\item \textbf{Privacy-Enhanced:} \gls{genai} often processes vast amounts of data, potentially including sensitive information. Privacy risks are intertwined with security, as data breaches impact both. The \gls{rmf} advocates for privacy considerations throughout the lifecycle and mentions Privacy-Enhancing Technologies\cite{tabassi_artificial_2023}.
\item \textbf{Valid and Reliable:} Systems must perform accurately and consistently. Unreliable \gls{genai} could produce insecure code, faulty security recommendations, or fail in ways that create security openings \cite{tabassi_artificial_2023}.
\end{itemize}

\item \textbf{Risk Management Core Functions:} The \gls{rmf} outlines four functions to operationalize risk management:
\begin{itemize}
  \item \textbf{Govern:} Establishing a risk management culture, policies, accountability structures, and processes. Crucially, this includes policies addressing risks from "third-party software and data and other supply chain issues", vital for cloud-based \gls{genai} \cite[pp.21-24]{tabassi_artificial_2023}.
    \item \textbf{Map:} Establishing context, categorizing the \gls{ai} system, understanding capabilities and limitations, and mapping risks/benefits, explicitly including those from third-party components\cite{tabassi_artificial_2023}.
    \item \textbf{Measure:} Applying methods and metrics to assess risks and evaluate trustworthy characteristics, including specific evaluations for security and resilience and privacy\cite{tabassi_artificial_2023}.
    \item \textbf{Manage:} Prioritizing and responding to risks, including managing risks from third-party entities and implementing incident response and recovery plans\cite{tabassi_artificial_2023}.
\end{itemize}
\end{enumerate}

In essence, the \gls{nist} \gls{airmf} 1.0 provides a comprehensive framework that, while voluntary and high-level, guides organizations in systematically considering the multifaceted risks, including significant security and privacy challenges, inherent in developing, deploying, and using complex \gls{ai} systems like \gls{genai}, particularly within the context of third-party dependencies common in cloud computing environments. It stresses the importance of integrating risk management throughout the \gls{ai} lifecycle and addressing the unique characteristics and vulnerabilities of \gls{ai} technologies.
Adding to frameworks like the \gls{nist} \gls{airmf}, specific architectural approaches are emerging to address the unique security challenges of \gls{genai} in cloud environments. One prominent example is \gls{zta} \cite{dash_zero-trust_2024}. \gls{zta} moves away from traditional perimeter-based security towards a model where trust is never assumed, and verification is continuously required\cite{dash_zero-trust_2024}. This aligns well with the \gls{nist} \gls{rmf}'s emphasis on secure and resilient systems and proactive risk management, particularly given the "black box" nature and dynamic deployment of many \gls{genai} models \cite{dash_zero-trust_2024, tabassi_artificial_2023}. Key tenets include strict identity verification, micro-segmentation to limit lateral movement, least privilege access control, and continuous monitoring \cite{dash_zero-trust_2024}. Implementing \gls{zta} for \glspl{llm} involves specific considerations such as unified identity management across cloud platforms, \gls{ai}-driven dynamic access policies, automated network segmentation, robust data encryption and classification, continuous threat monitoring tailored to \gls{llm} vulnerabilities, and ensuring compliance \cite{dash_zero-trust_2024}. Interestingly, \gls{ai} itself can enhance \gls{zta} through behavioral analytics for continuous authentication or threat intelligence processing\cite{dash_zero-trust_2024}. However, implementing \gls{zta} effectively presents its own challenges, including complexity, integration with legacy systems, resource requirements, and potential performance impacts \cite{dash_zero-trust_2024}.

\subsection{Balance of Automation and Human Oversight} % (fold)
\label{sec:Balance of Automation and Human Oversight}

The integration of \gls{ai}, particularly \gls{genai}, into cybersecurity presents a significant paradigm shift, offering powerful automation capabilities to counter increasingly sophisticated cyber threats. A recurring theme in the literature, however, is the inherent tension between the compelling benefits derived from this automation and the indispensable necessity of human oversight \cite{seth_ai_2025}. While \gls{ai}-powered security automation provides crucial safeguards against evolving cyber dangers, the unique characteristics and potential risks associated with \gls{ai} systems, especially \gls{genai}, underscore the continued importance of human expertise and intervention \cite{seth_ai_2025, patel_generative_2025}.

A fundamental principle, strongly articulated within risk management frameworks, is that no "high-risk" \gls{ai} system should be operated without substantial human oversight \cite[p.7]{tabassi_artificial_2023}. This necessitates careful deliberation regarding whether the potential benefits of deploying such systems truly outweigh the potential negative impacts and risks \cite{tabassi_artificial_2023}. In cybersecurity contexts, high-risk applications might include automated incident response systems with the potential for disruptive countermeasures, security policy generation influencing critical infrastructure, or threat analysis tools whose outputs directly inform high-stakes decisions. The \gls{nist} \gls{airmf} emphasizes that in situations where \gls{ai} systems present unacceptable negative risk levels, such as imminent significant negative impacts or the occurrence of severe harms, their development and deployment should cease until these risks can be sufficiently managed \cite{tabassi_artificial_2023}.

Despite the promising applications of \gls{genai} for security automation such as generating security reports, suggesting code fixes, or creating configuration scripts significant challenges remain in striking the right balance between automation and appropriate human oversight. Research highlights several critical issues stemming from the use of \gls{genai} in automated security operations \cite{patel_generative_2025}. One major concern is the potential for over-dependence on \gls{ai} tools, which could lead to complacency or a degradation of human skills \cite{patel_generative_2025}. Furthermore, \gls{genai} models themselves are susceptible to adversarial risks, including data poisoning or prompt injection attacks designed to manipulate their outputs, presenting unique security challenges \cite{patel_generative_2025}. The inherent complexity and often opaque nature of decision-making processes within sophisticated \gls{ai} systems, including \gls{genai}, can also hinder effective oversight and accountability \cite{tabassi_artificial_2023} \cite{patel_generative_2025}.

Effectively managing \gls{genai} in cybersecurity demands a recognition that complete automation without human intervention introduces unacceptable risks \cite{patel_generative_2025}. Human oversight is crucial not merely as a final checkpoint but throughout the \gls{ai} lifecycle. This includes defining system goals and constraints, interpreting ambiguous or novel situations that fall outside the \gls{ai}'s training data, providing contextual understanding that the \gls{ai} may lack, and making ethical judgments, particularly when potential actions have significant consequences \cite{tabassi_artificial_2023}. The \gls{nist} \gls{airmf} emphasizes the importance of clearly defined human roles and responsibilities within human-\gls{ai} configurations, acknowledging the influence of human cognitive biases and the need for systems that are explainable and interpretable to those operating or overseeing them \cite{tabassi_artificial_2023}.

Frameworks like the \gls{nist} \gls{airmf} provide structured approaches to managing these challenges. The \texttt{GOVERN} function stresses establishing a risk management culture, defining roles, and ensuring accountability \cite[p. 21-24]{tabassi_artificial_2023}. The \texttt{MAP} function requires establishing context, understanding system limitations, and defining processes for human oversight \cite[p. 24-28]{tabassi_artificial_2023}. \texttt{MEASURE} involves ongoing monitoring of performance, safety, and fairness, incorporating feedback mechanisms \cite[p. 28-31]{tabassi_artificial_2023}. Crucially, the \texttt{MANAGE} function includes planning risk responses and implementing mechanisms to supersede, disengage, or deactivate \gls{ai} systems demonstrating performance inconsistent with intended use, alongside robust post-deployment monitoring and incident response plans \cite[p. 31-33]{tabassi_artificial_2023}.

Ultimately, the effective use of \gls{genai} in cybersecurity hinges on achieving a balanced, symbiotic relationship between automated capabilities and human expertise. This balanced approach acknowledges the complementary strengths of humans and \gls{ai}. \gls{genai} can process vast amounts of data and automate repetitive tasks at scale and speed, while humans provide critical thinking, contextual awareness, ethical guidance, and ultimate accountability \cite{patel_generative_2025}. Preventive efforts and well-planned action plans, incorporating robust human oversight mechanisms, are essential to harness the benefits of \gls{genai} for cybersecurity while mitigating its inherent risks \cite{patel_generative_2025}.

% subsection Balance of Automation and Human Oversight (end)

\subsection{Summary Literature State of the Art} % (fold)
\label{sec:Summary Literature State of the Art}

This literature review demonstrates that \gls{genai} represents a transformative technology for security automation within hyperscale cloud environments. The analysis reveals significant potential for \gls{genai} to enhance security operations through automated threat detection, policy generation, and incident response, particularly across complex multi-cloud settings. Research highlights notable advancements in conceptual frameworks for multi-cloud policy orchestration, validation mechanisms to ensure trust and accuracy, and technical approaches for implementing \gls{genai} at scale. The most promising strategies often leverage multi-cloud architectures, zero-trust principles, and comprehensive security frameworks, while necessarily acknowledging the unique infrastructure requirements of \gls{genai} itself. However, despite this progress, persistent challenges related to trust, validation, data privacy and quality, and the crucial balance between automation and human oversight remain significant considerations. As this field continues its rapid evolution, interdisciplinary collaboration will be essential to develop robust ethical norms and innovative defense mechanisms, addressing current issues while guiding the responsible application of \gls{genai} in cybersecurity.

% subsection Summary LietaState of the Art (end)

% section State of the Art (end)

\section{Research Gaps} % (fold)
\label{sec:Research Gaps}

While the literature review demonstrates significant progress in applying Generative AI to cloud security, a deeper analysis reveals several critical research gaps. The convergence of GenAI and hyperscale security automation is still in its nascent stages, and many existing studies focus on high-level frameworks or narrow applications. This thesis addresses some of the underexamined areas by exploring the following gaps.

A primary gap is the practical scalability of these solutions to a true hyperscale level. Much of the current academic literature fails to fully address the operational challenges of implementing GenAI security in enterprise environments characterized by thousands of developers and high-velocity CI/CD pipelines \cite{fu_ai_2025}. The significant computational resources required for large-scale models and the potential for latency to create bottlenecks in rapid deployment cycles present substantial operational overhead. These questions of performance and resource management at scale remain largely unexamined in academic research. This work begins to address this gap by measuring the performance of the developed prototype (as detailed in Section \ref{sec:policy-generation-speed}) and discussing the architectural trade-offs essential for a scalable implementation (Section \ref{sec:limitations}), providing a basis for future research into optimizing these systems for performance in hyperscale environments.
Furthermore, while the security risks of using \gls{genai} are well-documented (Section \ref{sec:Security Risks}), a more advanced and less-explored research area is the adversarial resilience of the automation pipeline itself \cite{nyoto_cyber_2024,dash_zero-trust_2024}. A critical gap exists in understanding and defending against attacks that manipulate a defensive \gls{genai} into generating insecure policies. For instance, a malicious actor could craft \gls{iac} designed to trigger a prompt injection, causing the \gls{llm} to produce a security policy with a hidden backdoor. There is a pressing need for research into building resilient \gls{genai} security systems capable of withstanding such targeted adversarial inputs. This thesis contributes by establishing a multi-stage Validation Layer (Section \ref{subsec:policy-layer}), which includes a "self-scan" of generated policies, as a necessary architectural component. This opens the door for future work on more advanced defenses, such as using multiple \gls{ai} models as cross-checks or developing \gls{ai}-specific threat modeling for the automation pipeline.

Another significant challenge lies in the heterogeneity and complexity of multi-cloud orchestration \cite{seth_ai_2025-1,bringhenti_security_2023-1}. Despite strong industry demand, there is limited research on practical, fully-realized GenAI solutions that can effectively orchestrate security policy across the distinct technical ecosystems of the major cloud providers. The primary difficulty is managing the heterogeneity of services, APIs, and security best practices. A truly effective GenAI system must be able to translate a single high-level security requirement into multiple, provider-specific, and contextually-aware policy implementations. While the prototype developed in this thesis focuses on a single cloud provider for the implementation (Section \ref{sec:tech_stack}), its conceptual framework is designed to be cloud-agnostic by leveraging tools like Terraform and Open Policy Agent (OPA). In doing so, this work lays the theoretical groundwork for a multi-cloud solution and highlights the specific challenges that must be solved such as building a multi-cloud RAG knowledge base thereby defining a clear roadmap for future research.

Finally, the concept of a feedback loop is central to improving AI systems, yet its application in security automation is critically underdeveloped, specifically concerning automated learning and adaptation from human feedback. While many frameworks, including the one proposed here, incorporate a Human-in-the-Loop (HITL) process, a gap exists in defining and evaluating robust mechanisms for the system to automatically learn from the human expert’s decisions \cite{nicosia_6_2024,noauthor_human---loop_nodate}. Simply having a human approve or reject a policy is a control, not a learning process. Research is needed on how to safely use this feedback to refine system prompts, update the RAG knowledge base, or even fine-tune the model to improve its accuracy and contextual understanding over time without introducing new biases or vulnerabilities \cite{tabassi_artificial_2023-1,surathunmanun_exploring_2024-1}. The conceptualization of the HITL workflow (Section \ref{sub:Human-in-the-Loop for Review and Approval}) and its integration into CI/CD pipelines (Section \ref{sec:Integration with CI/CD Pipelines for Policy-as-Code}) within this thesis creates the necessary foundation for such a feedback loop. This work identifies the critical need for this capability and proposes an architecture for how such a learning system could be implemented, setting a clear direction for subsequent research and development.

% section Research Gaps (end)

% chapter Background and Related Work (end)
