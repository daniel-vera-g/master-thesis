% Chapter 2

\chapter{Background and Related Work} % (fold)
\label{chap:Background and Related Work}

\section{Foundational Concepts in Cloud Computing} % (fold)
\label{sec:Foundational Concepts in Cloud Computing}

TBD

% section Foundational Concepts in Cloud Computing (end)

\section{Foundational Concepts in Generative AI} % (fold)
\label{sec:Foundational Concepts in Generative AI}

TBD

% section Foundational Concepts in Generative AI (end)

\section{State of Cloud Provider Ecosystems} % (fold)
\label{sec:State of Cloud Provider Ecosystems}

TBD

% section State of Cloud Provider Ecosystems (end)

\section{Literature Review} % (fold)
\label{sec:Literature Review}

The convergence of Generative Artificial Intelligence (GenAI) and hyperscale cloud platforms presents a rapidly evolving frontier for cybersecurity. As organizations increasingly rely on complex cloud environments, the scale and sophistication of threats necessitate advanced automation capabilities. GenAI offers promising avenues for enhancing security posture through intelligent automation, but its integration also introduces challenges and risks. A comprehensive understanding of the existing research landscape is therefore essential to identify established practices, emerging trends, and critical gaps in knowledge.

This literature review synthesizes current academic and industry contributions pertinent to the application of GenAI for security automation within hyperscale and multi-cloud contexts. It begins by outlining the methodology employed to select and analyze relevant works. Subsequently, the review dives into several key thematic areas: the evolution from traditional security methods to AI-driven approaches, specific frameworks for scoping and managing GenAI security implementations, architectural patterns and techniques for security automation, a critical examination of the unique security risks associated with GenAI itself, risk management frameworks and the ongoing crucial discussion regarding the necessary balance between automation and human oversight.

By examining these facets, this review aims to provide a foundational understanding of the state-of-the-art, highlighting both the potential of GenAI in cloud security automation and the significant considerations that must be addressed for its responsible and effective deployment. This synthesis will inform the subsequent research presented in this thesis.

\subsubsection{Methodology} % (fold)
\label{sec:Methodology}

This literature review followed a structured approach to identify relevant publications, focusing on peer-reviewed articles addressing GenAI applications in hyperscale cloud security published primarily within the last five years. The search utilized academic databases with key search terms related to generative AI, cloud security automation, hyperscale platforms, and multi-cloud orchestration.
Papers were selected based on their relevance to:

\begin{itemize}
\item GenAI applications specifically in cloud security contexts
\item Hyperscale or multi-cloud environments
\item Technical solutions for security automation
\item Empirical evidence or theoretical frameworks with substantial methodological rigor
\end{itemize}

The selection process involved initial screening of titles and abstracts followed by full-text review of promising papers. The analysis employed a thematic approach, identifying recurring concepts, methodological approaches, and gaps in existing research. Particular attention was paid to identifying the theoretical foundations underpinning GenAI applications in security contexts, empirical evidence of effectiveness, and limitations of current approaches.

% subsubsection Methodology (end)

\subsubsection{AI-Driven Security Approaches} % (fold)
\label{sec:AI-Driven Security Approaches}

The landscape of cloud security is undergoing a significant transformation, shifting from traditional, often reactive methods towards more proactive and adaptive strategies powered by Artificial Intelligence (AI), particularly Generative AI (GenAI). This evolution marks a move beyond basic anomaly detection towards sophisticated security postures capable of learning from and responding dynamically to novel threat vectors in complex cloud environments.

Foundational work by Khanna \cite{khanna_enhancing_2024} explores the integration of GenAI into cloud security, outlining its core applications. According to Khanna, modern GenAI implementations focus on key capabilities such as processing vast amounts of data (e.g., log entries, network packets) for advanced anomaly detection and threat intelligence, enabling automated response mechanisms that dynamically adjust security protocols, and facilitating predictive security measures to forecast potential vulnerabilities. While highlighting these advancements, Khanna also acknowledges inherent challenges, including the need for large datasets and mitigating potential adversarial manipulation \cite{khanna_enhancing_2024}.

Building upon these foundational capabilities, the integration of GenAI represents a significant leap beyond conventional rule-based security systems. Research indicates that GenAI enhances security automation, particularly within multi-cloud and hybrid architectures. It allows systems to adapt infrastructure dynamically in response to varying traffic patterns and implements AI-powered defenses against continuously evolving cyber threats. This adaptive capability directly addresses persistent challenges in cloud security related to optimizing workload distribution, ensuring performance, and managing costs effectively \cite{seth_ai_2025}.

Furthermore, recent studies underscore the practical impact of integrating GenAI with established cloud-native security tools. Patel et al. \cite{patel_generative_2025} demonstrate how layering GenAI onto platforms like AWS GuardDuty and Google Cloud Security Command Center significantly boosts automated threat detection, enables real-time incident response, and improves comprehensive vulnerability management across distributed cloud infrastructures. Their work provides empirical evidence, citing examples like Netflix and JPMorgan Chase, which reported measurable improvements in detection accuracy and notable reductions in security incidents following the adoption of GenAI-driven security automation strategies \cite{patel_generative_2025}. This convergence of GenAI with existing security frameworks highlights its potential to transform security operations centers (SOCs) by enhancing both efficiency and effectiveness.

% subsubsection AI-Driven Security Approaches (end)

\subsubsection{GenAI Security Scoping Matrix} % (fold)
\label{sec: GenAI Security Scoping Matrix}

With the introduction of the "Generative AI Security Scoping Matrix," by AWS, a structured and comprehensive framework for assessing security requirements based on the type of GenAI deployment is provided\cite{noauthor_securing_nodate}. This framework aids organizations in evaluating their security posture, identifying potential vulnerabilities, and implementing appropriate controls throughout the AI lifecycle\cite{noauthor_securing_nodate}. While core security disciplines remain essential, the matrix specifically helps address the unique risks and additional considerations introduced by generative AI workloads\cite{noauthor_securing_nodate}. It classifies implementations into five scopes, representing increasing levels of ownership and control\cite{noauthor_securing_2023}:

\begin{enumerate}
    \item \textbf{Consumer apps (Scope 1):} Utilising public third-party GenAI services (e.g., public chatbots), often with no cost or paid access, where the business does not own or see the training data or model and cannot modify it. Interaction occurs via APIs or direct application use according to provider terms\cite{noauthor_securing_2023}\cite{noauthor_securing_nodate}. This falls under the "Buying" category of GenAI usage\cite{noauthor_securing_nodate}.
    \item \textbf{Enterprise apps (Scope 2):} Employing third-party enterprise applications (e.g., Amazon Q) with embedded GenAI features, involving an established business relationship with the vendor\cite{noauthor_securing_2023}\cite{noauthor_securing_nodate}. This also falls under the "Buying" category\cite{noauthor_securing_nodate}.
    \item \textbf{Pre-trained models (Scope 3):} Building custom applications that integrate with existing third-party foundation models (e.g., Amazon Bedrock base models) via APIs\cite{noauthor_securing_2023}\cite{noauthor_securing_nodate}. This represents the start of the "Building" category\cite{noauthor_securing_nodate}.
    \item \textbf{Fine-tuned models (Scope 4):} Refining an existing third-party foundation model by fine-tuning it with business-specific data, resulting in a new, specialized model (e.g., Amazon Bedrock customized models)\cite{noauthor_securing_2023}\cite{noauthor_securing_nodate}. This is part of the "Building" category\cite{noauthor_securing_nodate}.
    \item \textbf{Self-trained models (Scope 5):} Constructing and training a GenAI model from the ground up using proprietary or acquired data, implying full ownership of the model (e.g., using Amazon SageMaker)\cite{noauthor_securing_2023}\cite{noauthor_securing_nodate}. This represents the highest level of ownership in the "Building" category\cite{noauthor_securing_nodate}.
\end{enumerate}

This matrix serves as a mental model\cite{noauthor_securing_nodate} that helps security teams prioritize focus areas by identifying five key security disciplines whose requirements vary across the deployment scopes: governance and compliance, legal and privacy, risk management, controls, and resilience\cite{noauthor_securing_2023}\cite{noauthor_securing_nodate}. As organizations move across the scopes from consuming consumer apps (Scope 1) towards building self-trained models (Scope 5), the demands within these disciplines evolve significantly. For instance, governance requirements escalate from basic compliance with terms of service to comprehensive frameworks for model development and monitoring. Similarly, risk management priorities shift, potentially focusing on prompt injection for pre-trained models (Scope 3) versus data poisoning or model extraction for fine-tuned or self-trained models (Scopes 4 and 5). The necessary security controls also transition from emphasizing access policies in lower scopes to implementing technical safeguards like adversarial testing and output filtering in higher scopes. Resilience planning also adapts based on the application's criticality. By mapping their GenAI activities to this matrix, organizations can systematically assess risks and apply appropriate security measures tailored to their specific implementation context.

\subsubsection{GenAI Security Frameworks} % (fold)
\label{sec:GenAI Security Frameworks}

A notable contribution to the field is the SecGenAI framework, which provides a comprehensive approach to securing cloud-based Generative AI (GenAI) applications, with particular attention to Retrieval-Augmented Generation (RAG) systems within the context of Australian Critical Technologies of National Interest\cite{haryanto_secgenai_2024}. This framework addresses the unique security challenges introduced by the rapid advancement of GenAI technologies\cite{haryanto_secgenai_2024}.

SecGenAI is structured around three core pillars: functional, infrastructure, and governance requirements\cite{haryanto_secgenai_2024}. It integrates an end-to-end security analysis to generate detailed specifications. These specifications emphasize critical areas such as data privacy, secure deployment methodologies, and the implementation of shared responsibility models between cloud service providers and users\cite{haryanto_secgenai_2024}. The framework's development addresses key questions surrounding GenAI security, the requirements for Confidentiality, Integrity, and Availability, specific RAG implementation options, constraints within the Australian regulatory landscape, and alignment with ethical principles\cite{haryanto_secgenai_2024}.

A key aspect of SecGenAI is its alignment with established Australian guidelines, including the Australian Privacy Principles (APP) \cite{resources_australias_2024}, particularly APP 11 concerning the security of personal information, Australia's AI Ethics Principles \cite{resources_australias_2024}, and guidance from the Australian Cyber Security Centre (ACSC) and the Digital Transformation Agency (DTA)\cite{haryanto_secgenai_2024}. This alignment ensures that security measures incorporate regulatory compliance without hindering operational efficiency\cite{haryanto_secgenai_2024}. The framework specifically targets GenAI-specific threats that often evade traditional security countermeasures. These threats include data leakage (potentially revealing sensitive training or knowledge base data), various adversarial attacks (such as prompt injection, data poisoning, and model inversion techniques designed to extract underlying data or manipulate outputs), jailbreaking attacks (bypassing content restrictions), and the potential for GenAI to generate insecure code or be misused by threat actors\cite{haryanto_secgenai_2024}.

SecGenAI proposes a multi-layered security strategy combining advanced machine learning techniques with robust security measures.

Functional Security Requirements:
\begin{itemize}
    \item \textbf{Identity and Access Management:} Utilizing continuous and adaptive authentication mechanisms, alongside Attribute-Based Access Control (ABAC), to manage user access dynamically based on behavior and context\cite{haryanto_secgenai_2024}.
    \item \textbf{Data Confidentiality and Integrity:} Employing techniques like homomorphic encryption to process encrypted data, data masking and tokenization to protect sensitive information, and data integrity verification using methods like hashing and artificial fingerprinting\cite{haryanto_secgenai_2024}.
    \item \textbf{Model Security:} Implementing adversarial attack mitigation, encrypting model parameters, and ensuring secure model training protocols, potentially using differential privacy or federated learning\cite{haryanto_secgenai_2024}.
\end{itemize}

Infrastructure Security Requirements:
\begin{itemize}
    \item Implementing sandboxed environments (e.g., using containerization or virtualization) often within dedicated cloud availability zones and virtual private networks\cite{haryanto_secgenai_2024}.
    \item Securing database connections using read replicas, strict IAM policies, and robust encryption methods (e.g., AWS KMS or CloudHSM)\cite{haryanto_secgenai_2024}.
    \item Establishing stringent network security settings, segregating internal and external connections, and utilizing security groups\cite{haryanto_secgenai_2024}.
    \item Deploying external attack prevention mechanisms like Web Application Firewalls (WAF) and DDoS mitigation services, potentially integrated with data processing and monitoring tools (e.g., AWS Kinesis, Glue, Athena, Grafana)\cite{haryanto_secgenai_2024}.
    \item Ensuring robust data backup and disaster recovery strategies, considering Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO)\cite{haryanto_secgenai_2024}.
\end{itemize}

Governance Requirements:
\begin{itemize}
    \item Adhering to AI governance principles (based on the ISO 38500 Evaluate-Direct-Monitor cycle \cite{noauthor_isoiec_nodate}) covering fairness, accountability, content traceability (e.g., watermarking), data protection, regular audits, reliability, user consent, third-party risk management, transparency, and legal compliance\cite{haryanto_secgenai_2024}.
    \item Clearly defining roles and responsibilities through an AI-specific Shared Responsibility Model (SRM) for cloud environments, outlining obligations for Cloud Service Providers (CSPs), customers, and areas of shared duty\cite{haryanto_secgenai_2024}.
\end{itemize}

By addressing these functional, infrastructure, and governance aspects in detail, the SecGenAI framework provides actionable strategies for the secure implementation and operation of cloud-based GenAI systems, fostering innovation while safeguarding national interests and enhancing the overall reliability and trustworthiness of these transformative technologies\cite{haryanto_secgenai_2024}.

As organizations increasingly adopt multi-cloud strategies, effective policy orchestration across diverse environments becomes critical for maintaining consistent security postures and operational efficiency\cite{sushil_prabhu_prabhakaran_integration_2024}. A comprehensive analysis of unified AI and cloud platforms published in 2024 examines architectural frameworks and integration patterns that enable the convergence of AI tools, machine learning operations (MLOps), data processing systems, and workflow orchestration within cloud-native environments, primarily focusing on transforming process automation and decision systems\cite{sushil_prabhu_prabhakaran_integration_2024}. This research investigates how these unified platforms address challenges like managing distributed AI workloads, ensuring real-time processing, and maintaining regulatory compliance\cite{sushil_prabhu_prabhakaran_integration_2024}.

This research identifies three key innovations within these unified platforms with significant implications for advanced automation, including security automation:
\begin{itemize}
    \item \textbf{Federated AI implementations:} These allow organizations to train AI models across distributed nodes or cloud boundaries while preserving data sovereignty and privacy, as sensitive data does not need to be centralized. This approach also reduces network bandwidth requirements and utilizes techniques like secure aggregation protocols and differential privacy\cite{sushil_prabhu_prabhakaran_integration_2024}.
    \item \textbf{Real-time data processing architectures:} Leveraging advanced streaming technologies (like Apache Kafka or Flink), in-memory processing, and real-time analytics engines, these architectures enable sub-second decision-making and immediate response to events (such as potential threats) by efficiently handling continuous data streams with high reliability and fault tolerance\cite{sushil_prabhu_prabhakaran_integration_2024}.
    \item \textbf{Multi-cloud integration patterns:} These patterns establish standardized interfaces, communication protocols, service discovery mechanisms, load balancing, and security controls to ensure seamless and consistent operation, including policy enforcement, across different cloud providers. Hybrid cloud deployment strategies are also highlighted, intelligently distributing workloads between on-premise and cloud resources based on performance, cost, and compliance needs, managed by sophisticated orchestration\cite{sushil_prabhu_prabhakaran_integration_2024}.
\end{itemize}

These architectural approaches, integrating MLOps frameworks for lifecycle management, robust data processing, workflow orchestration, and advanced capabilities like federated learning and real-time analytics within a multi-cloud context, provide the necessary foundation upon which advanced solutions like GenAI-driven security automation can be built across heterogeneous cloud environments\cite{sushil_prabhu_prabhakaran_integration_2024}. While the analyzed paper focuses broadly on process automation and decision systems, the detailed exploration of scalable, resilient, governable, and interoperable architectures directly supports the implementation of sophisticated, automated security measures in complex multi-cloud settings\cite{sushil_prabhu_prabhakaran_integration_2024}.

For organizations operating containerized workloads across multiple clusters, particularly in multi-domain architectures involving different administrative entities, research from 2023 proposes an automated approach for generating network security policies in Kubernetes deployments\cite{bringhenti_security_2023}. Manually configuring security in such environments is complex, often leading to inconsistencies between policies defined in different clusters and requiring domain administrators to possess knowledge about other domains' configurations (like service locations or IP addresses), which is not always feasible\cite{bringhenti_security_2023}. This approach addresses two critical challenges in multi-cluster security: reducing the configuration errors commonly made by human administrators and creating transparent cross-cluster communications without requiring extensive information sharing between domains\cite{bringhenti_security_2023}.

The proposed solution involves a top-level entity named the "Multi-Cluster Orchestrator"\cite{bringhenti_security_2023}. This orchestrator acts as a central management point, receiving inputs from managers of different domains\cite{bringhenti_security_2023}. These inputs include:
\begin{itemize}
    \item A description of each domain's structure (listing clusters and exposed services with their details)\cite{bringhenti_security_2023}.
    \item High-level security requirements specifying allowed communications (e.g., between services within the same domain, services in different domains, or services and external IP addresses)\cite{bringhenti_security_2023}. These requirements can be defined using an extended YAML syntax with special labels (`service`, `cluster`, `domain`) that abstract away low-level details\cite{bringhenti_security_2023}.
\end{itemize}

Based on these inputs, the Multi-Cluster Orchestrator refines the high-level requirements into concrete configurations through a two-step process\cite{bringhenti_security_2023}:
\begin{enumerate}
    \item It generates a "Global Configuration" that tracks communication pairs between services and required links between clusters, optimizing the overall cluster mesh setup\cite{bringhenti_security_2023}.
    \item It derives "Single Configurations" for each individual cluster, containing the specific parameters needed to connect the cluster to the mesh (e.g., using technologies like Cilium Cluster Mesh), the Kubernetes Network Policies to enforce the desired security rules, and commands to create local service entries that enable transparent name resolution for services located in external clusters\cite{bringhenti_security_2023}.
\end{enumerate}

The implementation, known as Multi-Cluster Orchestrator (developed in Java with REST APIs), demonstrates how automated policy generation can improve security consistency across distributed environments while reducing the cognitive load on security administrators by handling the complexity of multi-domain interactions transparently\cite{bringhenti_security_2023}. This research is particularly relevant for hyperscale cloud platforms and organizations that utilize container orchestration technologies like Kubernetes to manage numerous workloads across multiple clusters, potentially spanning different regions, availability zones, or administrative boundaries\cite{bringhenti_security_2023}.

Another approach to security automation in the context of policies involves the use of digital twins for validating security policies before deployment in production environments\cite{hammar_digital_2023}. This approach utilizes an emulation system specifically designed to create high-fidelity digital replicas of target IT infrastructures\cite{hammar_digital_2023}. These digital twins replicate key functionalities of the corresponding physical or virtual systems, allowing security teams to play out complex security scenarios, such as intrusion attempts and defense responses, within a safe and controlled environment\cite{hammar_digital_2023}. This capability avoids impacting operational workflows on the real-world infrastructure\cite{hammar_digital_2023}.

The digital twin approach, as detailed in the research by Hammar and Stadler, facilitates a closed-loop learning process for developing and refining security policies\cite{hammar_digital_2023}. The process involves several key steps:
\begin{enumerate}
    \item \textbf{Create Digital Twin:} A digital twin of the target infrastructure is generated using an emulation system built on virtualization technologies like Docker containers, virtual links, and virtual switches\cite{hammar_digital_2023}.
    \item \textbf{Run Scenarios \& Collect Data:} Security scenarios, involving emulated attackers, defenders, and client populations, are executed within the digital twin\cite{hammar_digital_2023}. During these runs, detailed system measurements and logs are collected via monitoring agents that push metrics to data pipelines (e.g., using Kafka and Spark)\cite{hammar_digital_2023}.
    \item \textbf{Model \& Learn:} The collected data and statistics are used to instantiate simulations, often modeled as Markov decision processes \cite{hammar_digital_2023}. Reinforcement learning techniques are then applied to these simulations to learn potentially optimal security policies\cite{hammar_digital_2023}.
    \item \textbf{Validate \& Iterate:} The performance of the learned policies is then rigorously evaluated back within the high-fidelity digital twin environment\cite{hammar_digital_2023}.
\end{enumerate}

This methodology provides continuous, iterative feedback and improvement cycles, as the results from validation can inform further scenario runs and learning phases, enhancing policy effectiveness over time\cite{hammar_digital_2023}. The authors demonstrate this by applying the approach to an intrusion response scenario, showing that the digital twin provided the necessary evaluative feedback to learn near-optimal policies that outperformed baseline systems like the SNORT IDPS\cite{zhou_study_2010}. This represents a significant advancement in validation mechanisms, particularly relevant for potentially complex GenAI-driven security automation strategies, by bridging the gap between simulation-based learning and real-world applicability\cite{hammar_digital_2023}.

Regarding policies, ensuring the trustworthiness and accuracy of GenAI-generated security policies and responses remains a significant challenge. The already mentioned SecGenAI framework demonstrates how advanced machine learning techniques can be combined with robust security measures to enhance the reliability of GenAI systems while maintaining compliance with regulatory requirements.\cite{haryanto_secgenai_2024}
As described, this approach integrates continuous validation processes throughout the AI lifecycle, from model development to deployment and monitoring, creating multiple checkpoints that verify the integrity and effectiveness of security responses. By emphasizing explainability alongside accuracy, the framework addresses one of the primary concerns associated with GenAI applications in security contexts: the "black box" nature of complex models.\cite{haryanto_secgenai_2024}

While not specifically focused on cloud security, research on GenAI applications in the energy sector offers transferable insights into implementation approaches for complex operating environments. This comprehensive literature review identifies how GenAI enhances productivity through data creation, forecasting, optimization, and natural language understanding, while also addressing challenges such as hallucinations, data biases, privacy concerns, and system errors \cite{surathunmanun_exploring_2024}.
The proposed solutions—including improving training data quality, implementing system fine-tuning processes, establishing human oversight mechanisms, and deploying robust security measures—provide a valuable framework for GenAI implementations in cloud security contexts. These approaches are particularly relevant for hyperscale environments where scale and complexity amplify both the benefits and risks of GenAI adoption \cite{surathunmanun_exploring_2024}.

% subsubsection GenAI Security Frameworks (end)

\subsubsection{Agent-Based Approaches} % (fold)
\label{sec:Agent-Based Approaches}

A recent paper from 2024 introduces and validates the concept of employing Generative AI (GenAI)-driven agentic workflows to achieve comprehensive security automation, particularly in complex modern environments. A notable example is the DevSecOps Sentinel system\cite{noauthor_devsecops_nodate}, specifically designed to address the mounting security challenges inherent in modern software supply chains. Challenges coming from microservices, containerization, and cloud-native architectures that often outpace traditional DevSecOps practices\cite{noauthor_devsecops_nodate}.

The DevSecOps Sentinel system exemplifies this approach by utilizing intelligent agents integrated into automated workflows. These agents are powered by advanced GenAI models, such as Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG), enabling sophisticated analysis capabilities\cite{noauthor_devsecops_nodate}. Key characteristics of these agents include:

\begin{itemize}
    \item \textbf{Autonomy:} Operating independently based on predefined goals and policies.
    \item \textbf{Reactivity:} Responding in real-time to environmental changes like new vulnerability disclosures.
    \item \textbf{Proactivity:} Taking initiative, such as preemptively scanning for risks or suggesting improvements\cite{noauthor_devsecops_nodate}.
\end{itemize}

These agents execute critical security tasks throughout the software development lifecycle, including:

\begin{itemize}
    \item \textbf{Automated Vulnerability and Impact Analysis:} Leveraging GenAI to analyze code, dependencies (tracked via SBOMs), and infrastructure configurations for potential threats, assessing their potential impact in context\cite{noauthor_devsecops_nodate}.
    \item \textbf{Adaptive Compliance and Release Gating:} Enforcing security policies and compliance requirements dynamically, acting as automated checks before deployment\cite{noauthor_devsecops_nodate}.
    \item \textbf{Predictive Security:} Utilizing AI to identify potential future risks based on historical data and emerging threat patterns\cite{noauthor_devsecops_nodate}.
\end{itemize}

The implementation and testing of DevSecOps Sentinel demonstrate several key points relevant to broader security automation:

\begin{enumerate}
    \item \textbf{Viability for Complexity:} Agentic workflows powered by GenAI are shown to be a viable and effective method for tackling the intricate and rapidly evolving security issues found in modern, distributed systems\cite{noauthor_devsecops_nodate}.
    \item \textbf{Synergy of AI and Agents:} The integration of GenAI's deep analysis capabilities with the autonomous, proactive nature of agentic systems offers a powerful paradigm for strengthening organizational security posture\cite{noauthor_devsecops_nodate}. While Sentinel focuses on the supply chain, the principle applies broadly to automating security operations in complex cloud environments.
    \item \textbf{Measurable Improvements:} Such systems can contribute to building and deploying software that is simultaneously faster, safer, and more reliable. The DevSecOps Sentinel study reported significant quantitative improvements in key security and operational metrics, including reduced Mean Time to Detect (MTTD) and Resolve (MTTR) for vulnerabilities, lower false positive rates, increased compliance pass rates, higher deployment frequency, and reduced change failure rates\cite{noauthor_devsecops_nodate}.
\end{enumerate}

This approach, exemplified by DevSecOps Sentinel, highlights a promising direction for leveraging GenAI to automate and enhance security functions, moving beyond traditional limitations to offer more adaptive, context-aware, and efficient security management in demanding environments like hyperscale clouds.

% subsubsection Agent-Based Approaches (end)

\subsubsection{Security Risks} % (fold)
\label{sec:Security Risks}

The increasing integration of Generative Artificial Intelligence (GenAI) into various domains, including cybersecurity, presents significant opportunities but also introduces complex and multifaceted risks. Insights from recent literature reviews highlight these emerging challenges. A systematic literature review by Nyoto et al. \cite{nyoto_cyber_2024}, analyzing 17 relevant studies according to PRISMA 2020 guidelines \cite{page_prisma_2021}, identifies several significant cybersecurity threats stemming primarily from the irresponsible application of GenAI technology. Complementing this, Surathunmanun et al. \cite{surathunmanun_exploring_2024}, while reviewing GenAI in the energy sector, outline key challenges that possess direct and critical relevance to security implementations, particularly within cloud environments reliant on third-party models and data. Synthesizing these findings provides a comprehensive overview of the risks:

\begin{itemize}
    \item \textbf{Enhanced Malicious Content Generation and Misuse:} GenAI significantly lowers the barrier for creating sophisticated malicious content and tools. It can be abused to generate highly personalized and convincing phishing messages and social engineering tactics, increasing their effectiveness even with minimal target information \cite{nyoto_cyber_2024}. Furthermore, GenAI facilitates the creation of effective ransomware and diverse forms of malware, potentially empowering individuals with limited coding expertise to launch attacks \cite{nyoto_cyber_2024}. Beyond typical malware, it can also generate other executable attack code, such as SQL injection scripts \cite{nyoto_cyber_2024}. This potential for misuse is a major concern, where uncontrolled access or improper application can lead to significant harm \cite{surathunmanun_exploring_2024}. This includes leveraging GenAI to bypass security controls through techniques like prompt injection or jailbreaking, as highlighted in literature concerning Large Language Models. \cite{surathunmanun_exploring_2024}.

    \item \textbf{Information Integrity:} GenAI poses substantial risks to information integrity. It enables the creation of highly realistic deepfake audio and video content, often without clear legal frameworks or consent, leading to potential fraud, manipulation, reputational damage, and the spread of disinformation \cite{nyoto_cyber_2024}. Concurrently, GenAI models are prone to generating plausible but factually incorrect or nonsensical information, known as hallucinations \cite{nyoto_cyber_2024, surathunmanun_exploring_2024}. This issue often arises from poor quality training data or suboptimal parameter settings \cite{surathunmanun_exploring_2024} and can be exacerbated by data poisoning during the model training phase \cite{nyoto_cyber_2024}. In a security context, hallucinations can manifest as faulty threat analyses, incorrect vulnerability assessments, or misleading security recommendations \cite{surathunmanun_exploring_2024}. Compounding this is the issue of data bias, where biases inherent in training data or introduced during feature selection lead to skewed or unfair outputs \cite{surathunmanun_exploring_2024}. For security applications, this could result in certain threat types being consistently overlooked or specific user groups being unfairly flagged, thereby undermining the reliability of automated systems \cite{surathunmanun_exploring_2024}. These challenges are often exacerbated by the inherent 'black box' nature of many LLMs, characterized by their complexity, lack of transparency in internal decision-making, and limited explainability, making it difficult to fully diagnose or prevent issues like hallucinations or bias \cite{noauthor_zero-trust_nodate}.

    \item \textbf{Data Privacy, Security Vulnerabilities, and Intellectual Property:} The foundation of GenAI models—vast datasets—introduces significant privacy and security risks. Models are often trained on data scraped without explicit consent, potentially including sensitive personal information or copyrighted material \cite{nyoto_cyber_2024}. User interactions and prompts can also be incorporated into training data, leading to potential data leakage and privacy violations \cite{nyoto_cyber_2024}. This raises substantial intellectual property concerns and challenges compliance with regulations like GDPR \cite{nyoto_cyber_2024}. The lack of transparency and control over how data is utilized presents considerable privacy risks \cite{nyoto_cyber_2024}. Furthermore, insecure data handling practices can create security vulnerabilities \cite{surathunmanun_exploring_2024}. Specific risks associated with LLMs, often used in cloud-hosted GenAI services, include inference attacks, data extraction attacks, data poisoning, supply chain vulnerabilities \cite{surathunmanun_exploring_2024} and vulnerabilities to adversarial attacks stemming from the models complex and often opaque nature \cite{noauthor_zero-trust_nodate}.

    \item \textbf{Systemic and Operational Risks:} Beyond content generation and data issues, GenAI systems can introduce operational risks. Logical inconsistencies within the model or unforeseen external events can cause GenAI systems to produce errors or fail entirely \cite{surathunmanun_exploring_2024}. In automated security workflows operating in cloud environments (e.g., incident response, configuration management), such errors could propagate rapidly, leading to service disruptions, critical misconfigurations, or a failure to respond effectively to genuine threats \cite{surathunmanun_exploring_2024}.
\end{itemize}

These diverse risks, spanning malicious misuse, information integrity compromises, privacy violations, intellectual property infringements, and operational failures, underscore the critical need for robust countermeasures and responsible governance. Addressing these challenges necessitates comprehensive approaches, including rigorous data governance frameworks, cross-verification of GenAI outputs, continuous model monitoring and updating, incorporating human-in-the-loop validation processes, implementing strong security measures \cite{surathunmanun_exploring_2024} and architectures like Zero Trust \cite{surathunmanun_exploring_2024, noauthor_zero-trust_nodate}, establishing clear ethical guidelines, and potentially developing new regulations specific to GenAI development and deployment \cite{nyoto_cyber_2024}. Ensuring the responsible use of GenAI is paramount to harnessing its benefits while mitigating the significant emerging cybersecurity challenges, particularly in sensitive contexts like cloud security where the consequences of unreliable or misused AI can severely impact organizational risk posture and operational integrity \cite{surathunmanun_exploring_2024}.

\newpage

Another significant challenge in implementing GenAI for security automation is the comprehensive identification and management of the unique risks these systems introduce, which differ significantly from traditional software risks. The NIST Artificial Intelligence Risk Management Framework (AI RMF 1.0) \cite{tabassi_artificial_2023} provides a structured, voluntary approach to address these challenges.

The AI RMF defines an AI system as an "engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments" \cite[p.1]{tabassi_artificial_2023}. It acknowledges that while AI offers transformative potential, it also poses distinct risks due to factors like data dependency, complexity, opacity, and the socio-technical context of deployment \cite{tabassi_artificial_2023}.

In the paper, the NIST describes some key points relevant to GenAI Security Risks in Cloud Computing.

\begin{enumerate}
    \item \textbf{Unique AI Risk Landscape:} The framework highlights that AI risks differ from traditional software risks. Appendix B specifically notes challenges pertinent to GenAI and cloud environments, including:
    \begin{itemize}
        \item Dependency on vast datasets which may harbor biases or quality issues, and are susceptible to poisoning attacks \cite{tabassi_artificial_2023}.
        \item Risks associated with using pre-trained models, which can "increase levels of statistical uncertainty and cause issues with bias management, scientific validity, and reproducibility" \cite[p.38]{tabassi_artificial_2023}. This is crucial in cloud settings where models might be sourced from third parties.
        \item Increased opacity and difficulty in predicting failure modes or emergent behaviors, complicating security validation \cite{tabassi_artificial_2023}. This aligns with the widely recognized 'black box' problems of LLMs, encompassing their complexity, lack of transparency, and limited explainability \cite{noauthor_zero-trust_nodate}. Specific security concerns not fully addressed by traditional frameworks, such as "evasion, model extraction, membership inference, availability, or other machine learning attacks" \cite[p.39]{tabassi_artificial_2023}, including adversarial vulnerabilities common in LLMs \cite{noauthor_zero-trust_nodate}.
        \item Specific security concerns not fully addressed by traditional frameworks, such as "evasion, model extraction, membership inference, availability, or other machine learning attacks" \cite[p.39]{tabassi_artificial_2023}.
        \item Risks associated with "third-party AI technologies, transfer learning, and off-label use," which are highly relevant when using GenAI models hosted or integrated via cloud services \cite[p.39]{tabassi_artificial_2023}.
    \end{itemize}

    \item \textbf{Trustworthiness Characteristics:} The RMF emphasizes achieving trustworthy AI by balancing several characteristics \cite{tabassi_artificial_2023}. For security, the most critical are:
    \begin{itemize}
      \item \textbf{Secure and Resilient:} AI systems should maintain "confidentiality, integrity, and availability" and be able to "withstand unexpected adverse events or unexpected changes" \cite[p.15]{tabassi_artificial_2023}. This includes protecting against data poisoning, adversarial examples, and model exfiltration – key threats for GenAI. The RMF notes applicability of existing standards like the NIST Cybersecurity Framework here \cite[p.15]{tabassi_artificial_2023}.
        \item \textbf{Accountable and Transparent:} While distinct from security, transparency and accountability are vital for security incident analysis, understanding vulnerabilities, and assigning responsibility, especially in complex cloud supply chains \cite{tabassi_artificial_2023}.
        \item \textbf{Privacy-Enhanced:} GenAI often processes vast amounts of data, potentially including sensitive information. Privacy risks are intertwined with security, as data breaches impact both. The RMF advocates for privacy considerations throughout the lifecycle and mentions Privacy-Enhancing Technologies\cite{tabassi_artificial_2023}.
        \item \textbf{Valid and Reliable:} Systems must perform accurately and consistently. Unreliable GenAI could produce insecure code, faulty security recommendations, or fail in ways that create security openings \cite{tabassi_artificial_2023}.
    \end{itemize}

    \item \textbf{Risk Management Core Functions:} The RMF outlines four functions to operationalize risk management:
    \begin{itemize}
      \item \textbf{Govern:} Establishing a risk management culture, policies, accountability structures, and processes. Crucially, this includes policies addressing risks from "third-party software and data and other supply chain issues", vital for cloud-based GenAI \cite[pp.21-24]{tabassi_artificial_2023}.
        \item \textbf{Map:} Establishing context, categorizing the AI system, understanding capabilities and limitations, and mapping risks/benefits, explicitly including those from third-party components\cite{tabassi_artificial_2023}.
        \item \textbf{Measure:} Applying methods and metrics to assess risks and evaluate trustworthy characteristics, including specific evaluations for security and resilience and privacy\cite{tabassi_artificial_2023}.
        \item \textbf{Manage:} Prioritizing and responding to risks, including managing risks from third-party entities and implementing incident response and recovery plans\cite{tabassi_artificial_2023}.
    \end{itemize}
\end{enumerate}

In essence, the NIST AI RMF 1.0 provides a comprehensive framework that, while voluntary and high-level, guides organizations in systematically considering the multifaceted risks, including significant security and privacy challenges, inherent in developing, deploying, and using complex AI systems like GenAI, particularly within the context of third-party dependencies common in cloud computing environments. It stresses the importance of integrating risk management throughout the AI lifecycle and addressing the unique characteristics and vulnerabilities of AI technologies.

Complementing overarching frameworks like the NIST AI RMF, specific architectural approaches are emerging to address the unique security challenges of GenAI in cloud environments. One prominent example is Zero Trust Architecture (ZTA) \cite{noauthor_zero-trust_nodate}. ZTA moves away from traditional perimeter-based security towards a model where trust is never assumed, and verification is continuously required\cite{noauthor_zero-trust_nodate}. This aligns well with the NIST RMF's emphasis on secure and resilient systems and proactive risk management, particularly given the 'black box' nature and dynamic deployment of many GenAI models \cite{noauthor_zero-trust_nodate, tabassi_artificial_2023}. Key tenets include strict identity verification, micro-segmentation to limit lateral movement, least privilege access control, and continuous monitoring \cite{noauthor_zero-trust_nodate}. Implementing ZTA for LLMs involves specific considerations such as unified identity management across cloud platforms, AI-driven dynamic access policies, automated network segmentation, robust data encryption and classification, continuous threat monitoring tailored to LLM vulnerabilities, and ensuring compliance \cite{noauthor_zero-trust_nodate}. Interestingly, AI itself can enhance ZTA through behavioral analytics for continuous authentication or threat intelligence processing\cite{noauthor_zero-trust_nodate}. However, implementing ZTA effectively presents its own challenges, including complexity, integration with legacy systems, resource requirements, and potential performance impacts \cite{noauthor_zero-trust_nodate}.

\subsubsection{Balance of Automation and Human Oversight} % (fold)
\label{sec:Balance of Automation and Human Oversight}

The integration of Artificial Intelligence (AI), particularly Generative AI (GenAI), into cybersecurity presents a significant paradigm shift, offering powerful automation capabilities to counter increasingly sophisticated cyber threats. A recurring theme in the literature, however, is the inherent tension between the compelling benefits derived from this automation and the indispensable necessity of human oversight \cite{seth_ai_2025}. While AI-powered security automation provides crucial safeguards against evolving cyber dangers, the unique characteristics and potential risks associated with AI systems, especially GenAI, underscore the continued importance of human expertise and intervention \cite{seth_ai_2025, patel_generative_2025}.

A fundamental principle, strongly articulated within risk management frameworks, is that no "high-risk" AI system should be operated without substantial human oversight \cite[p.7]{tabassi_artificial_2023}. This necessitates careful deliberation regarding whether the potential benefits of deploying such systems truly outweigh the potential negative impacts and risks \cite{tabassi_artificial_2023}. In cybersecurity contexts, high-risk applications might include automated incident response systems with the potential for disruptive countermeasures, security policy generation influencing critical infrastructure, or threat analysis tools whose outputs directly inform high-stakes decisions. The NIST AI Risk Management Framework (AI RMF) emphasizes that in situations where AI systems present unacceptable negative risk levels, such as imminent significant negative impacts or the occurrence of severe harms, their development and deployment should cease until these risks can be sufficiently managed \cite{tabassi_artificial_2023}.

Despite the promising applications of GenAI for security automation—such as generating security reports, suggesting code fixes, or creating configuration scripts significant challenges remain in striking the right balance between automation and appropriate human oversight. Research highlights several critical issues stemming from the use of GenAI in automated security operations \cite{patel_generative_2025}. One major concern is the potential for over-dependence on AI tools, which could lead to complacency or a degradation of human skills \cite{patel_generative_2025}. Furthermore, GenAI models themselves are susceptible to adversarial risks, including data poisoning or prompt injection attacks designed to manipulate their outputs, presenting unique security challenges \cite{patel_generative_2025}. The inherent complexity and often opaque nature of decision-making processes within sophisticated AI systems, including GenAI, can also hinder effective oversight and accountability \cite{tabassi_artificial_2023} \cite{patel_generative_2025}.

Effectively managing GenAI in cybersecurity demands a recognition that complete automation without human intervention introduces unacceptable risks \cite{patel_generative_2025}. Human oversight is crucial not merely as a final checkpoint but throughout the AI lifecycle. This includes defining system goals and constraints, interpreting ambiguous or novel situations that fall outside the AI's training data, providing contextual understanding that the AI may lack, and making ethical judgments, particularly when potential actions have significant consequences \cite{tabassi_artificial_2023}. The NIST AI RMF emphasizes the importance of clearly defined human roles and responsibilities within human-AI configurations, acknowledging the influence of human cognitive biases and the need for systems that are explainable and interpretable to those operating or overseeing them \cite{tabassi_artificial_2023}.

Frameworks like the NIST AI RMF provide structured approaches to managing these challenges. The \texttt{GOVERN} function stresses establishing a risk management culture, defining roles, and ensuring accountability \cite[p. 21-24]{tabassi_artificial_2023}. The \texttt{MAP} function requires establishing context, understanding system limitations, and defining processes for human oversight \cite[p. 24-28]{tabassi_artificial_2023}. \texttt{MEASURE} involves ongoing monitoring of performance, safety, and fairness, incorporating feedback mechanisms \cite[p. 28-31]{tabassi_artificial_2023}. Crucially, the \texttt{MANAGE} function includes planning risk responses and implementing mechanisms to supersede, disengage, or deactivate AI systems demonstrating performance inconsistent with intended use, alongside robust post-deployment monitoring and incident response plans \cite[p. 31-33]{tabassi_artificial_2023}.

Ultimately, the effective use of GenAI in cybersecurity hinges on achieving a balanced, symbiotic relationship between automated capabilities and human expertise. This balanced approach acknowledges the complementary strengths of humans and AI. GenAI can process vast amounts of data and automate repetitive tasks at scale and speed, while humans provide critical thinking, contextual awareness, ethical guidance, and ultimate accountability \cite{patel_generative_2025}. Preventive efforts and well-planned action plans, incorporating robust human oversight mechanisms, are essential to harness the benefits of GenAI for cybersecurity while mitigating its inherent risks \cite{patel_generative_2025}.

% subsubsection Balance of Automation and Human Oversight (end)

% section Literature Review (end)

\subsubsection{Summary Literature review} % (fold)
\label{sec:Summary Literature review}

This literature review demonstrates that Generative AI (GenAI) represents a transformative technology for security automation within hyperscale cloud environments. The analysis reveals significant potential for GenAI to enhance security operations through automated threat detection, policy generation, and incident response, particularly across complex multi-cloud settings. Research highlights notable advancements in conceptual frameworks for multi-cloud policy orchestration, validation mechanisms to ensure trust and accuracy, and technical approaches for implementing GenAI at scale. The most promising strategies often leverage multi-cloud architectures, zero-trust principles, and comprehensive security frameworks, while necessarily acknowledging the unique infrastructure requirements of GenAI itself. However, despite this progress, persistent challenges related to trust, validation, data privacy and quality, and the crucial balance between automation and human oversight remain significant considerations. As this field continues its rapid evolution, interdisciplinary collaboration will be essential to develop robust ethical norms and innovative defense mechanisms, addressing current issues while guiding the responsible application of GenAI in cybersecurity.

% subsubsection Summary Literature review (end)


\section{Research Gaps} % (fold)
\label{sec:Research Gaps}

% Enhanced validation mechanisms: Developing more robust techniques for verifying the accuracy and reliability of GenAI security decisions, moving beyond current red-teaming approaches

% Feffer et al., “Red-Teaming for Generative AI.”

% Cross-platform orchestration: Creating unified frameworks for consistent security policy application across diverse cloud environments

% Vootkuri, “Multi-Cloud Data Strategy Security for Generative AI.”

% Domain-specific LLMs for security: Exploring purpose-built language models optimized for security applications rather than general-purpose models

% Energy-efficient security operations: Developing approaches that balance computational demands with sustainability concerns, particularly for inference operations

% Multi-disciplinary approaches: Bridging the gap between scientific developments and ethical considerations through collaborative research involving computer science, law, ethics, and policy-making experts

% Yigit et al., “Review of Generative AI Methods in Cybersecurity.”

% Standardized Evaluation Frameworks
% The analysis of current literature reveals a significant need for standardized frameworks to evaluate the effectiveness and security of GenAI-driven automation in hyperscale cloud environments. Future research should focus on developing metrics and methodologies that enable consistent assessment of GenAI implementations across different cloud providers and security contexts.
% Hybrid Security Approaches
% Promising directions for future research include the investigation of hybrid approaches that combine GenAI with traditional security methods to leverage the strengths of both paradigms. These hybrid models could provide the adaptability and pattern recognition capabilities of GenAI while maintaining the explainability and predictability of rule-based systems in critical security functions.

% Explainable AI for Security Operations
% Research on explainable AI approaches specifically tailored to security operations could increase transparency and trust in GenAI-generated security policies and decisions. This focus area is particularly important for regulatory compliance and stakeholder confidence in automated security systems.

% section Research Gaps (end)

% chapter Background and Related Work (end)
