\chapter{Implementation & System Architecture}

\section{Chapter Overview}
% State the chapter's goal, list the two codebases, and explain how the section ties back to the theoretical framework presented earlier.

\section{Design Objectives & Functional Requirements}
% Summarise the practical goals (e.g., fully-managed GenAl pipeline, reproducible infrastructure, policy accuracy ≥ 95%). Map them to the research questions.

\section{Technology & Tooling Stack}
% Briefly justify all key technologies (AWS Bedrock, Terraform, Rego, Python 3.12, etc.). A table works well.

\section{High-Level Architecture}
% Component diagram
% Data-flow diagram
% Responsibilities of each tier (ingestion, processing, code-gen, validation).

\section{Cloud-Infrastructure Codebase (IaC)}

\subsection{Repository Layout}
% Explain folder structure shown in the screenshot (e.g., terraform/database, terraform/knowledge_base).

\subsection{Core Terraform Modules}
% Describe Bedrock, knowledge-base S3 buckets, Lambda warmers, IAM roles, VPC endpoints, etc.

\subsection{Prompt & Knowledge-Base Management}
% Outline prompt-versioning strategy and RAG storage schema.

\subsection{Security Controls}
% Shared-responsibility matrix, least-privilege IAM, S3 encryption, logging.

\subsection{Deployment Workflow}
% GitHub Actions “deploy_knowledgebase.yml", artefact promotion, environment parity.

\section{Prototype Application Codebase}

\subsection{Repository Layout}
% Summarise folders in src/,config/, tests/.

\subsection{Module-Level Description}
% - analyzer: static-scanner wrapper
% policy_generator: RAG prompt builder & Bedrock client
% validator: Rego syntax & semantic checks
% metrics: coverage, false-positive reduction
% Explain main control loop in main.py.

\subsection{Testing Strategy}
% PyTest layout, fixture design, Cl gate, coverage targets.

\subsection{Packaging & Dependency Management}
% requirements.txt, Dockerfile (if any), version pinning.

\section{End-to-End Workflow}
% Step-by-step sequence from Terraform push → static scan → GenAl analysis → policy commit → Rego evaluation. A sequence diagram is helpful here.

\section{CI/CD & DevSecOps Integration}
% - GitHub Actions pipelines (deploy_knowledgebase, destroy_knowledgebase, prototype checks)
% Quality gates (unit tests, Rego tests, policy coverage ≥ 90%).

\section{Observability & Runtime Telemetry}
% Metrics (MTTD, policy-generation latency), structured logging (JSON Logs + AWS CloudWatch), dashboards.

\section{Limitations & Trade-offs}
% Model latency vs. cost, Terraform state confidentiality, policy false-negatives, Bedrock service quotas.

\section{Summary}
% Recap key design choices and link forward to the Results chapter.