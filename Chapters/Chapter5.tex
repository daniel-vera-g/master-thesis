\chapter{Implementation & System Architecture}

% TODO use abreviations from list
% TODO add references

This chapter details the design and implementation of the prototype system, a practical realization of the conceptual framework for GenAI-driven security automation introduced in Chapter 4. The work is impemented through two distinct but interconnected codebases: a cloud-native infrastructure for the generative AI backend, and a Python-based application that orchestrates the analysis and policy generation workflow.

The primary goal of this implementation is to empirically validate the central hypothesis of the theoretical framework: that a hybrid approach, combining traditional static analysis with advanced Large Language Model (LLM) capabilities, can significantly enhance the automation of security policy generation for Infrastructure-as-Code (IaC). This chapter will demonstrate how the system architecture directly maps to the four-layered conceptual model—Data Ingestion, Data Processing, Code Generation, and Validation—and realizes the core principles of leveraging Retrieval-Augmented Generation (RAG) for contextual accuracy and integrating a Human-in-the-Loop (HITL) for safety and oversight.

We will first present the high-level architecture and the technology stack chosen to satisfy the functional requirements of a robust, scalable, and reproducible security pipeline. Subsequently, the chapter will provide a detailed examination of both the cloud infrastructure, deployed via Terraform, and the Python prototype, focusing on the specific modules that implement the core logic of the system. The chapter will conclude by illustrating the end-to-end workflow, from the initial analysis of a Terraform file to the generation and validation of a corresponding Rego security policy, thereby providing a comprehensive account of the system's practical application.

\section{Design Objectives & Functional Requirements}
% Summarise the practical goals (e.g., fully-managed GenAl pipeline, reproducible infrastructure, policy accuracy ≥ 95%). Map them to the research questions.

\section{Technology & Tooling Stack}
% Briefly justify all key technologies (AWS Bedrock, Terraform, Rego, Python 3.12, etc.). A table works well.

\section{High-Level Architecture}
% Component diagram
% Data-flow diagram
% Responsibilities of each tier (ingestion, processing, code-gen, validation).

\section{Cloud-Infrastructure Codebase (IaC)}

\subsection{Repository Layout}
% Explain folder structure shown in the screenshot (e.g., terraform/database, terraform/knowledge_base).

\subsection{Core Terraform Modules}
% Describe Bedrock, knowledge-base S3 buckets, Lambda warmers, IAM roles, VPC endpoints, etc.

\subsection{Prompt & Knowledge-Base Management}
% Outline prompt-versioning strategy and RAG storage schema.

\subsection{Security Controls}
% Shared-responsibility matrix, least-privilege IAM, S3 encryption, logging.

\subsection{Deployment Workflow}
% GitHub Actions “deploy_knowledgebase.yml", artefact promotion, environment parity.

\section{Prototype Application Codebase}

\subsection{Repository Layout}
% Summarise folders in src/,config/, tests/.

\subsection{Module-Level Description}
% - analyzer: static-scanner wrapper
% policy_generator: RAG prompt builder & Bedrock client
% validator: Rego syntax & semantic checks
% metrics: coverage, false-positive reduction
% Explain main control loop in main.py.

\subsection{Testing Strategy}
% PyTest layout, fixture design, Cl gate, coverage targets.

\subsection{Packaging & Dependency Management}
% requirements.txt, Dockerfile (if any), version pinning.

\section{End-to-End Workflow}
% Step-by-step sequence from Terraform push → static scan → GenAl analysis → policy commit → Rego evaluation. A sequence diagram is helpful here.

\section{CI/CD & DevSecOps Integration}
% - GitHub Actions pipelines (deploy_knowledgebase, destroy_knowledgebase, prototype checks)
% Quality gates (unit tests, Rego tests, policy coverage ≥ 90%).

\section{Observability & Runtime Telemetry}
% Metrics (MTTD, policy-generation latency), structured logging (JSON Logs + AWS CloudWatch), dashboards.

\section{Limitations & Trade-offs}
% Model latency vs. cost, Terraform state confidentiality, policy false-negatives, Bedrock service quotas.

\section{Summary}
% Recap key design choices and link forward to the Results chapter.