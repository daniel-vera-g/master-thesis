% Chapter Template

\chapter{Discussion}
\label{chap:discussion}

This chapter interprets the findings from the prototype evaluation, discusses their implications, and connects them back to the research questions. It also addresses the role of human oversight, acknowledges the study's limitations, and outlines concrete avenues for future research.

%----------------------------------------------------------------------------------------
%	SECTION 1: Interpretation of Findings
%----------------------------------------------------------------------------------------
\section{Interpretation of Key Findings}
\label{sec:interpretation}

The empirical results from Chapter~\ref{chap:results} demonstrate both the promise and the current limitations of using a GenAI-driven framework for security policy generation. This section interprets these key findings, focusing on policy accuracy, effectiveness, and the system's contextual reasoning capabilities.

\subsection{The Contrast Between Policy Accuracy and Effectiveness}
A central finding of this research is the significant gap between policy accuracy and policy effectiveness. While the prototype consistently achieved 100\% syntactic accuracy, its logical effectiveness was variable. This highlights a critical distinction in automated security policy generation.

\subsubsection{Achieving Perfect Syntactic Accuracy}
The prototype's 100\% policy accuracy, as reported in Table~\ref{tab:effectiveness-by-severity}, is not an incidental outcome but a direct result of deliberate design choices. The primary mechanism is an automated self-correction loop: if the OPA validator rejects a generated policy, the system captures the specific error feedback and re-prompts the LLM to fix it. This iterative refinement, combined with systematic prompt engineering and the grounding provided by a RAG knowledge base, creates a resilient framework that guarantees the syntactic validity of the final output. This finding suggests that with proper engineering, GenAI models can reliably produce syntactically correct code for domain-specific languages like Rego.

\subsubsection{The Challenge of Logical Effectiveness}
In contrast, achieving 100\% logical effectiveness automatically is a far more complex challenge. The prototype's varied success rates (Table~\ref{tab:effectiveness-by-severity}) underscore this complexity. Verifying effectiveness requires a deep, context-aware understanding of the intended outcome, a problem analogous to the broader challenges in automated program verification and semantic analysis, which are known to be computationally difficult to automate fully \cite{katz_verifying_2017}. Unlike syntactic validation, where error messages are precise and actionable, logical failures lack a clear, structured feedback loop. Debugging an ineffective policy would require providing the LLM with an extensive context including the IaC, the generated policy, the Terraform plan, and a description of the desired behavior making manual intervention by a human expert a more pragmatic approach. This challenge is a known limitation of generative AI, where models can produce plausible but incorrect outputs, often referred to as "hallucinations" \cite{reversinglabs_why_2024}. This reinforces the indispensable role of the Human-in-the-Loop (HITL), a core tenet of this thesis, for validating the logical soundness of automated outputs

\subsection{Contextual Reasoning: Successes and Shortcomings}
The evaluation of the prototype's contextual reasoning, summarized in Table~\ref{tab:context-reasoning-summary}, reveals a nuanced picture. The system demonstrated a strong ability to understand and reason about relationships between different parts of the IaC, successfully identifying complex, cross-resource, and conditional vulnerabilities. This confirms the hypothesis that LLMs can move beyond the limitations of traditional static analysis by interpreting the broader context in which resources operate.

However, the failures are equally instructive. The prototype struggled with nuances of human intent (e.g., ignoring developer comments) and external context not explicitly present in the code (e.g., recognizing the valid business purpose of a public S3 bucket for a website). These cases underscore the challenges that remain in achieving true contextual understanding and highlight the system's dependency on the data it was trained on. This limitation reinforces the need for a HITL approach, where the automated system provides a strong baseline analysis that a human expert can then refine and validate with their broader, real-world knowledge.

%----------------------------------------------------------------------------------------
%	SECTION 2: The Role of the Human-in-the-Loop
%----------------------------------------------------------------------------------------
\section{The Role of the Human-in-the-Loop}
\label{sec:hitl_in_action}

While GenAI can automate policy creation, it does not eliminate the need for human expertise. The Human-in-the-Loop (HITL) process is therefore not merely a supplementary feature but a cornerstone of a responsible and effective security automation framework. Its importance is most evident when considering the gap between policy accuracy and effectiveness discussed in Section~\ref{sec:interpretation}.

As shown in Chapter~\ref{chap:results}, the prototype generated syntactically perfect policies (100\% accuracy) but struggled with logical effectiveness, which was as low as 36\% for some categories (Table~\ref{tab:effectiveness-by-severity}). A syntactically valid but logically flawed policy can create a false sense of security or, worse, introduce new risks. For example, an overly restrictive policy could cause an outage, while an overly permissive one fails to mitigate the intended threat. The HITL process serves as the critical validation gate to prevent such outcomes by embedding human judgment into the AI-driven workflow \cite{mohsin_unified_2024}. It ensures that a knowledgeable human expert reviews each policy for correctness, relevance, and safety before it is deployed, aligning with established frameworks for responsible AI implementation \cite{ibm_what_2025}.

Furthermore, the HITL process provides two additional benefits that a fully automated system cannot:
\begin{itemize}
    \item \textbf{Contextual Enrichment:} A human expert can bring in external context that is unavailable to the model, such as business justifications for an unusual configuration (as seen in the ''False Positive Reduction'' failure in Section~\ref{sec:results-context}) or knowledge of an impending architectural change. This prevents the system from flagging legitimate configurations as vulnerabilities.
    \item \textbf{System Improvement:} The corrections and approvals from the human reviewer serve as a valuable feedback loop. This data can be used to fine-tune the LLM, refine the RAG knowledge base, and improve the prompt engineering over time. This process creates a valuable feedback loop, a principle central to methods like Reinforcement Learning from Human Feedback (RLHF), which leads to a more intelligent and effective system in the long run \cite{ouyang_training_2022}.
\end{itemize}

The workflow is designed to be straightforward: the system presents its recommendation, justification, and source context to a reviewer. The expert can then approve, reject, or modify the policy. This ensures that automation accelerates the process without ceding final control, embodying a partnership between the AI and the human expert.

%----------------------------------------------------------------------------------------
%	SECTION 3: Limitations
%----------------------------------------------------------------------------------------
\section{Limitations of the Current Study}
\label{sec:limitations}

While the prototype demonstrates the viability of the approach, it is important to acknowledge its limitations. These limitations provide the context for the scope of the findings and form the basis for future work.

\subsection{Prototype Scope and Generalizability}
The prototype was intentionally developed with a narrow focus to serve as a proof-of-concept. Specifically, the evaluation was conducted exclusively on Infrastructure-as-Code written in Terraform for the Amazon Web Services (AWS) cloud platform, using \texttt{tfsec} as the primary static analysis linter. This specificity has several implications for the generalizability of the findings.

First, the contextual analysis and policy generation logic are tightly coupled with AWS resource types and IAM semantics. Expanding support to other cloud providers, such as Google Cloud Platform (GCP) or Microsoft Azure, would require further development. This would involve not only extending the knowledge base with provider-specific information but also adapting the contextual analysis prompts and logic to handle different resource models and security paradigms.

Second, the system is dependent on the output of a single linter, \texttt{tfsec}. While effective, other popular linters like Checkov or Terrascan identify different sets of issues or provide different contextual details \cite{spacelift_infrastructure_2025}. Integrating these tools would require building specific adapters to normalize their outputs into a format the GenAI core can process, as outlined in Section~\ref{subsec:future_linters}.

Finally, the policy generation is specific to the Rego language for the Open Policy Agent (OPA). Supporting other policy-as-code engines, such as Sentinel, would necessitate a complete rewrite of the generation prompts and validation logic. Therefore, while the conceptual framework is designed to be provider- and tool-agnostic, the current implementation's results are directly applicable only to the AWS, Terraform, and \texttt{tfsec} ecosystem.

\subsection{GenAI Component Tuning and Optimization}
The Generative AI component, built on Amazon Web Services (AWS) Bedrock, is functional but has not been exhaustively optimized. The current implementation uses Anthropic's Claude 3.5 Sonnet model and leverages the built-in Knowledge Bases for Bedrock for its Retrieval-Augmented Generation (RAG) capabilities. This setup provides a solid baseline but leaves considerable room for performance and reliability enhancements.

A primary area for improvement is the knowledge base itself. The effectiveness of the RAG system is directly dependent on the quality, relevance, and comprehensiveness of the source documents, as the retriever can only surface information present in its knowledge base \cite{lewis_retrieval-augmented_2021}. The current knowledge base could be expanded with a more diverse corpus of information, including official AWS security bulletins, Terraform best practice guides, and internal security standards. Furthermore, the configuration of the Bedrock Knowledge Base specifically its chunking strategy and the underlying embedding model was left at the default settings for this study. Future work should involve systematically experimenting with different chunking sizes and overlaps to optimize how documents are segmented and retrieved. The choice of a chunking strategy is a critical, non-trivial factor that directly impacts retrieval relevance and the coherence of the context provided to the LLM \cite{gao_retrieval-augmented_2024}.

Additionally, while Anthropic's Claude 3.5 Sonnet proved capable, AWS Bedrock offers a diverse and evolving landscape of models from various providers. A thorough evaluation of alternative models, including newer versions of Claude like the Sonnet 3.5 model or specialized models from other providers, could yield significant improvements in policy generation accuracy, latency, or cost-effectiveness. Such an evaluation is critical, as studies show that overall correctness can vary significantly between models, especially for security-critical tasks \cite{pearce_deployability-centric_2025}. This would be a critical next step in maturing the prototype from a proof-of-concept to a production-ready tool.

%----------------------------------------------------------------------------------------
%	SECTION 4: Avenues for Future Research
%----------------------------------------------------------------------------------------
\section{Future Work and Prototype Expansion}
\label{sec:future_work}

The limitations of this study highlight several promising directions for future research and development. This section outlines key areas for expanding the prototype's capabilities and robustness.

\subsection{Knowledge Base Retrieval}
The current retrieval-augmented generation (RAG) mechanism provides a foundational level of context, but its precision can be enhanced. Future work should explore more advanced RAG techniques, such as implementing semantic search, which improves relevance by matching conceptual meaning rather than keywords \cite{zoomin_benefits_2024}. Other areas include using query transformations to better align user intent with document content and developing hybrid search strategies. Improving the relevance of retrieved information is critical for generating more accurate and context-aware security policies \cite{openai_retrieval_2025}.

\subsection{Model Selection and Tuning}
The choice of the Large Language Model is a pivotal factor in the system's performance. The current prototype uses a general-purpose model, but future iterations should involve a systematic evaluation of various LLMs. This includes benchmarking newer, more powerful models (e.g., GPT-4, Claude 3.7 Sonnet), as well as domain-specific models that are fine-tuned on cybersecurity and IaC data. The evaluation criteria should include not only the accuracy of the generated policies but also performance metrics like latency and throughput, and the overall cost-effectiveness of the solution.

\subsection{Interaction and Orchestration Logic}
The robustness of the interaction between the prototype and the LLM can be substantially improved. The current retry logic is basic; a more sophisticated approach, such as implementing exponential backoff with jitter, is a best practice for building resilient API clients \cite{doordash_best_nodate}. Furthermore, as IaC configurations grow, the strategy for chunking content becomes crucial. Future work should move beyond simple fixed-size chunking to "content-aware" methods that preserve the semantic integrity of the code, such as recursive character splitting or language-specific parsers \cite{pinecone_chunking_2025}.

\subsection{Context Consolidation}
The function responsible for consolidating information from multiple sources, such as linter outputs, knowledge base articles, and IaC snippets is currently rudimentary. A more advanced consolidation function could itself leverage a language model to summarize and synthesize these disparate pieces of information into a single, coherent prompt. This would improve the signal-to-noise ratio in the context provided to the primary LLM, reducing ambiguity and leading to more precise and relevant security policy generation.

\subsection{Multi-Cloud Support}
Expanding the prototype to be multi-cloud capable would be a primary objective. This would involve abstracting the cloud-specific logic into separate modules. For instance, one could create a \texttt{CloudProvider} interface with concrete implementations for AWS, Azure, and GCP. The analysis layer would then dynamically load the appropriate module based on the IaC being scanned. This would require changes to the code that interprets linter outputs and maps them to cloud-specific resource configurations and security concepts.

\subsection{Integration of Additional Linters}
\label{subsec:future_linters}
Similarly, the prototype can be extended to support more IaC linters (e.g., Checkov, Terrascan). This would require creating a generic \texttt{Linter} interface and then implementing specific adapters for each tool. Each adapter would be responsible for executing the linter, parsing its JSON output, and normalizing the findings into a standardized format that the core application can process. This modular design would make the system more versatile and adaptable to different security scanning environments, increasing its overall utility.