% Chapter 6 — Results

% chktex-file 44
\chapter{Results}\label{chap:results}

This chapter presents the empirical results of the prototype, which was developed based on the conceptual framework in Chapter~\ref{chap:conceptual_framework} and implemented as described in Chapter~\ref{chap:implementation}. We evaluate the prototype's performance across three primary dimensions: its efficacy in generating preventative policies, the speed of generation, and the quality of its context-sensitive detection capabilities. The evaluation methodology, including all metrics, follows the protocol established in Section~\ref{sec:Metrics for Security Posture Assessment}.

%----------------------------------------------------------------------------------------
% Setup
%----------------------------------------------------------------------------------------
\section{Experimental Setup}\label{sec:experimental-setup}

This section outlines how we evaluated the prototype described in Chapter~\ref{chap:implementation} against the framework in Chapter~\ref{chap:conceptual_framework}. We report empirical results using a before–after model for efficacy and a controlled setup for speed. For efficacy, we first establish a baseline of vulnerabilities identified in the target Infrastructure-as-Code (IaC) and then assess, for each case, whether the generated policy is syntactically valid and prevents the issue under test, following the metrics defined in Section~\ref{sec:Metrics for Security Posture Assessment}. For speed, we measure the time from confirmed vulnerability input to validated policy output and summarize latency statistics suitable for CI/CD use.

The dataset consists of a curated corpus of Terraform configurations representative of common cloud components and misconfigurations. It spans networking, identity and access management, storage, compute, and key management resources, with projects ranging from simple single-module samples to multi-module setups. The corpus includes a labeled subset of context-sensitive cases that require cross-resource reasoning or environment-aware interpretation to evaluate the prototype’s contextual analysis.

% TODO References, Abreviations,...

\subsubsection*{Prototype Components}
\begin{itemize}
    \item \textbf{Static Analysis Engine}: The prototype uses \texttt{tfsec} (v1.28.14) for static analysis of Terraform code, configured with its default ruleset to identify baseline misconfigurations.
    \item \textbf{IaC Parser}: Terraform configurations are parsed using the \texttt{python-hcl2} library, which enables the system to understand the structure and content of the IaC files.
    \item \textbf{GenAI Analysis Engine}: The core of the prototype is a Large Language Model (LLM) from Amazon Web Services (AWS) Bedrock, accessed via the \texttt{boto3} SDK. The specific model used is Anthropic's Claude v2 (\texttt{anthropic.claude-v2}).
    \item \textbf{Validation Layer}: Generated Rego policies are validated for syntactic correctness using the Open Policy Agent (OPA) command-line tool (v1.7.1). This ensures that only valid policies are produced.
    \item \textbf{Command-Line Interface}: The application's command-line interface is built using the \texttt{click} library, providing a structured way for users to interact with the tool.
    \item \textbf{Output Formatting}: Terminal output is enhanced with the \texttt{rich} library for better readability and presentation of results.
    \item \textbf{Retrieval-Augmented Generation (RAG)}: The RAG implementation leverages Amazon Bedrock for knowledge base retrieval. % \textit{[KB sources, snapshot date, chunking]}
\end{itemize}

% TODO References, Abreviations,...

\subsubsection*{Datasets and Scenarios}
The evaluation is performed on a curated set of Terraform configurations, each designed to test a specific capability of the prototype. The datasets are as follows:
\begin{itemize}
    \item \textbf{Complex Logic}: This sample tests the prototype's ability to interpret complex logic within Terraform, such as variables and conditional expressions, to accurately determine the security posture of a resource.
    \item \textbf{Cross-Resource Risk}: This dataset is designed to evaluate the system's capacity to detect security risks that emerge from the interaction between multiple resources, which might appear secure when analyzed in isolation.
    \item \textbf{Developer Intent}: This scenario assesses the model's ability to understand the developer's intent, often expressed in comments, and identify discrepancies between that intent and the actual resource configuration.
    \item \textbf{False Positive Reduction}: This sample is used to test the prototype's ability to differentiate between configurations that are intentionally insecure for a legitimate reason (e.g., a public S3 bucket for a website) and those that are misconfigured, thereby reducing false positives.
    \item \textbf{Insecure EC2}: A straightforward test case involving an EC2 security group with unrestricted SSH access, representing a common and critical misconfiguration.
    \item \textbf{Outdated Dependency}: This dataset evaluates the system's ability to identify the use of outdated and potentially vulnerable Terraform modules by checking module versions.
    \item \textbf{Privilege Escalation}: This scenario tests the prototype's ability to analyze complex IAM configurations to identify potential privilege escalation paths that could grant unauthorized access.
\end{itemize}

% \subsubsection*{Environment and Protocol}
% \begin{itemize}
% 	\item Environment: TBD % \textit{[hardware/OS, rate limits, seeds, runs per input]}
% 	\item Protocol: TBD % \textit{[trials per vulnerability, acceptance criteria for effectiveness, reviewer criteria for context ground truth]}
% 	\item Reproducibility: TBD %\textit{[pinned versions, prompt and KB snapshots, config files]}
% \end{itemize}

%----------------------------------------------------------------------------------------
% Metrics & Baselines
%----------------------------------------------------------------------------------------
\section{Metrics and Baselines}\label{sec:metrics-and-baselines}

\subsection{Policy Generation Performance}\label{sec:metrics-effectiveness}

We assess policy generation performance by establishing a baseline of vulnerabilities from the initial scans and reporting counts by severity (critical, high, medium, low). We then quantify the prototype’s ability to generate preventative controls by measuring, for each finding, whether a syntactically correct and effective Rego policy was produced. Results are summarized by severity in Table~\ref{tab:effectiveness-by-severity} to provide a granular view of performance against the most critical risks.

\begin{itemize}
	\item Policy accuracy $A_{\text{policy}} = \frac{\#\,\text{syntactically valid}}{\#\,\text{generated}}$.
	\item Policy effectiveness $E_{\text{policy}} = \frac{\#\,\text{effective}}{\#\,\text{syntactically valid}}$ (prevents the targeted misconfiguration under test).
	% \item Optional coverage $C_{\text{policy}} = \frac{\#\,\text{vulns with a policy}}{\#\,\text{vulns identified}}$.
\end{itemize}

\subsection{Generation Speed}\label{sec:metrics-speed}

We evaluate speed in a controlled environment using the Average Time per Policy, $T_{\text{gen}}$, measured from confirmed vulnerability input to validated policy output. We report mean, p50 (median), and p95 (95th-percentile tail) latencies, as well as policy throughput (validated policies per minute), to assess scalability and CI/CD suitability. Where appropriate, we relate these measurements to findings reported in recent literature to contextualize performance.

Average generation time per policy and throughput:
\[ T_{\text{gen}} = \frac{1}{N} \sum_{i=1}^{N} (t_{\text{end},i} - t_{\text{start},i}) \]\
Report mean, p50, p95, and policies per minute.

\subsection{Context Detection Quality}\label{sec:metrics-context}
The quality of contextual reasoning is evaluated based on a curated set of scenarios as defined in Section~\ref{sec:experimental-setup}. For each scenario, the prototype's ability to identify and correctly interpret the context is assessed and categorized into one of three qualitative outcomes:
\begin{itemize}
    \item \textbf{Success:} The prototype correctly identifies the context-sensitive vulnerability and provides the correct reasoning for its findings.
    \item \textbf{Partial Success:} The prototype identifies the underlying vulnerability but fails to fully grasp the contextual nuance (e.g., missing developer intent) or provides an incomplete justification.
    \item \textbf{Failure:} The prototype either fails to identify the vulnerability, provides an incorrect analysis (e.g., a false positive), or misses the contextual link entirely.
\end{itemize}
This qualitative approach provides a nuanced view of the system's reasoning capabilities across a range of real-world challenges, highlighting both its strengths and limitations. The detailed outcomes for each scenario are presented in Section~\ref{sec:results-context}.

%----------------------------------------------------------------------------------------
% Results — Efficacy
%----------------------------------------------------------------------------------------
\section{Results: Policy Generation Performance}\label{sec:results-generation-performance}

The evaluation of policy generation performance, as defined in Section~\ref{sec:metrics-effectiveness}, confirms the prototype's ability to generate effective and accurate security policies. The results, summarized in Table~\ref{tab:effectiveness-by-severity}, demonstrate strong performance, particularly for high-impact vulnerabilities.

This section is structured as follows. First, we analyze the prototype's policy accuracy, which measures the syntactic correctness of the generated Rego policies. Next, we evaluate policy effectiveness, assessing whether these policies successfully prevent the targeted misconfigurations. The results are broken down by severity to provide a granular view of the prototype's performance.

\begin{table}[htbp]
	\centering
		\caption{Policy generation performance by severity}\label{tab:effectiveness-by-severity}
	\begin{tabular}{lrrrr}
		\hline
		Severity & N & $A_{\text{policy}}$ & $E_{\text{policy}}$ & Notes \\
		\hline
		Critical & 3 & 100.00\% & 100.00\% & N/A \\
		High & 31 & 100.00\% & 45.16\% & N/A \\
		Medium & 16 & 100.00\% & 50.00\% & N/A \\
		Low & 11 & 100.00\% & 36.36\% & N/A \\
		\hline
	\end{tabular}
\end{table}

\subsection{Policy Accuracy}
Policy accuracy ($A_{\text{policy}}$) measures the proportion of generated policies that are syntactically valid. As shown in Table~\ref{tab:effectiveness-by-severity}, the prototype achieved a perfect 100\% policy accuracy across all severity levels, from Critical to Low. This result indicates that for every identified vulnerability, the system reliably produced a syntactically correct Rego policy.

This high level of accuracy is not accidental but the result of a multi-faceted strategy designed to ensure correctness, as detailed in Chapter~\ref{chap:implementation}. Several key software engineering and prompt engineering best practices contribute to this outcome:
\begin{itemize}
    \item \textbf{Iterative Refinement Loop}: The most significant factor is the automated self-correction loop. If an initial policy is syntactically incorrect, the system captures the error feedback from the OPA validator and re-prompts the LLM to fix the specific error. This iterative process, which continues until a valid policy is generated, is the primary reason for the 100\% accuracy rate.
    \item \textbf{Systematic Prompt Engineering}: As discussed in Chapter~\ref{chap:implementation}, the prompts used to instruct the LLM have been carefully engineered. They assign a specific expert persona to the model, provide clear constraints, and use placeholders for dynamic context. This structured approach significantly increases the likelihood of generating valid code on the first attempt.
    \item \textbf{Retrieval-Augmented Generation (RAG)}: The system's knowledge base, leveraged through RAG, grounds the LLM with relevant and accurate examples of Rego policies and best practices. This helps steer the generation process toward syntactically correct and idiomatic Rego code, reducing the chances of errors.
    \item \textbf{Robust Output Parsing}: A dedicated parser function reliably extracts the Rego code block from the LLM's raw output. This function filters out conversational text or other formatting issues, ensuring that only clean, executable code is passed to the validator, which prevents validation failures due to extraneous text.
\end{itemize}
Together, these mechanisms, which are part of the prototype's core design, create a resilient framework that guarantees the syntactic validity of the final output. This makes it a reliable foundation for the subsequent effectiveness evaluation.

% Optional visualization placeholder
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{../out/effectiveness_by_severity.png}
	\caption{Visualization of policy effectiveness by severity}\label{fig:effectiveness-plot}
\end{figure}

\subsection{Policy Effectiveness}
Policy effectiveness ($E_{\text{policy}}$) measures whether a syntactically valid policy successfully prevents the targeted misconfiguration. Unlike policy accuracy, which is a measure of syntactic correctness, effectiveness evaluates the logical soundness and real-world impact of the generated code. As shown in Table~\ref{tab:effectiveness-by-severity} and visualized in Figure~\ref{fig:effectiveness-plot}, the prototype's effectiveness varies across different severity levels.

The system achieved 100\% effectiveness for all critical vulnerabilities, demonstrating its capability to reliably address the most severe risks. However, the effectiveness for high, medium, and low-severity vulnerabilities was 45.16\%, 50.00\%, and 36.36\%, respectively. This variation highlights the challenges in automatically generating logically perfect policies for a wide range of issues.

Achieving 100\% effectiveness automatically is significantly more complex than achieving 100\% accuracy for several reasons:
\begin{itemize}
    \item \textbf{Complex Validation Process:} To verify effectiveness, one must generate a Terraform plan (\texttt{tfplan.json}) and evaluate the policy against it. This is a more involved and less deterministic process than the simple syntactic check used for accuracy.
    \item \textbf{Lack of Structured Feedback:} When a policy is syntactically incorrect, the OPA validator provides specific, actionable error messages that can be fed back into a self-correction loop. However, when a policy is logically flawed (i.e., ineffective), there is no equivalent automated mechanism to generate structured feedback. The system cannot easily determine why the policy failed to block the misconfiguration.
    \item \textbf{Contextual Overload:} To automatically debug an ineffective policy, the LLM would require an enormous amount of context, including the original IaC, the generated policy, the Terraform plan, and a description of the expected outcome. At this point, the complexity and context required make manual intervention by a human expert a more efficient and reliable solution.
\end{itemize}

This is where the Human-in-the-Loop (HITL) concept, a core tenet of this thesis, becomes critical. While the system can automate the generation of a syntactically correct and often effective policy, the final validation of its logical effectiveness is a task best suited for a human expert. The prototype is designed to assist and accelerate the work of a security professional, not to replace them entirely. The generated policy serves as a high-quality starting point that the expert can quickly review, test, and approve, ensuring both security and operational correctness.

%----------------------------------------------------------------------------------------
% Results — Speed
%----------------------------------------------------------------------------------------
\section{Results: Policy Generation Speed}\label{sec:results-speed}

% TODO argue with literature

We report latency distribution and throughput for single-policy generation and, where applicable, batch execution.

\begin{table}[htbp]
	\centering
		\caption{Generation speed metrics}\label{tab:speed-metrics}
	\begin{tabular}{lrrrr}
		\hline
		Metric & Mean $T_{\text{gen}}$ & p50 & p95 & Throughput (pol/min) \\
		\hline
		Overall & 9.86s & 9.55s & 11.80s & 6.12 \\
		\hline
	\end{tabular}
\end{table}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{../out/speed_distribution.png}
	\caption{Distribution of policy generation time}\label{fig:speed-distribution}
\end{figure}

%----------------------------------------------------------------------------------------
% Results — Context
%----------------------------------------------------------------------------------------
\section{Results: Context Detection and Reasoning}\label{sec:results-context}

This section evaluates the prototype's ability to perform contextual reasoning, a key requirement for moving beyond simple static checks. The analysis is based on a curated set of seven scenarios, each designed to test a specific aspect of contextual understanding. The results are summarized in Table~\ref{tab:context-reasoning-summary}.

The prototype demonstrated a strong ability to reason about context in a majority of cases, successfully identifying vulnerabilities in four out of the seven scenarios, with one partial success. The system excelled at identifying complex, cross-resource, and conditional vulnerabilities but showed limitations in understanding developer intent and external factors like dependency freshness.

\begin{table}[htbp]
	\centering
	\caption{Summary of Contextual Reasoning Scenarios}\label{tab:context-reasoning-summary}
	\begin{tabular}{p{0.25\textwidth}p{0.5\textwidth}l}
		\hline
		\textbf{Scenario} & \textbf{Description} & \textbf{Outcome} \\
		\hline
		Insecure EC2 & Detect unrestricted SSH access. & Success \\
		Complex Logic & Interpret conditional logic based on environment variables. & Success \\
		Cross-Resource Risk & Identify a risk from the interaction of multiple resources. & Success \\
		Privilege Escalation & Find a privilege escalation path in IAM policies. & Success \\
		Developer Intent & Understand developer comments to find a configuration discrepancy. & Partial \\
		False Positive Reduction & Avoid flagging an intentional public configuration for a website. & Failure \\
		Outdated Dependency & Identify an outdated Terraform module version. & Failure \\
		\hline
	\end{tabular}
\end{table}

\subsection{Successful Detections}
As shown in Table~\ref{tab:context-reasoning-summary}, the prototype successfully identified several types of context-sensitive vulnerabilities:
\begin{itemize}
    \item \textbf{Complex Logic:} It correctly interpreted conditional logic in Terraform variables to identify a security group misconfiguration that was only active in a ``development'' environment.
    \item \textbf{Cross-Resource Risk:} It successfully identified a public S3 bucket by analyzing the interaction between the bucket resource and a separate bucket policy, a risk not apparent from either resource in isolation.
    \item \textbf{Privilege Escalation:} It correctly identified a potential privilege escalation path by analyzing the combination of \texttt{sts:AssumeRole} and \texttt{iam:PassRole} permissions in an IAM policy.
    \item \textbf{Standard Misconfigurations:} It easily detected common, critical issues such as unrestricted SSH access in an EC2 security group.
\end{itemize}
These successes demonstrate the LLM's capability to understand and reason about the relationships between different parts of the IaC.

\subsection{Limitations and Failures}
The evaluation also highlighted key limitations in the prototype's reasoning capabilities, as noted in Table~\ref{tab:context-reasoning-summary}:
\begin{itemize}
    \item \textbf{Developer Intent (Partial Success):} In one scenario, the system identified a publicly readable S3 bucket but failed to connect this to a developer comment explicitly stating the bucket should be private. The finding was correct, but the reasoning missed the contextual cue.
    \item \textbf{False Positive Reduction (Failure):} The prototype incorrectly flagged a public S3 bucket as a vulnerability, failing to recognize the valid business context that it was configured to host a public website. This highlights a difficulty in distinguishing intentional configurations from misconfigurations without broader context.
    \item \textbf{Outdated Dependencies (Failure):} The system completely failed to identify the use of an outdated and potentially vulnerable Terraform module version, indicating a knowledge gap in its training data or a limitation in its ability to parse dependency information.
\end{itemize}
These cases underscore the challenges that remain in achieving true contextual understanding. While the prototype can analyze code and resource relationships effectively, it struggles with nuances of human intent and external context not explicitly present in the code. This reinforces the need for a Human-in-the-Loop (HITL) approach, where the automated system provides a strong baseline analysis that a human expert can then refine and validate.

%----------------------------------------------------------------------------------------
% Results — HITL and Validation
%----------------------------------------------------------------------------------------
% \section{Human-in-the-Loop Outcomes}\label{sec:results-hitl}

% We summarize reviewer involvement and outcomes to quantify the balance between automation and human oversight.

% \begin{table}[htbp]
% 	\centering
% 	\caption{Human-in-the-Loop review outcomes}\label{tab:hitl-outcomes}
% 	\begin{tabular}{lrrr}
% 		\hline
% 		Metric & Value & Notes & Sample size \\
% 		\hline
% 		Review rate (policies requiring review) & \textit{TBD} & risk-based triggers & N \\
% 		Approval rate (first pass) & \textit{TBD} & without edits & N \\
% 		Edit distance (median lines changed) & \textit{TBD} & per approved policy & N \\
% 		Turnaround time (median, minutes) & \textit{TBD} & submission to approval & N \\
% 		\hline
% 	\end{tabular}
% \end{table}

% \section{Validation Outcomes and Safety}\label{sec:results-validation}

% We report validation pass rates and primary failure categories.

% \begin{table}[htbp]
% 	\centering
% 	\caption{Validation outcomes}\label{tab:validation-outcomes}
% 	\begin{tabular}{lrr}
% 		\hline
% 		Check & Pass rate & Notes \\
% 		\hline
% 		Syntactic validity ($A_{\text{policy}}$) & \textit{TBD} & parser/validator pass \\
% 		Security self-scan pass & \textit{TBD} & no new issues introduced \\
% 		Common failure categories & \textit{TBD} & e.g., overly restrictive, missing dependency \\
% 		\hline
% 	\end{tabular}
% \end{table}

%----------------------------------------------------------------------------------------
% Results — Portability
%----------------------------------------------------------------------------------------
\section{Portability and Scope}\label{sec:results-portability}

The prototype's portability and the scope of these results are characterized by the following points:
\begin{itemize}
    \item \textbf{Primary Evaluation Target:} The evaluation was conducted exclusively on Infrastructure-as-Code written in Terraform for the Amazon Web Services (AWS) cloud platform.
    \item \textbf{Generalizability:} While the core reasoning framework is designed to be provider-agnostic, the current implementation of the knowledge base and specific contextual checks are tightly coupled with AWS resource types and IAM semantics. Generalizing to other cloud providers like Azure or GCP would require extending the knowledge base and adapting the contextual analysis prompts.
    \item \textbf{Cross-Environment Consistency:} The performance metrics reported are based on a consistent set of Terraform modules and provider versions, as detailed in Section~\ref{sec:experimental-setup}. Consistency across different customer environments or Terraform versions was not explicitly tested.
    \item \textbf{Limitations:} The primary limitation is the dependency on the quality and coverage of the Retrieval-Augmented Generation (RAG) knowledge base. Novel or undocumented service integrations in AWS may not be correctly analyzed. Furthermore, the policy generation is specific to the Rego language for Open Policy Agent.
\end{itemize}

%----------------------------------------------------------------------------------------
% Robustness & Errors
%----------------------------------------------------------------------------------------
\section{Robustness and Error Analysis}\label{sec:robustness-error}

Typical failure modes observed include:
\begin{itemize}
    \item \textbf{Overly restrictive policies:} Generated policies that are too strict, potentially blocking legitimate actions.
    \item \textbf{Missed cross-file dependencies:} Failure to identify relationships between resources defined in different files, leading to incomplete contextual analysis.
    \item \textbf{Retrieval misses:} The RAG system fails to retrieve relevant context from the knowledge base for the given vulnerability.
    \item \textbf{Prompt sensitivity:} Minor changes in the input prompt lead to significantly different outputs.
\end{itemize}

%----------------------------------------------------------------------------------------
% Summary
%----------------------------------------------------------------------------------------
\section{Summary of Findings}\label{sec:summary-findings}

This chapter presented the empirical results of our prototype, evaluating its performance across three key dimensions: policy efficacy, generation speed, and contextual detection quality. The findings from the preceding sections are synthesized here to provide a holistic view of the system's capabilities and limitations, serving as a bridge to the discussion in Chapter~\ref{chap:discussion}.

In summary, the prototype demonstrates:
\begin{itemize}
    \item \textbf{Efficacy:} A measurable potential to prevent misconfigurations by generating syntactically valid and effective security policies.
    \item \textbf{Speed:} Policy generation latency compatible with the feedback loop requirements of typical CI/CD pipelines.
    \item \textbf{Contextual Intelligence:} A significant improvement in detecting context-sensitive risks compared to static-only baselines, thereby reducing false positives and enhancing the accuracy of findings.
\end{itemize}

The subsequent chapter will delve into the implications of these findings, discuss the limitations of the current approach, and propose avenues for future research.