
@article{weedon_generative_2023,
	title = {Generative {AI}: security implications for business automation},
	volume = {2023},
	issn = {1353-4858, 1872-9371},
	url = {http://www.magonlinelibrary.com/doi/10.12968/S1353-4858%2823%2970045-7},
	doi = {10.12968/S1353-4858(23)70045-7},
	shorttitle = {Generative {AI}},
	abstract = {{AI} tools such as {ChatGPT} certainly appear to offer significant benefits to organisations looking to save costs and improve efficiency. But the technology is being adopted with an enthusiasm that leaves little room for the careful consideration of potential security vulnerabilities. So how do you embrace this revolution without putting yourself at risk?},
	pages = {S1353--4858(23)70045--7},
	number = {9},
	journaltitle = {Network Security},
	author = {Weedon, Scott},
	urldate = {2024-09-07},
	date = {2023-09},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/3ICS935N/ba42b88132000be4ea8cf6b9a119f6b71d0b07f1.html:text/html},
}

@misc{haryanto_secgenai_2024,
	title = {{SecGenAI}: Enhancing Security of Cloud-based Generative {AI} Applications within Australian Critical Technologies of National Interest},
	url = {http://arxiv.org/abs/2407.01110},
	doi = {10.48550/arXiv.2407.01110},
	shorttitle = {{SecGenAI}},
	abstract = {The rapid advancement of Generative {AI} ({GenAI}) technologies offers transformative opportunities within Australia's critical technologies of national interest while introducing unique security challenges. This paper presents {SecGenAI}, a comprehensive security framework for cloud-based {GenAI} applications, with a focus on Retrieval-Augmented Generation ({RAG}) systems. {SecGenAI} addresses functional, infrastructure, and governance requirements, integrating end-to-end security analysis to generate specifications emphasizing data privacy, secure deployment, and shared responsibility models. Aligned with Australian Privacy Principles, {AI} Ethics Principles, and guidelines from the Australian Cyber Security Centre and Digital Transformation Agency, {SecGenAI} mitigates threats such as data leakage, adversarial attacks, and model inversion. The framework's novel approach combines advanced machine learning techniques with robust security measures, ensuring compliance with Australian regulations while enhancing the reliability and trustworthiness of {GenAI} systems. This research contributes to the field of intelligent systems by providing actionable strategies for secure {GenAI} implementation in industry, fostering innovation in {AI} applications, and safeguarding national interests.},
	number = {{arXiv}:2407.01110},
	publisher = {{arXiv}},
	author = {Haryanto, Christoforus Yoga and Vu, Minh Hieu and Nguyen, Trung Duc and Lomempow, Emily and Nurliana, Yulia and Taheri, Sona},
	urldate = {2024-08-26},
	date = {2024-07-01},
	eprinttype = {arxiv},
	eprint = {2407.01110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/d.veragilliard/Zotero/storage/Y87CT6NW/Haryanto et al. - 2024 - SecGenAI Enhancing Security of Cloud-based Genera.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/7S4ZGDKG/2407.html:text/html},
}

@article{senior_software_engineer_cisco_systems_inc_usa_next-gen_2024,
	title = {Next-Gen Firewalls: Enhancing Cloud Security with Generative {AI}},
	volume = {3},
	url = {https://www.onlinescientificresearch.com/articles/nextgen-firewalls-enhancing-cloud-security-with-generative-ai.pdf},
	doi = {10.47363/JAICC/2024(3)404},
	shorttitle = {Next-Gen Firewalls},
	abstract = {Next-generation firewalls are available and use machine learning and generative modeling to enhance the detection of hard-to-detect cyber threats. These systems incorporate advanced security controls, policies, and protocols with Layer 7 of the {OSI} model. This chapter updates these steep {AI}-based protection systems and applications.},
	pages = {1--9},
	number = {4},
	journaltitle = {J Arti Inte \& Cloud Comp},
	author = {{Senior Software Engineer, Cisco Systems Inc, USA} and Lekkala, Seshagirirao},
	urldate = {2025-03-31},
	date = {2024-08-30},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/D9LSZDNH/4373239ca9de9d5ab0dbf0b2e60bf1885beca545.html:text/html},
}

@article{patel_generative_2025,
	title = {Generative {AI} for Automated Security Operations in Cloud Computing},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10849302/},
	doi = {10.1109/ICAIC63015.2025.10849302},
	abstract = {New opportunities in cloud computing have brought many new risks that require effective protection of dynamic distributed environments. Introducing a new formative technology, generative {AI}, to cloud security has far-reaching benefits for automating threat detection, real-time incident addressing, and vulnerability management. This paper focuses on extending generative {AI} with cloud security tools like {AWS} {GuardDuty} and Google Cloud Security Command Center; the contemplation of accuracy enhancement and response efficiency highlights its aim. Concerning actual applications such as {SOAR} systems, the study demonstrates how media industry giants, such as Netflix and {JPMorgan} Chase, have used {AI} to minimize risk factors while increasing operational efficiency. The paper also discusses the significant increase in response time, enhanced detection accuracy, and the shift to proactive security strategies brought by generative {AI}. Drawing attention to {AI} systems’ opportunities, the study examines the subsequent issues connected with {AI} applications, including over-dependence on {AI} tools, adversarial risk to models, and the complex nature of decisionmaking in the context of {AI} systems. The present study also highlights the importance of generative {AI} in strengthening the defense of the cloud environment, but, at the same time, it recognizes the significance of preventive efforts and planned action plans to manage these technologies efficiently.},
	pages = {1--7},
	journaltitle = {2025 {IEEE} 4th International Conference on {AI} in Cybersecurity ({ICAIC})},
	author = {Patel, Advait and Pandey, Pravin and Ragothaman, Hariharan and Molleti, Ramasankar and Peddinti, Diwakar Reddy},
	urldate = {2025-03-31},
	date = {2025-02-05},
	note = {Conference Name: 2025 {IEEE} 4th International Conference on {AI} in Cybersecurity ({ICAIC})
{ISBN}: 9798331518882
Place: Houston, {TX}, {USA}
Publisher: {IEEE}},
}

@article{seth_ai_2025,
	title = {{AI} and Generative {AI}-Driven Automation for Multi-Cloud and Hybrid Cloud Architectures: Enhancing Security, Performance, and Operational Efficiency},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10903928/},
	doi = {10.1109/CCWC62904.2025.10903928},
	shorttitle = {{AI} and Generative {AI}-Driven Automation for Multi-Cloud and Hybrid Cloud Architectures},
	abstract = {The emergence of cloud and hybrid cloud structures presents {eCommerce} firms with the adaptability and robustness needed to manage expansion and varying user requirements effectively. However, this also brings about challenges concerning security enhancements, distribution of workloads, and cost-effectiveness optimization. Traditional cloud management models often need help to meet these evolving demands efficiently. This research presents a system that leverages Artificial Intelligence ({AI}) and Generative {AI} (Gen {AI}) to effectively automate and enhance cloud and hybrid infrastructures for ecommerce websites. The system adapts infrastructure to traffic times like holidays or sales events by utilizing {AI} to scale resources as needed. It conserves resources during low user activity periods such as overnight. Ensuring optimal system performance and availability during peak traffic times while cutting costs during traffic periods is essential for cost-effectiveness and efficient resource management. In addition, {AI}-powered security automation safeguards against changing cyber dangers, and compliance automation guarantees conformity with rules like {PCI} {DSS} for payment handling. This report also delves into merging Gen {AI} into cloud coordination systems, facilitating workflows, and enhancing {eCommerce} processes. The outcome is a significant drop in operational expenses, a quicker service rollout, and decreased security breaches. Through real-world {eCommerce} case studies, this paper provides actionable insights for cloud engineers and architects on leveraging {AI}-driven cloud management to enhance performance, security, and cost-efficiency in multi-cloud and hybrid environments, ensuring seamless user experiences and business continuity.},
	pages = {00784--00793},
	journaltitle = {2025 {IEEE} 15th Annual Computing and Communication Workshop and Conference ({CCWC})},
	author = {Seth, Dhruv Kumar and Ratra, Karan Kumar and Sundareswaran, Aneeshkumar P},
	urldate = {2025-04-08},
	date = {2025-01-06},
	note = {Conference Name: 2025 {IEEE} 15th Annual Computing and Communication Workshop and Conference ({CCWC})
{ISBN}: 9798331507695
Place: Las Vegas, {NV}, {USA}
Publisher: {IEEE}},
}

@article{khanna_enhancing_2024,
	title = {{ENHANCING} {CLOUD} {SECURITY} {WITH} {GENERATIVE} {AI}: {EMERGING} {STRATEGIES} {AND} {APPLICATIONS}},
	volume = {3},
	issn = {2295-5152},
	url = {https://iaeme.com/Home/article_id/JARET_03_01_021},
	shorttitle = {{ENHANCING} {CLOUD} {SECURITY} {WITH} {GENERATIVE} {AI}},
	abstract = {This article explores the potential of generative {AI} for enhancing cloud security. With the rapid adoption of cloud technologies and the increasing sophistication of cyber threats, traditional security measures often struggle to keep pace. Generative {AI}, with its ability to learn from vast amounts of data and generate intelligent outputs, presents a powerful tool to address these challenges. The article delves into the fundamentals of generative {AI} and its specific applications in cloud security, including anomaly detection, threat intelligence, and automated response mechanisms. It also discusses the challenges and future directions in this field, highlighting the need for large and diverse datasets, addressing adversarial attacks, improving model interpretability, and considering the ethical implications of using generative {AI} in cloud security.},
	pages = {234--244},
	number = {1},
	journaltitle = {{JARET}},
	author = {Khanna, Karan},
	urldate = {2025-04-08},
	date = {2024-06-14},
	langid = {english},
	note = {Number: 1
Publisher: {IAEME} Publication},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/WATZ45IV/Khanna - 2024 - ENHANCING CLOUD SECURITY WITH GENERATIVE AI EMERGING STRATEGIES AND APPLICATIONS.pdf:application/pdf},
}

@online{noauthor_securing_2023,
	title = {Securing generative {AI}: An introduction to the Generative {AI} Security Scoping Matrix {\textbar} {AWS} Security Blog},
	url = {https://aws.amazon.com/blogs/security/securing-generative-ai-an-introduction-to-the-generative-ai-security-scoping-matrix/},
	shorttitle = {Securing generative {AI}},
	urldate = {2025-04-08},
	date = {2023-10-19},
	langid = {american},
	note = {Section: Amazon Bedrock},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/748SVXTA/securing-generative-ai-an-introduction-to-the-generative-ai-security-scoping-matrix.html:text/html},
}

@report{tabassi_artificial_2023,
	location = {Gaithersburg, {MD}},
	title = {Artificial Intelligence Risk Management Framework ({AI} {RMF} 1.0)},
	url = {http://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf},
	abstract = {As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the {AI} {RMF} is to offer a resource to the organizations designing, developing, deploying, or using {AI} systems to help manage the many risks of {AI} and promote trustworthy and responsible development and use of {AI} systems. The Framework is intended to be voluntary, rights-preserving, non-sector specific, and use-case agnostic, providing flexibility to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework.   The {AI} {RMF} is intended to be practical, to adapt to the {AI} landscape as {AI} technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from {AI} while also being protected from its potential harms.},
	pages = {NIST AI 100--1},
	number = {{NIST} {AI} 100-1},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {Tabassi, Elham},
	urldate = {2025-04-08},
	date = {2023-01-26},
	langid = {english},
	doi = {10.6028/NIST.AI.100-1},
	file = {PDF:/Users/d.veragilliard/Zotero/storage/IFJQD782/Tabassi - 2023 - Artificial Intelligence Risk Management Framework (AI RMF 1.0).pdf:application/pdf;PDF:/Users/d.veragilliard/Zotero/storage/5EPR3V9L/Tabassi - 2023 - Artificial Intelligence Risk Management Framework (AI RMF 1.0).pdf:application/pdf},
}

@article{nyoto_cyber_2024,
	title = {Cyber Security Risks in the Rapid Development of Generative Artificial Intelligence: A Systematic Literature Review},
	volume = {1},
	issn = {3063-0630},
	url = {https://journal.unilak.ac.id/index.php/ComniTech/article/view/24539},
	shorttitle = {Cyber Security Risks in the Rapid Development of Generative Artificial Intelligence},
	abstract = {This study aims to identify the cybersecurity risks arising from the use of Generative Artificial Intelligence ({GenAI}). By employing a systematic literature review ({SLR}) method and following the {PRISMA} 2020 guidelines, this research systematically selects and analyzes relevant literature to discover and understand the risks associated with the use of {GenAI}. From the seventeen studies successfully collected and reviewed, various cybersecurity risks were identified, including phishing attacks, social engineering, ransomware, malware, deepfakes, misinformation, data leakage, misuse of personal data, executable attack code generation, privacy risks, and intellectual property violations. These findings provide crucial insights into the potential threats that may emerge from the irresponsible use of {GenAI}. The study is designed to offer valuable information for various stakeholders in their risk mitigation efforts and in the development of relevant regulations concerning the ethical use of {GenAI}. It is hoped that these findings will serve as a solid foundation for developing more effective security strategies and policies to address the challenges posed by this technology, and encourage the implementation of improved protective measures to tackle emerging risks.},
	pages = {57--66},
	number = {2},
	journaltitle = {{ComniTech} : Journal of Computational Intelligence and Informatics},
	author = {Nyoto, Rebecca La Volla and Devega, Mariza and Nyoto, Nyoto},
	urldate = {2025-04-08},
	date = {2024-12-29},
	langid = {english},
	keywords = {cybersecurity, generative artificial intelligence ({GenAI}), systematic literature review.},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/9VNR6T4Q/Nyoto et al. - 2024 - Cyber Security Risks in the Rapid Development of Generative Artificial Intelligence A Systematic Li.pdf:application/pdf},
}

@misc{yigit_review_2024,
	title = {Review of Generative {AI} Methods in Cybersecurity},
	url = {http://arxiv.org/abs/2403.08701},
	doi = {10.48550/arXiv.2403.08701},
	abstract = {Over the last decade, Artificial Intelligence ({AI}) has become increasingly popular, especially with the use of chatbots such as {ChatGPT}, Gemini, and {DALL}-E. With this rise, large language models ({LLMs}) and Generative {AI} ({GenAI}) have also become more prevalent in everyday use. These advancements strengthen cybersecurity's defensive posture and open up new attack avenues for adversaries as well. This paper provides a comprehensive overview of the current state-of-the-art deployments of {GenAI}, covering assaults, jailbreaking, and applications of prompt injection and reverse psychology. This paper also provides the various applications of {GenAI} in cybercrimes, such as automated hacking, phishing emails, social engineering, reverse cryptography, creating attack payloads, and creating malware. {GenAI} can significantly improve the automation of defensive cyber security processes through strategies such as dataset construction, safe code development, threat intelligence, defensive measures, reporting, and cyberattack detection. In this study, we suggest that future research should focus on developing robust ethical norms and innovative defense mechanisms to address the current issues that {GenAI} creates and to also further encourage an impartial approach to its future application in cybersecurity. Moreover, we underscore the importance of interdisciplinary approaches further to bridge the gap between scientific developments and ethical considerations.},
	number = {{arXiv}:2403.08701},
	publisher = {{arXiv}},
	author = {Yigit, Yagmur and Buchanan, William J. and Tehrani, Madjid G. and Maglaras, Leandros},
	urldate = {2025-04-08},
	date = {2024-03-19},
	eprinttype = {arxiv},
	eprint = {2403.08701 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/93NDWF8C/Yigit et al. - 2024 - Review of Generative AI Methods in Cybersecurity.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/P2VFTJVJ/2403.html:text/html},
}

@incollection{feffer_red-teaming_2025,
	title = {Red-Teaming for Generative {AI}: Silver Bullet or Security Theater?},
	shorttitle = {Red-Teaming for Generative {AI}},
	abstract = {In response to rising concerns surrounding the safety, security, and trustworthiness of Generative {AI} ({GenAI}) models, practitioners and regulators alike have pointed to {AI} red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite {AI} red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the {AI} industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for {AI} red-teaming practices. Our analysis reveals that prior methods and practices of {AI} red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing {GenAI} harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard {AI}, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative {AI}, we synthesize our recommendations into a question bank meant to guide and scaffold future {AI} red-teaming practices.},
	pages = {421--437},
	booktitle = {Proceedings of the 2024 {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
	publisher = {{AAAI} Press},
	author = {Feffer, Michael and Sinha, Anusha and Deng, Wesley H. and Lipton, Zachary C. and Heidari, Hoda},
	urldate = {2025-04-08},
	date = {2025-02-07},
}

@article{vootkuri_multi-cloud_2024,
	title = {Multi-Cloud Data Strategy \& Security for Generative {AI}},
	volume = {12},
	abstract = {The rapid growth of generative artificial intelligence has fundamentally changed the requirements for cloud computing infrastructure, including requisites such as innovative approaches to resource management and development strategies. A multi-cloud strategy involves leveraging multiple cloud providers to execute an application to optimize data management, storage, and processing capabilities for training and inference. This comprehensive research paper aims to study the evolving paradigm of multi-cloud strategies tailored for Generative Artificial intelligence (Gen-{AI}) using the multi-cloud platforms to enhance their infrastructure, reliability, and security and how the costs are optimized by effectively reducing vendor lock-ins and provide a chance to strategically leverage a variety of providers and their skills to meet specific company demands. The paper demonstrates multi cloud data strategy and security frameworks for Gen {AI} applications. The research discusses how to protect {GenAI} using different strategies in enterprise ecosystems.},
	number = {1},
	author = {Vootkuri, Chaitanya},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/d.veragilliard/Zotero/storage/UWZHX4FU/Vootkuri - 2024 - Multi-Cloud Data Strategy & Security for Generative AI.pdf:application/pdf},
}

@article{sushil_prabhu_prabhakaran_integration_2024,
	title = {Integration Patterns in Unified {AI} and Cloud Platforms: A Systematic Review of Process Automation Technologies},
	volume = {10},
	rights = {https://creativecommons.org/licenses/by/4.0},
	issn = {2456-3307},
	url = {https://ijsrcseit.com/index.php/home/article/view/CSEIT241061229},
	doi = {10.32628/CSEIT241061229},
	shorttitle = {Integration Patterns in Unified {AI} and Cloud Platforms},
	abstract = {This article comprehensively analyzes unified {AI} and cloud platforms, examining their role in transforming process automation and decision systems across industries. The article investigates the architectural frameworks and integration patterns that enable the convergence of {AI} tools, machine learning operations, and workflow orchestration within cloud-native environments. The article explores key innovations, including federated {AI} implementations, real-time data processing architectures, and multi-cloud integration patterns. It provides insights into their practical applications across finance, healthcare, retail, and manufacturing sectors. The article identifies critical success factors in platform implementation, including integrating {MLOps} frameworks, automated decision engines, and compliance tools for {AI} governance. Through case study analysis and architectural evaluation, we demonstrate how unified platforms address traditional challenges in {AI} deployment while enabling scalable, cost-efficient solutions. The findings reveal emerging patterns in platform architecture that facilitate seamless integration of edge computing, real-time analytics, and distributed {AI} systems, contributing to the broader understanding of enterprise {AI} implementation strategies. This article provides valuable insights for researchers and practitioners in cloud engineering, artificial intelligence, and systems integration while highlighting future directions for platform evolution and standardization.},
	pages = {1932--1940},
	number = {6},
	journaltitle = {Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol},
	author = {{Sushil Prabhu Prabhakaran}},
	urldate = {2025-04-08},
	date = {2024-12-15},
}

@article{bringhenti_security_2023,
	title = {Security automation for multi-cluster orchestration in Kubernetes},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10175419/},
	doi = {10.1109/NetSoft57336.2023.10175419},
	abstract = {In the latest years, multi-domain Kubernetes architectures composed of multiple clusters have been getting more frequent, so as to provide higher workload isolation, resource availability flexibility and scalability for application deployment. However, manually configuring their security may lead to inconsistencies among policies defined in different clusters, or it may require knowledge that the administrator of each domain cannot have. Therefore, this paper proposes an automatic approach for the automatic generation of the network security policies to be deployed in each cluster of a multi-domain Kubernetes deployment. The objectives of this approach are to reduce of configuration errors that human administrators commonly make, and to create transparent cross-cluster communications. This approach has been implemented as a framework named Multi-Cluster Orchestrator, which has been validated in realistic use cases to assess its benefits to Kubernetes orchestration.},
	pages = {480--485},
	journaltitle = {2023 {IEEE} 9th International Conference on Network Softwarization ({NetSoft})},
	author = {Bringhenti, Daniele and Sisto, Riccardo and Valenza, Fulvio},
	urldate = {2025-04-08},
	date = {2023-06-19},
	note = {Conference Name: 2023 {IEEE} 9th International Conference on Network Softwarization ({NetSoft})
{ISBN}: 9798350399806
Place: Madrid, Spain
Publisher: {IEEE}},
}

@article{hammar_digital_2023,
	title = {Digital Twins for Security Automation},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10154288/},
	doi = {10.1109/NOMS56928.2023.10154288},
	abstract = {We present a novel emulation system for creating high-fidelity digital twins of {IT} infrastructures. The digital twins replicate key functionality of the corresponding infrastructures and allow to play out security scenarios in a safe environment. We show that this capability can be used to automate the process of finding effective security policies for a target infrastructure. In our approach, a digital twin of the target infrastructure is used to run security scenarios and collect data. The collected data is then used to instantiate simulations of Markov decision processes and learn effective policies through reinforcement learning, whose performances are validated in the digital twin. This closed-loop learning process executes iteratively and provides continuously evolving and improving security policies. We apply our approach to an intrusion response scenario. Our results show that the digital twin provides the necessary evaluative feedback to learn near-optimal intrusion response policies.},
	pages = {1--6},
	journaltitle = {{NOMS} 2023-2023 {IEEE}/{IFIP} Network Operations and Management Symposium},
	author = {Hammar, Kim and Stadler, Rolf},
	urldate = {2025-04-08},
	date = {2023-05-08},
	note = {Conference Name: {NOMS} 2023-2023 {IEEE}/{IFIP} Network Operations and Management Symposium
{ISBN}: 9781665477161
Place: Miami, {FL}, {USA}
Publisher: {IEEE}},
}

@article{surathunmanun_exploring_2024,
	title = {Exploring the Role of Generative Artificial Intelligence in the Energy Sector: A Comprehensive Literature Review},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10795598/},
	doi = {10.1109/ICUE63019.2024.10795598},
	shorttitle = {Exploring the Role of Generative Artificial Intelligence in the Energy Sector},
	abstract = {Generative Artificial Intelligence ({GenAI}) enhances productivity by creating data, forecasting, optimizing, and understanding human language. In the energy sector, it is projected to have a \$240 billion global economic impact, though research remains limited. This paper reviews {GenAI}'s benefits, challenges, and research gaps in the energy sector, also focusing on climate change efforts. A {PRISMA}-{SCR}-based literature review from January 2022 to May 2024 was conducted using {IEEE} Xplore, {ScienceDirect}, {ACM} Digital Library, and Google Scholar. {GenAI} tools extracted data, verified by researchers. Analysis of 33 papers shows {GenAI} excels in knowledge integration and prediction. It generates synthetic electricity demand data, manages grids, forecasts energy demand, and optimizes renewable energy systems. Key challenges include hallucinations, data biases, privacy concerns, misuse, and system errors. Solutions involve improving training data, system fine-tuning, human oversight, and security measures. Research gaps include synthetic data realism, model evaluation standards, and integrating {GenAI} with blockchain and {IoT}.},
	pages = {1--11},
	journaltitle = {2024 International Conference on Sustainable Energy: Energy Transition and Net-Zero Climate Future ({ICUE})},
	author = {Surathunmanun, Surasak and Ongsakul, Weerakorn and Singh, Jai Govind},
	urldate = {2025-04-08},
	date = {2024-10-21},
	note = {Conference Name: 2024 International Conference on Sustainable Energy: Energy Transition and Net-Zero Climate Future ({ICUE})
{ISBN}: 9798331517076
Place: Pattaya City, Thailand
Publisher: {IEEE}},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/T6QIN8FX/69dd137c561619884ee4aecd95f81a351b7ce328.html:text/html},
}

@online{noauthor_securing_nodate,
	title = {Securing Generative {AI}: Introduction to the Generative {AI} Security Scoping Matrix},
	url = {https://aws.amazon.com/ai/generative-ai/security/scoping-matrix/},
	shorttitle = {Securing Generative {AI}},
	abstract = {Explore how to secure generative {AI} applications with {AWS}. This webpage introduces the Generative {AI} Security Scoping Matrix, providing essential guidelines for securing {AI} infrastructure, models, and applications. Learn best practices for implementing effective security measures to protect your {AI} investments.},
	titleaddon = {Amazon Web Services, Inc.},
	urldate = {2025-04-09},
	langid = {american},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/PPEMLSLJ/scoping-matrix.html:text/html},
}

@online{resources_australias_2024,
	title = {Australia’s {AI} Ethics Principles {\textbar} Australia’s Artificial Intelligence Ethics Principles {\textbar} Department of Industry Science and Resources},
	url = {https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-principles/australias-ai-ethics-principles},
	abstract = {Consider these voluntary principles to ensure {AI} is safe, secure and reliable.},
	titleaddon = {https://www.industry.gov.au/node/91877},
	author = {Resources, Department of Industry Science and},
	urldate = {2025-04-09},
	date = {2024-10-11},
	langid = {australian},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/EYMMH8PS/australias-ai-ethics-principles.html:text/html},
}

@online{noauthor_isoiec_nodate,
	title = {{ISO}/{IEC} 38500:2024},
	url = {https://www.iso.org/standard/81684.html},
	shorttitle = {{ISO}/{IEC} 38500},
	abstract = {Information technology — Governance of {IT} for the organization},
	titleaddon = {{ISO}},
	urldate = {2025-04-09},
	langid = {english},
	file = {d11300:/Users/d.veragilliard/Zotero/storage/HBJ39RQS/d11300.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/UU4JNT3B/81684.html:text/html},
}

@article{zhou_study_2010,
	title = {The study on network intrusion detection system of Snort},
	doi = {10.1109/ICNDS.2010.5479341},
	abstract = {Network security is a complex and systematic project. The intrusion detection system is the first line of defense against network security. Snort is a famous intrusion detection system in the field of open source software. It is widely used in the intrusion prevention and detection domain in the world. In this paper, we explain how Snort implements the intrusion detection, which includes building the compiling environment and analysizing the work-flow and rule tree. This paper will provide a valuable reference for the study of Snort.},
	author = {Zhou, Zhimin and Zhongwen, Chen and Tiecheng, Zhou and Xiaohui, Guan},
	date = {2010-05-01},
}

@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	issn = {1756-1833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.n71},
	doi = {10.1136/bmj.n71},
	shorttitle = {The {PRISMA} 2020 statement},
	abstract = {{POINTS} To ensure a systematic review is valuable to users, authors should prepare a transparent, complete, and accurate account of why the review was done, what they did, and what they found The {PRISMA} 2020 statement provides updated reporting guidance for systematic reviews that reflects advances in methods to identify, select, appraise, and synthesise studies The {PRISMA} 2020 statement consists of a 27-item checklist, an expanded checklist that details reporting recommendations for each item, the {PRISMA} 2020 abstract checklist, and revised flow diagrams for original and updated reviews We anticipate that the {PRISMA} 2020 statement will benefit authors, editors, and peer reviewers of systematic reviews, and different users of reviews, including guideline developers, policy makers, healthcare providers, patients, and other stakeholders {BMJ}: {firPsrtopteucbtliesdhbeyd} caosp1y0r.i1g1h3t,6/ibncmlju.ndi7n1gofnor2u9sMesarrcehla2te02d1t.{oDtoexwtnalonadddeadtafromimnihnttg},{psA}:I//twrawinwi.nbgm, ja.cnodms/imoinla1r2teAcphrinlo2l0o2gi5ebs.y guest.},
	pages = {n71},
	journaltitle = {{BMJ}},
	author = {Page, Matthew J and {McKenzie}, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hróbjartsson, Asbjørn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and {McDonald}, Steve and {McGuinness}, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
	urldate = {2025-04-12},
	date = {2021-03-29},
	langid = {english},
	file = {PDF:/Users/d.veragilliard/Zotero/storage/29PKSQ2J/Page et al. - 2021 - The PRISMA 2020 statement an updated guideline for reporting systematic reviews.pdf:application/pdf},
}

@article{mell_nist_nodate,
	title = {The {NIST} Definition of Cloud Computing},
	author = {Mell, Peter and Grance, Timothy},
	langid = {english},
	file = {PDF:/Users/d.veragilliard/Zotero/storage/63CCNWW2/Mell and Grance - The NIST Definition of Cloud Computing.pdf:application/pdf},
}

@misc{abimbola_cloud_2021,
	title = {Cloud Computing Concept and Roots},
	url = {http://arxiv.org/abs/2102.00981},
	doi = {10.48550/arXiv.2102.00981},
	abstract = {Cloud computing is a particular implementation of distributed computing. It inherited many properties of distributed computing such as scalability, reliability and distribution transparency. The transparency middle layer abstracts the underlying platform away from the end user. Virtualization technology is the foundation of Cloud computing. Virtual machine provides abstraction of the physical server resources and securely isolates different users in multi-tenant environment. To the Cloud services consumer, all the computing power and resources are accessed through high speed internet access by client platforms. This eliminates the cost to build and maintain local data center. Resource pooling and rapid elasticity are the main characters of Cloud computing. The scalability of Cloud computing comes from resources which can span multiple data centers and geographic regions. There is virtually no limitation on the amount of resources available from Cloud. New processing and storage resources can be added into the Cloud resource pool seamlessly.},
	number = {{arXiv}:2102.00981},
	publisher = {{arXiv}},
	author = {Abimbola, Bola},
	urldate = {2025-04-15},
	date = {2021-02-09},
	eprinttype = {arxiv},
	eprint = {2102.00981 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/9ITFYC7G/Abimbola - 2021 - Cloud Computing Concept and Roots.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/6NP6GTQI/2102.html:text/html},
}

@inproceedings{nikita_enterprise_2023,
	title = {Enterprise Security Architecture For Cloud Computing: A Review},
	url = {https://ieeexplore.ieee.org/document/10307676},
	doi = {10.1109/ICCCNT56998.2023.10307676},
	shorttitle = {Enterprise Security Architecture For Cloud Computing},
	abstract = {Cloud computing has transformed {IT} in recent years by isolating application and data resources from foundational structures. Providing high-quality service, however, necessitates guaranteeing cloud computing security. Security risks develop when apps run outside of the defined firewall and into the public domain. Any security compromise in a cloud component might be catastrophic for both the organization (the client) and the supplier. As a result, we present in this study a framework and technique for cloud security that can detect cloud computing security concerns.},
	eventtitle = {2023 14th International Conference on Computing Communication and Networking Technologies ({ICCCNT})},
	pages = {1--7},
	booktitle = {2023 14th International Conference on Computing Communication and Networking Technologies ({ICCCNT})},
	author = {Nikita, Nikita and Parashar, Gaurav},
	urldate = {2025-04-15},
	date = {2023-07},
	note = {{ISSN}: 2473-7674},
	keywords = {Cloud computing, Cloud computing security, Computer architecture, Data protection, Firewalls (computing), Network security, Organizations, Security Architecture, Security Frameworks, Stars},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/WADIF3DE/10307676.html:text/html},
}

@article{moura_review_nodate,
	title = {Review and Analysis of Networking Challenges in Cloud Computing},
	author = {Moura, Jose},
	langid = {english},
	file = {PDF:/Users/d.veragilliard/Zotero/storage/JRK8RUIK/Moura - Review and Analysis of Networking Challenges in Cloud Computing.pdf:application/pdf},
}

@online{blog_blueprint_2024,
	title = {Blueprint for {AI} Agents in Cybersecurity},
	url = {https://www.cybersec-automation.com/p/blueprint-for-ai-agents-in-cybersecurity},
	abstract = {Leveraging {AI} Agents to Evolve Cybersecurity Practices},
	titleaddon = {Cyber Security Automation and Orchestration},
	author = {Blog, Cyber Automation \{and\} Autonomous {SOC}},
	urldate = {2025-04-18},
	date = {2024-07-11},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/Q8TCUBKF/blueprint-for-ai-agents-in-cybersecurity.html:text/html},
}

@online{noauthor_agentic_nodate,
	title = {Agentic {AI} Threat Modeling Framework: {MAESTRO} {\textbar} {CSA}},
	url = {https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro},
	shorttitle = {Agentic {AI} Threat Modeling Framework},
	abstract = {{MAESTRO} (Multi-Agent Environment, Security, Threat, Risk, \& Outcome) is a novel threat modeling framework for Agentic {AI}. Assess risks across the {AI} lifecycle.},
	urldate = {2025-04-18},
}

@online{noauthor_cyber_nodate,
	title = {Cyber Swarm: the rise of the machines Potential application of {AI} agents in offensive and defensive cybersecurity},
	url = {https://eviden.com/publications/digital-security-magazine/ai-and-cybersecurity/ai-agents-system-2-thinking/},
	shorttitle = {Cyber Swarm},
	abstract = {Explore the concept of System 2 thinking, its application through multi-agent systems, and their roles in strengthening offensive and defensive cybersecurity},
	titleaddon = {Eviden},
	urldate = {2025-04-18},
	langid = {american},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/D5NY9QWF/ai-agents-system-2-thinking.html:text/html},
}

@article{ramasankar_molleti_automated_2024,
	title = {Automated threat detection and response using {LLM} agents},
	volume = {24},
	issn = {25819615},
	url = {https://wjarr.com/node/15847},
	doi = {10.30574/wjarr.2024.24.2.3329},
	abstract = {The increase of cyber threats from individual cases to a worldwide problem is the reason why people have shifted their cybersecurity perspectives. Basic defense processes, originally well understood and effective, fail to match modern attacks’ complexity and velocity. Taking into consideration {LLMs} as a recent addition to {AI}, this paper aims at discussing their application in integrating threat detection and response automation systems. As a result, {LLMs}, which have higher capabilities for natural language processing, deliver a revolutionary perspective regarding cybersecurity. Since {LLM} agents can review massive amounts of security data, distinguish patterns, and create contextually appropriate responses, they can bridge the gap between emerging threats and stable security systems. The paper examines the tools used by {LLM} agents, such as natural language processing to analyse the logs, contextual anomaly detection, pattern identification in network traffic, and the analysis of the user’s behaviour. Also, it describes how {LLM} agents can support automated threat handling in the context of threat identification, alert prioritization, context-driven response generation, security policy enforcement, and threat handling. The integration of {LLM} agents into already known systems, including {SIEM} systems and {AI}-Ops platforms, is also considered, which allows for further conclusions on the opportunities to create proactive cybersecurity systems. However, open dilemmas such as adversarial attacks and interpretability are still present, the future for {LLM} agents in cybersecurity is still bright, and there are more possibilities in multi-modal threat analysis and quantum-safe {LLM}-based cryptography.},
	pages = {079--090},
	number = {2},
	journaltitle = {World J. Adv. Res. Rev.},
	author = {{Ramasankar Molleti} and {Vinod Goje} and {Puneet Luthra} and {Prathap Raghavan}},
	urldate = {2025-04-18},
	date = {2024-11-30},
}

@article{kaswan_generative_2023,
	title = {Generative {AI}: A Review on Models and Applications},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10421601/},
	doi = {10.1109/ICCSAI59793.2023.10421601},
	shorttitle = {Generative {AI}},
	abstract = {Generative Artificial Intelligence ({AI}) stands as a transformative paradigm in machine learning, enabling the creation of complex and realistic data from latent representations. This review paper comprehensively surveys the landscape of Generative {AI}, encompassing its foundational concepts, diverse models, training methodologies, applications, challenges, recent advancements, evaluation metrics, and ethical dimensions. The paper begins by introducing Generative {AI}'s significance across various domains, presenting its pivotal role in producing synthetic data with applications spanning image synthesis, text generation, music composition, drug discovery, and more. The objectives lie in elucidating the foundational concepts, delving into model intricacies, unveiling the training procedures, exploring its application landscape, addressing challenges, envisioning future directions, and discussing ethical ramifications. The foundational section elucidates the diverse array of generative models, including Generative Adversarial Networks ({GANs}), Variational Autoencoders ({VAEs}), flow-based models, Generative Reinforcement Learning ({GRL}), and advanced hybrid architectures. Subsequently, evaluation metrics ranging from Inception Score to perceptual similarity metrics and human evaluations are surveyed to assess generative model performance. Finally, ethical considerations underscore the necessity for addressing biases, misuse, intellectual property concerns, and the call for responsible {AI} development and regulation in the Generative {AI} landscape.},
	pages = {699--704},
	journaltitle = {2023 International Conference on Communication, Security and Artificial Intelligence ({ICCSAI})},
	author = {Kaswan, Kuldeep Singh and Dhatterwal, Jagjit Singh and Malik, Kiran and Baliyan, Anupam},
	urldate = {2025-04-18},
	date = {2023-11-23},
	note = {Conference Name: 2023 International Conference on Communication, Security and Artificial Intelligence ({ICCSAI})
{ISBN}: 9798350369960
Place: Greater Noida, India
Publisher: {IEEE}},
}

@article{gatla_advancements_2024,
	title = {Advancements in Generative {AI}: Exploring Fundamentals and Evolution},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10594003/},
	doi = {10.1109/ICECCC61767.2024.10594003},
	shorttitle = {Advancements in Generative {AI}},
	abstract = {Generative Artificial Intelligence ({AI}) has emerged as a transformative field with far-reaching implications across various domains. This review manuscript provides a advancements in generative {AI}, focusing on its fundamental concepts, methodologies, and evolutionary trends. We begin by elucidating the foundational principles underlying generative {AI} techniques, including autoregressive models, Variational Autoencoders ({VAEs}), and Generative Adversarial Networks ({GANs}). Subsequently, we delve into the evolution of generative {AI}, discussing recent advancements, challenges, and potential future directions. Through an in-depth analysis of research literature and real-world applications, this manuscript aims to offer insights into the current landscape of generative {AI} and its profound impact on diverse sectors.},
	pages = {1--5},
	journaltitle = {2024 International Conference on Electronics, Computing, Communication and Control Technology ({ICECCC})},
	author = {Gatla, Ranjith Kumar and Gatla, Anitha and Sridhar, Patti and Kumar, Devineni Gireesh and Rao, D S Naga Malleswara},
	urldate = {2025-04-18},
	date = {2024-05-02},
	note = {Conference Name: 2024 International Conference on Electronics, Computing, Communication and Control Technology ({ICECCC})
{ISBN}: 9798350371802
Place: Bengaluru, India
Publisher: {IEEE}},
}

@online{noauthor_generative_nodate,
	title = {Generative Artificial Intelligence: A Systematic Review and Applications},
	url = {https://arxiv.org/html/2405.11029v1},
	urldate = {2025-04-18},
	file = {Generative Artificial Intelligence\: A Systematic Review and Applications:/Users/d.veragilliard/Zotero/storage/FEFNVU47/2405.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-04-18},
	date = {2023-08-02},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/UE4FQBLZ/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/4Y7WG8PL/1706.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {{arXiv}:1406.2661},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2025-04-18},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/BTTJYYPT/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/IJBL4GLQ/1406.html:text/html},
}

@article{vijay_ramamoorthi_review_2024,
	title = {A Review of {AI} and Multi-Agent Systems for Cloud Performance and Security},
	volume = {10},
	rights = {https://creativecommons.org/licenses/by/4.0},
	issn = {2456-3307},
	url = {https://ijsrcseit.com/index.php/home/article/view/CSEIT24105112},
	doi = {10.32628/CSEIT24105112},
	abstract = {Cloud computing has become a critical backbone for distributed systems, offering scalability and flexibility across diverse industries. However, ensuring optimal performance and robust security in such dynamic environments presents significant challenges, including inefficient task scheduling, suboptimal resource utilization, and persistent security threats such as data breaches and Distributed Denial of Service ({DDoS}) attacks. This paper examines the transformative potential of Artificial Intelligence ({AI}) and Multi-Agent Systems ({MAS}) in addressing these complexities. {AI}-driven solutions, including real-time anomaly detection, predictive analytics, and resource optimization, are combined with {MAS} frameworks that leverage decentralized, autonomous agents for distributed decision-making and proactive threat mitigation. The integration of {AI} and {MAS} enables dynamic adaptation to workload fluctuations, enhances resource efficiency, and provides robust security measures in multi-cloud and large-scale systems. The paper further explores key challenges in implementing these technologies, such as scalability and integration across heterogeneous environments, and identifies promising research directions to advance their adoption. By synthesizing empirical evidence and recent advancements, this study highlights the critical role of {AI} and {MAS} in shaping the future of cloud performance and security.},
	pages = {326--337},
	number = {4},
	journaltitle = {Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol},
	author = {{Vijay Ramamoorthi}},
	urldate = {2025-04-18},
	date = {2024-07-15},
}

@article{kalava_best_2024,
	title = {Best {AI} Framework Guide: Build Production-Ready Agents That Work},
	volume = {03},
	issn = {25836129},
	url = {https://isjem.com/download/best-ai-framework-guide-build-production-ready-agents-that-work/},
	doi = {10.55041/ISJEM02191},
	shorttitle = {Best {AI} Framework Guide},
	abstract = {Artificial Intelligence ({AI}) frameworks serve as the foundation for building scalable, production-ready {AI} agents
that enable automation, decision-making, and intelligent interactions. This paper explores the architecture, core
components, and best practices for selecting, deploying, and optimizing {AI} agent frameworks. Additionally, it
addresses security considerations, compliance standards, performance enhancements, and real-world integration
strategies for enterprise adoption.
Keywords
Artificial Intelligence, {AI} Agents, Automation, Scalability, Security, Optimization, Enterprise {AI}},
	pages = {1--9},
	number = {12},
	journaltitle = {{ISJEM}},
	author = {Kalava, Sudheer Peddineni},
	urldate = {2025-04-18},
	date = {2024-12-17},
}

@online{hansen_introducing_2023,
	title = {Introducing Google’s Secure {AI} Framework},
	url = {https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/},
	abstract = {Today Google released released the Secure {AI} Framework to help collaboratively secure {AI} technology.},
	titleaddon = {Google},
	author = {Hansen, Royal and Venables, Phil},
	urldate = {2025-04-25},
	date = {2023-06-08},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/TC3VRHGX/introducing-googles-secure-ai-framework.html:text/html},
}

@online{editor_llm_nodate,
	title = {{LLM} Applications Cybersecurity and Governance Checklist v1.1 - English},
	url = {https://genai.owasp.org/resource/llm-applications-cybersecurity-and-governance-checklist-english/},
	abstract = {The {OWASP} Top 10 for {LLM} Applications Cybersecurity and Governance Checklist is for leaders across executive, tech, cybersecurity, privacy, compliance, and legal areas, {DevSecOps}, {MLSecOps}, and Cybersecurity teams and defenders. It is intended for people who are striving to stay ahead in the fast-moving {AI} world, aiming not just to leverage {AI} for corporate success […]},
	titleaddon = {{OWASP} Top 10 for {LLM} \& Generative {AI} Security},
	author = {Editor, {OWASPGenAIProject}},
	urldate = {2025-04-27},
	langid = {american},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/YDXV4DIE/llm-applications-cybersecurity-and-governance-checklist-english.html:text/html},
}

@online{editor_llm_nodate-1,
	title = {{LLM} and Generative {AI} Security Center of Excellence Guide},
	url = {https://genai.owasp.org/resource/llm-and-generative-ai-security-center-of-excellence-guide/},
	abstract = {As generative {AI} technologies evolve and integrate into various aspects of business and society, the need for robust governance, security, and policy management becomes paramount. Establishing a Center of Excellence ({COE}) for Generative {AI} Security aims to bring together diverse groups such as security, legal, data science, operations, and end-users to foster collaboration, develop best […]},
	titleaddon = {{OWASP} Top 10 for {LLM} \& Generative {AI} Security},
	author = {Editor, {OWASPGenAIProject}},
	urldate = {2025-04-27},
	langid = {american},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/9UE7J798/llm-and-generative-ai-security-center-of-excellence-guide.html:text/html},
}

@article{pillala_devsecops_2024,
	title = {{DevSecOps} Sentinel: {GenAI}-Driven Agentic Workflows for Comprehensive Supply Chain Security},
	volume = {18},
	rights = {Copyright (c) 2024 Gyani Pillala,Damoon Azarpazhooh,Scott Baxter},
	issn = {1913-8989},
	url = {https://ccsenet.org/journal/index.php/cis/article/view/0/51118},
	doi = {10.5539/cis.v18n1p39},
	shorttitle = {{DevSecOps} Sentinel},
	abstract = {A growing number of security challenges are born out of the complexity of modern software supply chains that span microservices, containerization, and cloud-native architectures. The increasing rate of new cyber-threats, and the need to quickly deploy software updates after a security incident, typically outpaces traditional {DevSecOps} security practices. In this paper, we propose a novel {DevSecOps} Sentinel system, which employs Generative {AI} ({GenAI}) driven agentic workflows to improve software supply chain security holistically.

In this paper, we elaborate on the architecture of {DevSecOps} Sentinel: by integrating cutting-edge {GenAI} models, and by deploying intelligent agentic workflows. Then we dive into how the system impacts our software development life cycle from code writing to production and beyond. Our results indicate that agentic workflows powered by {GenAI} are a viable method to tackle the intricate security issues of modern software supply chains. Integrating the analysis capability of {AI} and marrying this with the strengths that come from agentic systems, {DevSecOps} Sentinel reveals a way forward for organizations seeking to strengthen their security profile in an ever more hostile digital world - to build better software — faster, safer, and reliable.},
	pages = {p39},
	number = {1},
	journaltitle = {Computer and Information Science},
	author = {Pillala, Gyani and Azarpazhooh, Damoon and Baxter, Scott},
	urldate = {2025-04-27},
	date = {2024-12-20},
	langid = {english},
	note = {Number: 1},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/BPBW7V3V/Pillala et al. - 2024 - DevSecOps Sentinel GenAI-Driven Agentic Workflows for Comprehensive Supply Chain Security.pdf:application/pdf},
}

@misc{dash_zero-trust_2024,
	location = {Rochester, {NY}},
	title = {Zero-Trust Architecture ({ZTA}): Designing an {AI}-Powered Cloud Security Framework for {LLMs}' Black Box Problems},
	url = {https://papers.ssrn.com/abstract=4726625},
	doi = {10.2139/ssrn.4726625},
	shorttitle = {Zero-Trust Architecture ({ZTA})},
	abstract = {Businesses are becoming more interested in developing and testing Large Language Models ({LLMs}) in their own settings to support decision-making and growth as a result of the rapid emergence of {AI} and cloud computing. Here's the dilemma, though: to what extent do you believe these models and the data they were trained on? We don't know the feature list of an {LLM}, which presents the first obstacle when discussing trust and the reasons why there should be zero trust. Although it may seem a bit extreme, this is accurate for two reasons. When it comes to {GenAI} models nowadays, the more multimodal and more capabilities they have, the better. This way of thinking is great for exploring and confirming if {GenAI} can address a business problem, but it's a surefire way to run into trouble when attempting to put things into production in an organizational setting. An enterprise cybersecurity architecture known as a zero-trust architecture ({ZTA}) is built on the ideas of zero trust and is intended to stop data breaches, enhance privacy, and restrict internal lateral movement. This article discusses {ZTA}, its logical aspects, probable deployment scenarios, {AI} rules, threats and limitations in order to provide a detailed understanding of why enterprises must adapt a {ZTA} framework in a cloud-based environment for {AI} model deployment.},
	number = {4726625},
	publisher = {Social Science Research Network},
	author = {Dash, Bibhu},
	urldate = {2025-04-27},
	date = {2024-03-12},
	langid = {english},
	keywords = {{AI}-Powered framework, Black Box, {CCPA}, {GDPR}, {IPP}, {LLM}, {PDP}, Zero Trust},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/5ITUH79R/Dash - 2024 - Zero-Trust Architecture (ZTA) Designing an AI-Powered Cloud Security Framework for LLMs' Black Box.pdf:application/pdf},
}

@inproceedings{ismail_big_2025,
	title = {Big Data Architecture for Large Organizations},
	url = {https://www.semanticscholar.org/paper/Big-Data-Architecture-for-Large-Organizations-Ismail-Sengupta/4c954e46ecc91d41e554afba14d3a294b07b7e5e},
	abstract = {The exponential growth of big data has transformed how large organisations leverage information to drive innovation, optimise processes, and maintain competitive advantages. However, managing and extracting insights from vast, heterogeneous data sources requires a scalable, secure, and well-integrated big data architecture. This paper proposes a comprehensive big data framework that aligns with organisational objectives while ensuring flexibility, scalability, and governance. The architecture encompasses multiple layers, including data ingestion, transformation, storage, analytics, machine learning, and security, incorporating emerging technologies such as Generative {AI} ({GenAI}) and low-code machine learning. Cloud-based implementations across Google Cloud, {AWS}, and Microsoft Azure are analysed, highlighting their tools and capabilities. Additionally, this study explores advancements in big data architecture, including {AI}-driven automation, data mesh, and Data Ocean paradigms. By establishing a structured, adaptable framework, this research provides a foundational blueprint for large organisations to harness big data as a strategic asset effectively.},
	author = {Ismail, Fathima Nuzla and Sengupta, Abira and Amarasoma, Shanika},
	urldate = {2025-06-09},
	date = {2025-05-07},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/II2WH5JS/Ismail et al. - 2025 - Big Data Architecture for Large Organizations.pdf:application/pdf},
}

@misc{hayagreevan_security_2024,
	title = {Security of and by Generative {AI} platforms},
	url = {http://arxiv.org/abs/2410.13899},
	doi = {10.48550/arXiv.2410.13899},
	abstract = {This whitepaper highlights the dual importance of securing generative {AI} ({genAI}) platforms and leveraging {genAI} for cybersecurity. As {genAI} technologies proliferate, their misuse poses significant risks, including data breaches, model tampering, and malicious content generation. Securing these platforms is critical to protect sensitive data, ensure model integrity, and prevent adversarial attacks. Simultaneously, {genAI} presents opportunities for enhancing security by automating threat detection, vulnerability analysis, and incident response. The whitepaper explores strategies for robust security frameworks around {genAI} systems, while also showcasing how {genAI} can empower organizations to anticipate, detect, and mitigate sophisticated cyber threats.},
	number = {{arXiv}:2410.13899},
	publisher = {{arXiv}},
	author = {Hayagreevan, Hari and Khamaru, Souvik},
	urldate = {2025-06-09},
	date = {2024-10-15},
	eprinttype = {arxiv},
	eprint = {2410.13899 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/FUB78C6M/Hayagreevan and Khamaru - 2024 - Security of and by Generative AI platforms.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/Q2RG8AVP/2410.html:text/html},
}

@misc{noseevich_towards_2015,
	title = {Towards automated web application logic reconstruction for application level security},
	url = {http://arxiv.org/abs/1511.02564},
	doi = {10.48550/arXiv.1511.02564},
	abstract = {Modern overlay security mechanisms like Web Application Firewalls ({WAF}) suffer from inability to recognize custom high-level application logic and data objects, which results in low accuracy, high false positives rates, and overhelming manual effort for fine tuning. In this paper we propose an approach to web application modeling for security purposes that could help next-generation {WAFs} to adapt to specific web applications, and do it automatically whenever possible. We aim at creating multi-layer models that adequately simulate various aspects of web application functionality that are significant for intrusion detection and prevention, including request parsing and routing, reconstruction of actions and data objects, and action interdependencies.},
	number = {{arXiv}:1511.02564},
	publisher = {{arXiv}},
	author = {Noseevich, George and Gamayunov, Dennis},
	urldate = {2025-06-09},
	date = {2015-11-09},
	eprinttype = {arxiv},
	eprint = {1511.02564 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/C54WINRY/Noseevich and Gamayunov - 2015 - Towards automated web application logic reconstruction for application level security.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/F76YUPCQ/1511.html:text/html},
}

@inproceedings{desai_gen-ai_2024,
	title = {Gen-{AI} for User Safety: A Survey},
	url = {http://arxiv.org/abs/2411.06606},
	doi = {10.1109/BigData62323.2024.10825656},
	shorttitle = {Gen-{AI} for User Safety},
	abstract = {Machine Learning and data mining techniques (i.e. supervised and unsupervised techniques) are used across domains to detect user safety violations. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing {ML}/{DM} classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-{AI} techniques, along with their inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains. In this manuscript, we provide a comprehensive overview of the various work done while using Gen-{AI} techniques w.r.t user safety. In particular, we first provide the various domains (e.g. phishing, malware, content moderation, counterfeit, physical safety) across which Gen-{AI} techniques have been applied. Next, we provide how Gen-{AI} techniques can be used in conjunction with various data modalities i.e. text, images, videos, audio, executable binaries to detect violations of user-safety. Further, also provide an overview of how Gen-{AI} techniques can be used in an adversarial setting. We believe that this work represents the first summarization of Gen-{AI} techniques for user-safety.},
	pages = {5315--5324},
	booktitle = {2024 {IEEE} International Conference on Big Data ({BigData})},
	author = {Desai, Akshar Prabhu and Ravi, Tejasvi and Luqman, Mohammad and Sharma, Mohit and Kota, Nithya and Yadav, Pranjul},
	urldate = {2025-06-09},
	date = {2024-12-15},
	eprinttype = {arxiv},
	eprint = {2411.06606 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/CDXZPFDL/Desai et al. - 2024 - Gen-AI for User Safety A Survey.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/UZGFQ34D/2411.html:text/html},
}

@misc{ling_enhancing_2024,
	title = {Enhancing Security Control Production With Generative {AI}},
	url = {http://arxiv.org/abs/2411.04284},
	doi = {10.48550/arXiv.2411.04284},
	abstract = {Security controls are mechanisms or policies designed for cloud based services to reduce risk, protect information, and ensure compliance with security regulations. The development of security controls is traditionally a labor-intensive and time-consuming process. This paper explores the use of Generative {AI} to accelerate the generation of security controls. We specifically focus on generating Gherkin codes which are the domain-specific language used to define the behavior of security controls in a structured and understandable format. By leveraging large language models and in-context learning, we propose a structured framework that reduces the time required for developing security controls from 2-3 days to less than one minute. Our approach integrates detailed task descriptions, step-by-step instructions, and retrieval-augmented generation to enhance the accuracy and efficiency of the generated Gherkin code. Initial evaluations on {AWS} cloud services demonstrate promising results, indicating that {GenAI} can effectively streamline the security control development process, thus providing a robust and dynamic safeguard for cloud-based infrastructures.},
	number = {{arXiv}:2411.04284},
	publisher = {{arXiv}},
	author = {Ling, Chen and Ghashami, Mina and Gao, Vianne and Torkamani, Ali and Vaulin, Ruslan and Mangam, Nivedita and Jain, Bhavya and Diwan, Farhan and {SS}, Malini and Cheng, Mingrui and Kumar, Shreya Tarur and Candelario, Felix},
	urldate = {2025-06-09},
	date = {2024-11-06},
	eprinttype = {arxiv},
	eprint = {2411.04284 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/63MHB3Y2/Ling et al. - 2024 - Enhancing Security Control Production With Generative AI.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/D5NYMBMI/2411.html:text/html},
}

@inproceedings{chen_platform-specific_2025,
	title = {Platform-specific code generation method for the Tyche embedded operating system based on {AADL} models},
	volume = {13545},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13545/1354517/Platform-specific-code-generation-method-for-the-Tyche-embedded-operating/10.1117/12.3061113.full},
	doi = {10.1117/12.3061113},
	abstract = {The Tyche Embedded Operating System is a highly customized real-time embedded operating system, widely applied in fields such as aerospace, medical devices, and industrial automation. Renowned for its robust support for real-time tasks and efficient resource management, this system has become a core technology in embedded system development. The Architecture Analysis and Design Language ({AADL}), as a stringent standardized modeling language, possesses powerful expressiveness and scalability. This paper proposes a method for generating platform-specific code for the Tyche Embedded Operating System based on {AADL} models, which involves describing complex systems through {AADL} modeling and generating safety-critical software suitable for the Tyche Embedded Operating System. Firstly, the paper elaborates on the key aspects of the {AADL} standard and adheres to these aspects for modeling safety-critical systems. Secondly, an intermediate layer is introduced, outlining the mapping rules from the {AADL} common subset to the intermediate code, generating intermediate code for each component within the system architecture, and ultimately compiling it into C code applicable to the Tyche platform. Finally, a prototype tool for code generation is provided, and the effectiveness of the proposed method is validated through a case study of a temperature control system.},
	eventtitle = {Third International Conference on Algorithms, Network, and Communication Technology ({ICANCT} 2024)},
	pages = {328--340},
	booktitle = {Third International Conference on Algorithms, Network, and Communication Technology ({ICANCT} 2024)},
	publisher = {{SPIE}},
	author = {Chen, Li and Jia, Zhangtao and An, Heng and Li, Haoyu},
	urldate = {2025-06-10},
	date = {2025-03-03},
}

@article{kumar_generative_nodate,
	title = {Generative {AI} in Software Architecture: Transforming Design and Development Processes},
	volume = {16},
	rights = {Creative Commons Attribution-{ShareAlike} 4.0 International License},
	issn = {2229-7677},
	url = {https://www.ijsat.org/research-paper.php?id=3718},
	doi = {10.71097/IJSAT.v16.i1.3718},
	shorttitle = {Generative {AI} in Software Architecture},
	number = {1},
	journaltitle = {{IJSAT} - International Journal on Science and Technology},
	author = {Kumar, Ritesh},
	urldate = {2025-06-10},
	note = {Publisher: {IJSAT}},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/L7W95BUV/Kumar - Generative AI in Software Architecture Transforming Design and Development Processes.pdf:application/pdf},
}

@inproceedings{ismail_big_2025-1,
	title = {Big Data Architecture for Large Organizations},
	url = {https://www.semanticscholar.org/paper/Big-Data-Architecture-for-Large-Organizations-Ismail-Sengupta/4c954e46ecc91d41e554afba14d3a294b07b7e5e},
	abstract = {The exponential growth of big data has transformed how large organisations leverage information to drive innovation, optimise processes, and maintain competitive advantages. However, managing and extracting insights from vast, heterogeneous data sources requires a scalable, secure, and well-integrated big data architecture. This paper proposes a comprehensive big data framework that aligns with organisational objectives while ensuring flexibility, scalability, and governance. The architecture encompasses multiple layers, including data ingestion, transformation, storage, analytics, machine learning, and security, incorporating emerging technologies such as Generative {AI} ({GenAI}) and low-code machine learning. Cloud-based implementations across Google Cloud, {AWS}, and Microsoft Azure are analysed, highlighting their tools and capabilities. Additionally, this study explores advancements in big data architecture, including {AI}-driven automation, data mesh, and Data Ocean paradigms. By establishing a structured, adaptable framework, this research provides a foundational blueprint for large organisations to harness big data as a strategic asset effectively.},
	author = {Ismail, Fathima Nuzla and Sengupta, Abira and Amarasoma, Shanika},
	urldate = {2025-06-10},
	date = {2025-05-07},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/EBP9KYNF/Ismail et al. - 2025 - Big Data Architecture for Large Organizations.pdf:application/pdf},
}

@inproceedings{chavan_automation_2024,
	title = {Automation of {AD}-{OHC} Dashbord and Monitoring of Cloud Resources using Genrative {AI} to Reduce Costing and Enhance Performance},
	url = {https://ieeexplore.ieee.org/document/10616299},
	doi = {10.1109/ICICET59348.2024.10616299},
	abstract = {In this extensive review, the incorporation of Generative Artificial Intelligence ({AI}) into ad-hoc dashboards and cloud resource monitoring is investigated in depth. A purpose of this work is to investigate a current research and highlight the transformational potential of generative artificial intelligence for optimising costs, automating tasks, and improving overall cost efficiency. This article includes a comprehensive analysis of a development of ad-hoc cloud computing, a relevance of cloud resource monitoring, and the role that generative artificial intelligence plays in reducing costs. Notable advantages include better user satisfaction, resource optimisation, adaptive learning, and efficient automation. Additionally, precise decision-making and resource optimisation are highlighted. By presenting actual evidence, the study highlights the association between the quality of generative artificial intelligence and the influence it has on society. It finishes with an analysis of generative artificial intelligence applications in ad-hoc dashboards, with a focus on increased resource utilisation, scalability, and timely consistency of cloud resources. In addition, the article addresses constraints and makes suggestions for future paths. These include models that protect users’ privacy, efficient resource utilisation, explainable artificial intelligence, dynamic autonomy, and security-driven generative models. The paper’s goal is to provide the groundwork for further study and improvement.},
	eventtitle = {2024 International Conference on Innovations and Challenges in Emerging Technologies ({ICICET})},
	pages = {1--9},
	booktitle = {2024 International Conference on Innovations and Challenges in Emerging Technologies ({ICICET})},
	author = {Chavan, Parikshit and Chavan, Peeyusha},
	urldate = {2025-06-10},
	date = {2024-06},
	keywords = {Cloud computing, Generative {AI}, Scalability, Resource management, Technological innovation, {AD}-{OHC} Dashbord, Automation, Cloud Computing, Cloud Monitoring., Cloud Resources, Cost Reductions, Costs},
}

@article{lo_increased_2024,
	title = {Increased Productivity and Reduced Waste with Robotic Process Automation and Generative {AI}-powered {IoE} Services},
	rights = {Copyright (c) 2024 Journal of Web Engineering},
	issn = {1544-5976},
	url = {https://journals.riverpublishers.com/index.php/JWE/},
	doi = {10.13052/jwe1540-9589.2313},
	abstract = {The convergence of robotic process automation ({RPA}) and generative {AI} ({GAI}) within the context of Internet of Everything ({IoE}) services represents a profound paradigm shift. This fusion of technologies not only streamlines routine tasks but also catalyzes innovation while harnessing the potential of interconnected devices. Such integration empowers organizations to achieve remarkable gains in efficiency and sustainability. This paper embarks on an exploration of these transformative services, designed to elevate productivity, and curtail wasteful practices in contemporary industries. By closely examining intricate case studies, we illuminate the multifaceted advantages of this integrated approach. Our investigation demonstrates how {RPA} accelerates the execution of repetitive processes, substantially diminishing the margin for human error and amplifying operational efficiency. In contrast, generative {AI} introduces a disruptive force, generating fresh ideas, designs, and solutions, thereby elevating the quality of products and services. The infusion of these cutting-edge technologies into the fabric of {IoE} services paves the way for organizations to attain unprecedented levels of automation, intelligence, and connectivity. Furthermore, this paper comprehensively addresses the intricate challenges and considerations associated with the proposed implementation. We delve into ethical concerns, security implications, and the necessary workforce adaptation to offer a balanced perspective on the adoption of these technologies. Additionally, we navigate through potential limitations and constraints, underscoring the imperative need for strategic planning and robust governance.},
	pages = {53--88},
	journaltitle = {Journal of Web Engineering},
	author = {Lo, Wei and Yang, Chun-Ming and Zhang, Qiansha and Li, Mingyuan},
	urldate = {2025-06-10},
	date = {2024-03-27},
	langid = {english},
	keywords = {waste management},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/LM6B6ER4/Lo et al. - 2024 - Increased Productivity and Reduced Waste with Robotic Process Automation and Generative AI-powered I.pdf:application/pdf},
}

@misc{barrett_actionable_2023,
	title = {Actionable Guidance for High-Consequence {AI} Risk Management: Towards Standards Addressing {AI} Catastrophic Risks},
	url = {http://arxiv.org/abs/2206.08966},
	doi = {10.48550/arXiv.2206.08966},
	shorttitle = {Actionable Guidance for High-Consequence {AI} Risk Management},
	abstract = {Artificial intelligence ({AI}) systems can provide many beneficial capabilities but also risks of adverse events. Some {AI} systems could present risks of events with very high or catastrophic consequences at societal scale. The {US} National Institute of Standards and Technology ({NIST}) has been developing the {NIST} Artificial Intelligence Risk Management Framework ({AI} {RMF}) as voluntary guidance on {AI} risk assessment and management for {AI} developers and others. For addressing risks of events with catastrophic consequences, {NIST} indicated a need to translate from high level principles to actionable risk management guidance. In this document, we provide detailed actionable-guidance recommendations focused on identifying and managing risks of events with very high or catastrophic consequences, intended as a risk management practices resource for {NIST} for {AI} {RMF} version 1.0 (released in January 2023), or for {AI} {RMF} users, or for other {AI} risk management guidance and standards as appropriate. We also provide our methodology for our recommendations. We provide actionable-guidance recommendations for {AI} {RMF} 1.0 on: identifying risks from potential unintended uses and misuses of {AI} systems; including catastrophic-risk factors within the scope of risk assessments and impact assessments; identifying and mitigating human rights harms; and reporting information on {AI} risk factors including catastrophic-risk factors. In addition, we provide recommendations on additional issues for a roadmap for later versions of the {AI} {RMF} or supplementary publications. These include: providing an {AI} {RMF} Profile with supplementary guidance for cutting-edge increasingly multi-purpose or general-purpose {AI}. We aim for this work to be a concrete risk-management practices contribution, and to stimulate constructive dialogue on how to address catastrophic risks and associated issues in {AI} standards.},
	number = {{arXiv}:2206.08966},
	publisher = {{arXiv}},
	author = {Barrett, Anthony M. and Hendrycks, Dan and Newman, Jessica and Nonnecke, Brandie},
	urldate = {2025-06-17},
	date = {2023-02-23},
	eprinttype = {arxiv},
	eprint = {2206.08966 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/TRQMFW6W/Barrett et al. - 2023 - Actionable Guidance for High-Consequence AI Risk Management Towards Standards Addressing AI Catastr.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/RYELLFI2/2206.html:text/html},
}

@article{zanzotto_human---loop_2019,
	title = {Human-in-the-loop Artificial Intelligence},
	volume = {64},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1710.08191},
	doi = {10.1613/jair.1.11345},
	abstract = {Little by little, newspapers are revealing the bright future that Artificial Intelligence ({AI}) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of {AI} is sustained by the biggest knowledge theft of the recent years. Learning {AI} machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves. In this paper, we propose Human-in-the-loop Artificial Intelligence ({HIT}-{AI}) as a fairer paradigm for Artificial Intelligence systems. {HIT}-{AI} will reward aware and unaware knowledge producers with a different scheme: decisions of {AI} systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, {HIT}-{AI} researchers should fight for a fairer Artificial Intelligence that gives back what it steals.},
	pages = {243--252},
	journaltitle = {jair},
	author = {Zanzotto, Fabio Massimo},
	urldate = {2025-06-17},
	date = {2019-02-10},
	eprinttype = {arxiv},
	eprint = {1710.08191 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/QP7ISIZ4/Zanzotto - 2019 - Human-in-the-loop Artificial Intelligence.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/4WXXLBSJ/1710.html:text/html},
}

@article{wu_survey_2022,
	title = {A Survey of Human-in-the-loop for Machine Learning},
	volume = {135},
	issn = {0167739X},
	url = {http://arxiv.org/abs/2108.00941},
	doi = {10.1016/j.future.2022.05.014},
	abstract = {Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field; along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.},
	pages = {364--381},
	journaltitle = {Future Generation Computer Systems},
	author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
	urldate = {2025-06-17},
	date = {2022-10},
	eprinttype = {arxiv},
	eprint = {2108.00941 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/YPAFM2DT/Wu et al. - 2022 - A Survey of Human-in-the-loop for Machine Learning.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/IWVQMSDD/2108.html:text/html},
}

@article{andrade_enhancing_2025,
	title = {Enhancing Security in Software Design Patterns and Antipatterns: A Framework for {LLM}-Based Detection},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/14/3/586},
	doi = {10.3390/electronics14030586},
	shorttitle = {Enhancing Security in Software Design Patterns and Antipatterns},
	abstract = {The detection of security vulnerabilities in software design patterns and antipatterns is crucial for maintaining robust and maintainable systems, particularly in dynamic Continuous Integration/Continuous Deployment ({CI}/{CD}) environments. Traditional static analysis tools, while effective for identifying isolated issues, often lack contextual awareness, leading to missed vulnerabilities and high rates of false positives. This paper introduces a novel framework leveraging Large Language Models ({LLMs}) to detect and mitigate security risks in design patterns and antipatterns. By analyzing relationships and behavioral dynamics in code, {LLMs} provide a nuanced, context-aware approach to identifying issues such as unauthorized state changes, insecure communication, and improper data handling. The proposed framework integrates key security heuristics—such as the principles of least privilege and input validation—to enhance {LLM} performance. An evaluation of the framework demonstrates its potential to outperform traditional tools in terms of accuracy and efficiency, enabling the proactive detection and remediation of vulnerabilities in real time. This study contributes to the field of software engineering by offering an innovative methodology for securing software systems using {LLMs}, promoting both academic research and practical application in industry settings.},
	pages = {586},
	number = {3},
	journaltitle = {Electronics},
	author = {Andrade, Roberto and Torres, Jenny and Ortiz-Garcés, Iván},
	urldate = {2025-06-18},
	date = {2025-01},
	langid = {english},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{LLM}, antipatterns, software security},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/7LELERKQ/Andrade et al. - 2025 - Enhancing Security in Software Design Patterns and Antipatterns A Framework for LLM-Based Detection.pdf:application/pdf},
}

@article{andrade_enhancing_2025-1,
	title = {Enhancing Security in Software Design Patterns and Antipatterns: A Framework for {LLM}-Based Detection},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/14/3/586},
	doi = {10.3390/electronics14030586},
	shorttitle = {Enhancing Security in Software Design Patterns and Antipatterns},
	abstract = {The detection of security vulnerabilities in software design patterns and antipatterns is crucial for maintaining robust and maintainable systems, particularly in dynamic Continuous Integration/Continuous Deployment ({CI}/{CD}) environments. Traditional static analysis tools, while effective for identifying isolated issues, often lack contextual awareness, leading to missed vulnerabilities and high rates of false positives. This paper introduces a novel framework leveraging Large Language Models ({LLMs}) to detect and mitigate security risks in design patterns and antipatterns. By analyzing relationships and behavioral dynamics in code, {LLMs} provide a nuanced, context-aware approach to identifying issues such as unauthorized state changes, insecure communication, and improper data handling. The proposed framework integrates key security heuristics—such as the principles of least privilege and input validation—to enhance {LLM} performance. An evaluation of the framework demonstrates its potential to outperform traditional tools in terms of accuracy and efficiency, enabling the proactive detection and remediation of vulnerabilities in real time. This study contributes to the field of software engineering by offering an innovative methodology for securing software systems using {LLMs}, promoting both academic research and practical application in industry settings.},
	pages = {586},
	number = {3},
	journaltitle = {Electronics},
	author = {Andrade, Roberto and Torres, Jenny and Ortiz-Garcés, Iván},
	urldate = {2025-06-18},
	date = {2025-01},
	langid = {english},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {{LLM}, antipatterns, software security},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/XMRADTJW/Andrade et al. - 2025 - Enhancing Security in Software Design Patterns and Antipatterns A Framework for LLM-Based Detection.pdf:application/pdf},
}

@misc{li_iris_2025,
	title = {{IRIS}: {LLM}-Assisted Static Analysis for Detecting Security Vulnerabilities},
	url = {http://arxiv.org/abs/2405.17238},
	doi = {10.48550/arXiv.2405.17238},
	shorttitle = {{IRIS}},
	abstract = {Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or {LLMs}) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose {IRIS}, a neuro-symbolic approach that systematically combines {LLMs} with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, {IRIS} leverages {LLMs} to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, {CWE}-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool {CodeQL} detects only 27 of these vulnerabilities whereas {IRIS} with {GPT}-4 detects 55 (+28) and improves upon {CodeQL}'s average false discovery rate by 5\% points. Furthermore, {IRIS} identifies 4 previously unknown vulnerabilities which cannot be found by existing tools. {IRIS} is available publicly at https://github.com/iris-sast/iris.},
	number = {{arXiv}:2405.17238},
	publisher = {{arXiv}},
	author = {Li, Ziyang and Dutta, Saikat and Naik, Mayur},
	urldate = {2025-06-18},
	date = {2025-04-06},
	eprinttype = {arxiv},
	eprint = {2405.17238 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Programming Languages, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/8GY7RFXK/Li et al. - 2025 - IRIS LLM-Assisted Static Analysis for Detecting Security Vulnerabilities.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/AR9WSYRE/2405.html:text/html},
}

@online{noauthor_streamlining_nodate,
	title = {Streamlining {CI}/{CD} Pipelines with Automated Policy Checks},
	url = {https://cloudsmith.com/blog/streamlining-ci-cd-pipelines-with-automated-policy-checks},
	abstract = {Continuous Integration and Continuous Deployment ({CI}/{CD}) pipelines power modern {DevOps}. They enable teams to deliver software faster, with greater reliability and confidence. However, as development a…},
	titleaddon = {Cloudsmith},
	urldate = {2025-06-26},
	langid = {british},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/4KN6HH5D/streamlining-ci-cd-pipelines-with-automated-policy-checks.html:text/html},
}

@article{preeta_pillai_python-driven_2025,
	title = {Python-driven security automation pipeline for enterprise financial reporting},
	volume = {15},
	issn = {25828266},
	url = {https://journalwjaets.com/node/593},
	doi = {10.30574/wjaets.2025.15.1.0473},
	abstract = {This article examines the implementation of Python-driven security automation within enterprise financial reporting systems. The article highlights critical vulnerabilities in financial applications and demonstrates how Python-based automation frameworks address these challenges through modular architectures, technology integration, and advanced sanitization techniques. The article involving 70,000+ financial reports illustrate significant improvements in vulnerability detection, remediation time, and compliance achievement through {CI}/{CD} pipeline integration, task orchestration with Airflow, and comprehensive monitoring systems. The article reveals that automated security controls substantially reduce error rates, improve operational efficiency, and enhance data integrity while decreasing resource requirements. The article concludes by exploring future trends in security automation, including the growing adoption of machine learning and advanced analytics to further improve threat detection capabilities, with broad implications for the financial services industry's approach to cybersecurity.},
	pages = {2233--2239},
	number = {1},
	journaltitle = {World J. Adv. Eng. Technol. Sci.},
	author = {{Preeta Pillai}},
	urldate = {2025-06-26},
	date = {2025-04-30},
	langid = {english},
	file = {PDF:/Users/d.veragilliard/Zotero/storage/QS4A8RSN/Preeta Pillai - 2025 - Python-driven security automation pipeline for enterprise financial reporting.pdf:application/pdf},
}

@online{noauthor_human---loop_nodate,
	title = {Human-in-the-loop in {SOC} Automation},
	url = {https://www.xenonstack.com/blog/human-loop-soc-automation},
	abstract = {Explore the role of human-in-the-loop in {SOC} automation, enhancing decision-making, efficiency, and responsiveness in cybersecurity operations.},
	urldate = {2025-06-26},
	langid = {english},
}

@misc{haque_sok_2025,
	title = {{SOK}: Exploring Hallucinations and Security Risks in {AI}-Assisted Software Development with Insights for {LLM} Deployment},
	url = {http://arxiv.org/abs/2502.18468},
	doi = {10.48550/arXiv.2502.18468},
	shorttitle = {{SOK}},
	abstract = {The integration of Large Language Models ({LLMs}) such as {GitHub} Copilot, {ChatGPT}, Cursor {AI}, and Codeium {AI} into software development has revolutionized the coding landscape, offering significant productivity gains, automation, and enhanced debugging capabilities. These tools have proven invaluable for generating code snippets, refactoring existing code, and providing real-time support to developers. However, their widespread adoption also presents notable challenges, particularly in terms of security vulnerabilities, code quality, and ethical concerns. This paper provides a comprehensive analysis of the benefits and risks associated with {AI}-powered coding tools, drawing on user feedback, security analyses, and practical use cases. We explore the potential for these tools to replicate insecure coding practices, introduce biases, and generate incorrect or non-sensical code (hallucinations). In addition, we discuss the risks of data leaks, intellectual property violations and the need for robust security measures to mitigate these threats. By comparing the features and performance of these tools, we aim to guide developers in making informed decisions about their use, ensuring that the benefits of {AI}-assisted coding are maximized while minimizing associated risks.},
	number = {{arXiv}:2502.18468},
	publisher = {{arXiv}},
	author = {Haque, Ariful and Siddique, Sunzida and Rahman, Md Mahfuzur and Hasan, Ahmed Rafi and Das, Laxmi Rani and Kamal, Marufa and Masura, Tasnim and Gupta, Kishor Datta},
	urldate = {2025-06-26},
	date = {2025-01-31},
	eprinttype = {arxiv},
	eprint = {2502.18468 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/F3JSM2Y2/Haque et al. - 2025 - SOK Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights f.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/N5B2C7PZ/2502.html:text/html},
}

@online{noauthor_evaluating_2024,
	title = {Evaluating Static Analysis Alerts with {LLMs}},
	url = {https://insights.sei.cmu.edu/blog/evaluating-static-analysis-alerts-with-llms/},
	abstract = {{LLMs} show promising initial results in adjudicating static analysis alerts, offering possibilities for better vulnerability detection. This post discusses initial experiments using {GPT}-4 to evaluate static analysis alerts.},
	urldate = {2025-06-26},
	date = {2024-10-07},
	langid = {english},
}

@misc{ozgur_simple_2024,
	title = {A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts},
	url = {http://arxiv.org/abs/2407.06718},
	doi = {10.48550/arXiv.2407.06718},
	abstract = {This study proposes a simple architecture for Enterprise application for Large Language Models ({LLMs}) for role based security and {NATO} clearance levels. Our proposal aims to address the limitations of current {LLMs} in handling security and information access. The proposed architecture could be used while utilizing Retrieval-Augmented Generation ({RAG}) and fine tuning of Mixture of experts models ({MoE}). It could be used only with {RAG}, or only with {MoE} or with both of them. Using roles and security clearance level of the user, documents in {RAG} and experts in {MoE} are filtered. This way information leakage is prevented.},
	number = {{arXiv}:2407.06718},
	publisher = {{arXiv}},
	author = {Özgür, Atilla and Uygun, Yılmaz},
	urldate = {2025-06-26},
	date = {2024-07-09},
	eprinttype = {arxiv},
	eprint = {2407.06718 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/LVD39QTQ/Özgür and Uygun - 2024 - A Simple Architecture for Enterprise Large Language Model Applications based on Role based security.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/TVHQJZXR/2407.html:text/html},
}

@online{noauthor_ground_nodate,
	title = {Ground responses using {RAG} {\textbar} Generative {AI} on Vertex {AI}},
	url = {https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/ground-responses-using-rag},
	titleaddon = {Google Cloud},
	urldate = {2025-06-26},
	langid = {english},
}

@inproceedings{noauthor_testbed_2025,
	title = {A Testbed for Operations in the Information Environment {\textbar} Request {PDF}},
	url = {https://www.researchgate.net/publication/383085784_A_Testbed_for_Operations_in_the_Information_Environment},
	doi = {10.1145/3675741.3675751},
	abstract = {Request {PDF} {\textbar} On Aug 13, 2024, Adam Tse and others published A Testbed for Operations in the Information Environment {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	booktitle = {{ResearchGate}},
	urldate = {2025-06-26},
	date = {2025-02-12},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/DETPWEVX/383085784_A_Testbed_for_Operations_in_the_Information_Environment.html:text/html},
}

@misc{lim_explicate_2025,
	title = {{EXPLICATE}: Enhancing Phishing Detection through Explainable {AI} and {LLM}-Powered Interpretability},
	url = {http://arxiv.org/abs/2503.20796},
	doi = {10.48550/arXiv.2503.20796},
	shorttitle = {{EXPLICATE}},
	abstract = {Sophisticated phishing attacks have emerged as a major cybersecurity threat, becoming more common and difficult to prevent. Though machine learning techniques have shown promise in detecting phishing attacks, they function mainly as "black boxes" without revealing their decision-making rationale. This lack of transparency erodes the trust of users and diminishes their effective threat response. We present {EXPLICATE}: a framework that enhances phishing detection through a three-component architecture: an {ML}-based classifier using domain-specific features, a dual-explanation layer combining {LIME} and {SHAP} for complementary feature-level insights, and an {LLM} enhancement using {DeepSeek} v3 to translate technical explanations into accessible natural language. Our experiments show that {EXPLICATE} attains 98.4 \% accuracy on all metrics, which is on par with existing deep learning techniques but has better explainability. High-quality explanations are generated by the framework with an accuracy of 94.2 \% as well as a consistency of 96.8{\textbackslash}\% between the {LLM} output and model prediction. We create {EXPLICATE} as a fully usable {GUI} application and a light Chrome extension, showing its applicability in many deployment situations. The research shows that high detection performance can go hand-in-hand with meaningful explainability in security applications. Most important, it addresses the critical divide between automated {AI} and user trust in phishing detection systems.},
	number = {{arXiv}:2503.20796},
	publisher = {{arXiv}},
	author = {Lim, Bryan and Huerta, Roman and Sotelo, Alejandro and Quintela, Anthonie and Kumar, Priyanka},
	urldate = {2025-06-26},
	date = {2025-03-22},
	eprinttype = {arxiv},
	eprint = {2503.20796 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/79YGBKVK/Lim et al. - 2025 - EXPLICATE Enhancing Phishing Detection through Explainable AI and LLM-Powered Interpretability.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/2FFKWE76/2503.html:text/html},
}

@misc{fakih_llm4cve_2025,
	title = {{LLM}4CVE: Enabling Iterative Automated Vulnerability Repair with Large Language Models},
	url = {http://arxiv.org/abs/2501.03446},
	doi = {10.48550/arXiv.2501.03446},
	shorttitle = {{LLM}4CVE},
	abstract = {Software vulnerabilities continue to be ubiquitous, even in the era of {AI}-powered code assistants, advanced static analysis tools, and the adoption of extensive testing frameworks. It has become apparent that we must not simply prevent these bugs, but also eliminate them in a quick, efficient manner. Yet, human code intervention is slow, costly, and can often lead to further security vulnerabilities, especially in legacy codebases. The advent of highly advanced Large Language Models ({LLM}) has opened up the possibility for many software defects to be patched automatically. We propose {LLM}4CVE an {LLM}-based iterative pipeline that robustly fixes vulnerable functions in real-world code with high accuracy. We examine our pipeline with State-of-the-Art {LLMs}, such as {GPT}-3.5, {GPT}-4o, Llama 38B, and Llama 3 70B. We achieve a human-verified quality score of 8.51/10 and an increase in groundtruth code similarity of 20\% with Llama 3 70B. To promote further research in the area of {LLM}-based vulnerability repair, we publish our testing apparatus, fine-tuned weights, and experimental data on our website},
	number = {{arXiv}:2501.03446},
	publisher = {{arXiv}},
	author = {Fakih, Mohamad and Dharmaji, Rahul and Bouzidi, Halima and Araya, Gustavo Quiros and Ogundare, Oluwatosin and Faruque, Mohammad Abdullah Al},
	urldate = {2025-06-26},
	date = {2025-01-07},
	eprinttype = {arxiv},
	eprint = {2501.03446 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/9SJ98W5G/Fakih et al. - 2025 - LLM4CVE Enabling Iterative Automated Vulnerability Repair with Large Language Models.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/56ZU5RN9/2501.html:text/html},
}

@inproceedings{noauthor_artificial_2025,
	title = {Artificial Intelligence for Cybersecurity: A State of the Art {\textbar} Request {PDF}},
	url = {https://www.researchgate.net/publication/388722398_Artificial_Intelligence_for_Cybersecurity_A_State_of_the_Art},
	doi = {10.1109/ICAIC63015.2025.10848980},
	shorttitle = {Artificial Intelligence for Cybersecurity},
	abstract = {Request {PDF} {\textbar} On Feb 5, 2025, Abdullah Al Siam and others published Artificial Intelligence for Cybersecurity: A State of the Art {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	booktitle = {{ResearchGate}},
	urldate = {2025-06-26},
	date = {2025-03-22},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/IDACMA4X/388722398_Artificial_Intelligence_for_Cybersecurity_A_State_of_the_Art.html:text/html},
}

@misc{zhang_empirical_2024,
	title = {An Empirical Study of Automated Vulnerability Localization with Large Language Models},
	url = {http://arxiv.org/abs/2404.00287},
	doi = {10.48550/arXiv.2404.00287},
	abstract = {Recently, Automated Vulnerability Localization ({AVL}) has attracted much attention, aiming to facilitate diagnosis by pinpointing the lines of code responsible for discovered vulnerabilities. Large Language Models ({LLMs}) have shown potential in various domains, yet their effectiveness in vulnerability localization remains underexplored. In this work, we perform the first comprehensive study of {LLMs} for {AVL}. Our investigation encompasses 10+ leading {LLMs} suitable for code analysis, including {ChatGPT} and various open-source models, across three architectural types: encoder-only, encoder-decoder, and decoder-only, with model sizes ranging from 60M to 16B parameters. We explore the efficacy of these {LLMs} using 4 distinct paradigms: zero-shot learning, one-shot learning, discriminative fine-tuning, and generative fine-tuning. Our evaluation framework is applied to the {BigVul}-based dataset for C/C++, and an additional dataset comprising smart contract vulnerabilities. The results demonstrate that discriminative fine-tuning of {LLMs} can significantly outperform existing learning-based methods for {AVL}, while other paradigms prove less effective or unexpectedly ineffective for the task. We also identify challenges related to input length and unidirectional context in fine-tuning processes for encoders and decoders. We then introduce two remedial strategies: the sliding window and the right-forward embedding, both of which substantially enhance performance. Furthermore, our findings highlight certain generalization capabilities of {LLMs} across Common Weakness Enumerations ({CWEs}) and different projects, indicating a promising pathway toward their practical application in vulnerability localization.},
	number = {{arXiv}:2404.00287},
	publisher = {{arXiv}},
	author = {Zhang, Jian and Wang, Chong and Li, Anran and Sun, Weisong and Zhang, Cen and Ma, Wei and Liu, Yang},
	urldate = {2025-06-26},
	date = {2024-03-30},
	eprinttype = {arxiv},
	eprint = {2404.00287 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/6AGRZFFX/Zhang et al. - 2024 - An Empirical Study of Automated Vulnerability Localization with Large Language Models.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/GQVKV35W/2404.html:text/html},
}

@inproceedings{gunathilaka_context-aware_2025,
	title = {Context-Aware Behavior-Driven Pipeline Generation},
	url = {https://ieeexplore.ieee.org/document/11011952},
	doi = {10.1109/ISDFS65363.2025.11011952},
	abstract = {An efficient {CI}/{CD} process is crucial for modern software teams, but manual pipeline creation is error-prone and requires high {DevOps} expertise, slowing deployment speed and reducing productivity. This research introduces a context-aware, behavior-driven approach to fully automating {CI}/{CD} pipeline generation by analyzing {GitHub} user activity patterns. The proposed solution utilizes a historical analysis of repository events, developer contributions, and workload distribution to dynamically generate pipelines and assign reviewers to pull requests based on expertise. Unlike previous template-based and generative {AI} solutions that require manual intervention, our approach leverages pattern recognition and adaptive decision-making to continuously refine automation. This paper presents the methodology behind data collection, analysis, and pipeline generation, demonstrating its effectiveness in reducing human effort while improving software delivery efficiency. This research highlights how behavior-driven automation streamlines the complexity of {CI}/{CD} pipeline creation, enabling more adaptive and intelligent systems that effectively respond to the evolving needs of software development teams.},
	eventtitle = {2025 13th International Symposium on Digital Forensics and Security ({ISDFS})},
	pages = {1--6},
	booktitle = {2025 13th International Symposium on Digital Forensics and Security ({ISDFS})},
	author = {Gunathilaka, Pawara and Senadheera, Dinal and Perara, Shenan and Gunawardana, Chamithu and Thelijjagoda, Samantha and Krishara, Jenny},
	urldate = {2025-06-26},
	date = {2025-04},
	note = {{ISSN}: 2768-1831},
	keywords = {Adaptive systems, Automation, behavior-driven development, {CI}/{CD}, context-aware, {DevOps}, {GitHub} activity analysis, Intelligent systems, Manuals, Pipelines, reviewer assignment, Reviews, Security, Software, Testing, workload optimization},
}

@article{alevizos_towards_2024,
	title = {Towards an {AI}-Enhanced Cyber Threat Intelligence Processing Pipeline},
	volume = {13},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/13/11/2021},
	doi = {10.3390/electronics13112021},
	abstract = {Cyber threats continue to evolve in complexity, thereby traditional cyber threat intelligence ({CTI}) methods struggle to keep pace. {AI} offers a potential solution, automating and enhancing various tasks, from data ingestion to resilience verification. This paper explores the potential of integrating artificial intelligence ({AI}) into {CTI}. We provide a blueprint of an {AI}-enhanced {CTI} processing pipeline and detail its components and functionalities. The pipeline highlights the collaboration between {AI} and human expertise, which is necessary to produce timely and high-fidelity cyber threat intelligence. We also explore the automated generation of mitigation recommendations, harnessing {AI}’s capabilities to provide real-time, contextual, and predictive insights. However, the integration of {AI} into {CTI} is not without its challenges. Thereby, we discuss the ethical dilemmas, potential biases, and the imperative for transparency in {AI}-driven decisions. We address the need for data privacy, consent mechanisms, and the potential misuse of technology. Moreover, we highlight the importance of addressing biases both during {CTI} analysis and within {AI} models, warranting their transparency and interpretability. Lastly, our work points out future research directions, such as the exploration of advanced {AI} models to augment cyber defenses, and human–{AI} collaboration optimization. Ultimately, the fusion of {AI} with {CTI} appears to hold significant potential in the cybersecurity domain.},
	pages = {2021},
	number = {11},
	journaltitle = {Electronics},
	author = {Alevizos, Lampis and Dekker, Martijn},
	urldate = {2025-06-26},
	date = {2024-01},
	langid = {english},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, {CTI} and {AI} biases, cyber resilience, cyber threat intelligence, ethical considerations},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/WF54MVFY/Alevizos and Dekker - 2024 - Towards an AI-Enhanced Cyber Threat Intelligence Processing Pipeline.pdf:application/pdf},
}

@inproceedings{akiri_generative_2025,
	title = {Generative {AI} for Real-Time Cloud Security: Advanced Anomaly Detection Using {GPT} Models},
	url = {https://ieeexplore.ieee.org/document/11011269},
	doi = {10.1109/ICCA65395.2025.11011269},
	shorttitle = {Generative {AI} for Real-Time Cloud Security},
	abstract = {As cloud infrastructures become increasingly complex and integral to modern enterprises, the demand for advanced, real-time security solutions has grown significantly. Traditional anomaly detection systems often struggle to keep pace with the rapid evolution of cyber threats in these dynamic environments, particularly when faced with novel or sophisticated attacks. Such systems typically rely on predefined rules or signature-based detection, which limits their effectiveness in identifying emerging security risks. This paper explores the potential of generative {AI} models, specifically {LLaMA} and {OpenAI}’s {GPT} architectures, to enhance real-time cloud security. By leveraging the advanced pattern recognition and adaptive capabilities of these models, we propose a framework that can analyze vast amounts of cloud data, including logs, network traffic, user behavior, and system activities, to detect abnormal patterns indicative of security threats. The real-time anomaly detection offered by generative {AI} provides a significant advantage over traditional methods, as it continuously learns from new data, thereby improving its ability to identify novel threats in complex cloud environments.This research addresses key gaps in current cloud security practices, highlighting the limitations of existing systems in detecting previously unknown threats. The proposed approach introduces generative {AI} as a highly adaptive and scalable solution for cloud anomaly detection, capable of responding to evolving threats in real-time. By using models such as {GPT}, which are known for their ability to generate coherent predictions based on diverse inputs, the framework offers a novel means of safe-guarding cloud infrastructure. This study not only underscores the benefits of employing generative {AI} for security purposes but also provides a robust methodology for integrating these models into cloud security systems. The paper concludes with an assessment of the practical deployment of these {AI} models in large-scale cloud environments, demonstrating their potential to significantly enhance the resilience and adaptability of modern cloud security frameworks.},
	eventtitle = {2025 {IEEE} Conference on Computer Applications ({ICCA})},
	pages = {1--6},
	booktitle = {2025 {IEEE} Conference on Computer Applications ({ICCA})},
	author = {Akiri, Charan Kumar and Jayabalan, Kathiresan and Lopes, Joel and Kareem, Shaik Abdul and Tabbassum, Ayisha},
	urldate = {2025-06-26},
	date = {2025-03},
	keywords = {Adaptation models, Anomaly detection, Cloud computing security, Cloud Security, Data models, Generative {AI}, {GPT} and {LLaMA} Models, Organizations, Real-time Anomaly Detection, Real-time systems, Scalability, Security, Telecommunication traffic, Zero-Day Attack Detection},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/6I4SWV5E/11011269.html:text/html},
}

@article{noauthor_pdf_2025,
	title = {({PDF}) {AI}-driven cybersecurity framework for software development based on the {ANN}-{ISM} paradigm},
	url = {https://www.researchgate.net/publication/390922505_AI-driven_cybersecurity_framework_for_software_development_based_on_the_ANN-ISM_paradigm},
	doi = {10.1038/s41598-025-97204-y},
	abstract = {{PDF} {\textbar} With the increasing reliance on software applications, cybersecurity threats have become a critical concern for developers and organizations. The... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	journaltitle = {{ResearchGate}},
	urldate = {2025-06-26},
	date = {2025-04-23},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/QE8HQFLA/390922505_AI-driven_cybersecurity_framework_for_software_development_based_on_the_ANN-ISM_parad.html:text/html},
}

@inproceedings{nicosia_risk_nodate,
	title = {Risk management in human-in-the-loop {AI}-assisted attention aware systems},
	url = {https://www.semanticscholar.org/paper/Risk-management-in-human-in-the-loop-AI-assisted-Nicosia-Kristensson/49d3e97575ba5e53fba8d772750a4fa93c7c2cb6},
	abstract = {Semantic Scholar extracted view of "Risk management in human-in-the-loop {AI}-assisted attention aware systems" by Max Nicosia et al.},
	author = {Nicosia, Max and Kristensson, Ola},
	urldate = {2025-06-26},
	file = {Full Text PDF:/Users/d.veragilliard/Zotero/storage/HGPPAGNJ/Nicosia and Kristensson - Risk management in human-in-the-loop AI-assisted attention aware systems.pdf:application/pdf},
}

@article{sarathe_krisshnan_jutoo_vijayaraghavan_policy_2025,
	title = {Policy as Code: A paradigm shifts in infrastructure security and governance},
	volume = {26},
	issn = {25819615},
	url = {https://journalwjarr.com/node/1380},
	doi = {10.30574/wjarr.2025.26.1.1441},
	shorttitle = {Policy as Code},
	abstract = {Policy as Code represents a transformative approach to infrastructure security and governance in modern cloud environments. By codifying security and compliance policies as machine-readable code, organizations can automate enforcement throughout the development lifecycle. This paradigm shift addresses the velocity gap between rapid development cycles and traditionally slower security processes, enabling consistent policy enforcement without sacrificing agility. The integration with {CI}/{CD} pipelines allows for "shifting left" security considerations, identifying and remediating issues before they reach production. Various implementation approaches have emerged, from open-source tools like Open Policy Agent to cloud-native solutions, each with distinct advantages. While implementation challenges exist, including policy language complexity and organizational alignment, established best practices help organizations navigate these hurdles. As infrastructure continues to evolve, Policy as Code emerges as an essential strategy for maintaining security and compliance in dynamic, cloud-native environments, transforming governance from a perceived roadblock into an enabler of innovation.},
	pages = {3399--3405},
	number = {1},
	journaltitle = {World J. Adv. Res. Rev.},
	author = {{Sarathe Krisshnan Jutoo Vijayaraghavan}},
	urldate = {2025-06-26},
	date = {2025-04-30},
}

@inproceedings{noauthor_towards_2025,
	title = {Towards Transparent Intrusion Detection: A Coherence-Based Framework in Explainable {AI} Integrating Large Language Models},
	url = {https://www.researchgate.net/publication/388090990_Towards_Transparent_Intrusion_Detection_A_Coherence-Based_Framework_in_Explainable_AI_Integrating_Large_Language_Models},
	doi = {10.1109/TPS-ISA62245.2024.00020},
	shorttitle = {Towards Transparent Intrusion Detection},
	abstract = {Download Citation {\textbar} On Oct 28, 2024, Areej Alnahdi and others published Towards Transparent Intrusion Detection: A Coherence-Based Framework in Explainable {AI} Integrating Large Language Models {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	booktitle = {{ResearchGate}},
	urldate = {2025-06-26},
	date = {2025-01-19},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/SR7VHLBW/388090990_Towards_Transparent_Intrusion_Detection_A_Coherence-Based_Framework_in_Explainable_AI.html:text/html},
}

@misc{howard_terraform_2022,
	title = {Terraform -- Automating Infrastructure as a Service},
	url = {http://arxiv.org/abs/2205.10676},
	doi = {10.48550/arXiv.2205.10676},
	abstract = {Developing a software service requires a strict software development life cycle and process. This process demands controlling all application code through source control management as well as a rigorous versioning and branching strategy. However, the platform and infrastructure also benefit from this rigor. Software services must be deployed to a target run time environment and provisioning that environment through manual user actions is tedious and error-prone. Provisioning manually also becomes prohibitive as the number of resources grow and spread globally over multiple regions. The answer is to apply the same rigor to provisioning the infrastructure as applied to developing the application software. Terraform provides a platform allowing infrastructure resources to be defined in code. This code not only allows the automation of the infrastructure provisioning but also allows for a strict development and review life cycle, same as the application software.},
	number = {{arXiv}:2205.10676},
	publisher = {{arXiv}},
	author = {Howard, Michael},
	urldate = {2025-07-16},
	date = {2022-05-21},
	eprinttype = {arxiv},
	eprint = {2205.10676 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering},
	file = {Preprint PDF:/Users/d.veragilliard/Zotero/storage/LF9VYKNJ/Howard - 2022 - Terraform -- Automating Infrastructure as a Service.pdf:application/pdf;Snapshot:/Users/d.veragilliard/Zotero/storage/9LQBL6L4/2205.html:text/html},
}

@online{noauthor_welcome_nodate,
	title = {Welcome to {AWS} Documentation},
	url = {https://docs.aws.amazon.com/},
	urldate = {2025-07-16},
}

@online{noauthor_claude_nodate,
	title = {Claude Code on Amazon Bedrock},
	url = {https://docs.anthropic.com/en/docs/claude-code/amazon-bedrock},
	abstract = {Learn about configuring Claude Code through Amazon Bedrock, including setup, {IAM} configuration, and troubleshooting.},
	titleaddon = {Anthropic},
	urldate = {2025-07-16},
	langid = {english},
}

@online{noauthor_terraform_nodate,
	title = {Terraform {CLI} Documentation {\textbar} Terraform {\textbar} {HashiCorp} Developer},
	url = {https://developer.hashicorp.com/terraform/cli},
	abstract = {Learn Terraform's {CLI}-based workflows. You can use the {CLI} alone or with {HCP} Terraform or Terraform Enterprise.},
	titleaddon = {Terraform {CLI} Documentation {\textbar} Terraform {\textbar} {HashiCorp} Developer},
	urldate = {2025-07-16},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/UXDIWRIF/cli.html:text/html},
}

@online{noauthor_introduction_nodate,
	title = {Introduction {\textbar} Open Policy Agent},
	url = {https://openpolicyagent.org/docs},
	abstract = {The Open Policy Agent ({OPA}, pronounced "oh-pa") is an open source,},
	urldate = {2025-07-16},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/DKYK29AF/docs.html:text/html},
}

@online{noauthor_whats_nodate,
	title = {What’s New In Python 3.12},
	url = {https://docs.python.org/3/whatsnew/3.12.html},
	abstract = {Editor, Adam Turner,. This article explains the new features in Python 3.12, compared to 3.11. Python 3.12 was released on October 2, 2023. For full details, see the changelog. Summary – Release hi...},
	titleaddon = {Python documentation},
	urldate = {2025-07-16},
	langid = {english},
	file = {Snapshot:/Users/d.veragilliard/Zotero/storage/HTLJ4T5J/3.12.html:text/html},
}

@online{noauthor_github_2025,
	title = {{GitHub} Actions},
	url = {https://github.com/features/actions},
	abstract = {Easily build, package, release, update, and deploy your project in any language—on {GitHub} or any external system—without having to run code yourself.},
	titleaddon = {{GitHub}},
	urldate = {2025-07-16},
	date = {2025},
	langid = {english},
}
